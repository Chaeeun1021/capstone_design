{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","authorship_tag":"ABX9TyPky8twZwPX7UWh/aNm5iT0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FJtOpADlpSMr","executionInfo":{"status":"ok","timestamp":1720348726096,"user_tz":-540,"elapsed":21730,"user":{"displayName":"양유석","userId":"08992042625290856644"}},"outputId":"0584a2ab-7063-48a8-995a-6ff8ae6e66bf"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["!pip install torch torchvision\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BTM55np7peQq","outputId":"e47afbfe-f4d6-42eb-a770-35c2125edadd","executionInfo":{"status":"ok","timestamp":1720348801356,"user_tz":-540,"elapsed":58294,"user":{"displayName":"양유석","userId":"08992042625290856644"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.18.0+cu121)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.4)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n","  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n","  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n","  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n","  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n","  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n","  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n","  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n","  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n","  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n","  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n","Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n","  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.0)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n","  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m64.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.25.2)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n","Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n","Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n","Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105\n"]}]},{"cell_type":"code","source":["import os\n","import json\n","import concurrent.futures\n","from PIL import Image\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","import torchvision.transforms as transforms\n","\n","class RipCurrentDataset(Dataset):\n","    def __init__(self, image_dir, annotation_dir, transform=None, num_samples=20000):\n","        self.image_dir = image_dir\n","        self.annotation_dir = annotation_dir\n","        self.transform = transform\n","\n","        self.image_files = [f for f in os.listdir(image_dir) if f.endswith('.jpg')]\n","\n","        # Filter out images without corresponding json files\n","        self.image_files = [f for f in self.image_files if os.path.exists(os.path.join(self.annotation_dir, f.replace('.jpg', '.json')))]\n","\n","        self.image_files = self.image_files[:num_samples]  # Limit to num_samples\n","\n","        self.annotations = self.load_annotations(self.image_files)\n","\n","    def load_annotations(self, image_files):\n","        annotations = []\n","        def load_json(img_name):\n","            annotation_path = os.path.join(self.annotation_dir, img_name.replace('.jpg', '.json'))\n","            with open(annotation_path) as f:\n","                return json.load(f)\n","\n","        with concurrent.futures.ThreadPoolExecutor() as executor:\n","            results = list(executor.map(load_json, image_files))\n","\n","        return results\n","\n","    def __len__(self):\n","        return len(self.image_files)\n","\n","    def __getitem__(self, idx):\n","        img_name = self.image_files[idx]\n","        img_path = os.path.join(self.image_dir, img_name)\n","        image = Image.open(img_path).convert(\"RGB\")\n","\n","        annotation = self.annotations[idx]\n","        boxes = annotation['annotations']['drawing']\n","\n","        # Print raw annotation for debugging\n","        # print(f'Raw annotation for {img_name}: {annotation}')\n","\n","        if annotation['annotations']['class'] == 0:\n","            # If class is 0, there are no bounding boxes\n","            boxes = torch.empty((0, 4), dtype=torch.float32)\n","            labels = torch.tensor([], dtype=torch.int64)\n","        else:\n","            # Convert points to [x_min, y_min, x_max, y_max] format\n","            valid_boxes = []\n","            for box in boxes:\n","                x_min = min(point[0] for point in box)\n","                y_min = min(point[1] for point in box)\n","                x_max = max(point[0] for point in box)\n","                y_max = max(point[1] for point in box)\n","                if x_max > x_min and y_max > y_min:\n","                    valid_boxes.append([x_min, y_min, x_max, y_max])\n","\n","            boxes = torch.tensor(valid_boxes, dtype=torch.float32)\n","            labels = torch.tensor([annotation['annotations']['class']] * len(boxes), dtype=torch.int64)\n","\n","        target = {}\n","        target['boxes'] = boxes\n","        target['labels'] = labels\n","\n","        # Print diagnostic information\n","        # print(f'Image {img_name} has {len(boxes)} valid boxes')\n","\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        return image, target\n","\n","transform = transforms.Compose([\n","    transforms.ToTensor(),\n","])\n","\n","image_dir = '/content/drive/MyDrive/RIPcurrent/TS_1.image_1.Haeundae_3.PARA2'\n","annotation_dir = '/content/drive/MyDrive/RIPcurrent/TL_1.JSON_1.Haeundae_3.PARA2'\n","\n","dataset = RipCurrentDataset(image_dir=image_dir,\n","                            annotation_dir=annotation_dir,\n","                            transform=transform,\n","                            num_samples=20000)\n","\n","def collate_fn(batch):\n","    # Filter out samples with no bounding boxes\n","    filtered_batch = [sample for sample in batch if sample[1]['boxes'].size(0) > 0]\n","    filtered_out_count = len(batch) - len(filtered_batch)\n","    if filtered_out_count > 0:\n","        print(f'Filtered out {filtered_out_count} samples with no valid bounding boxes')\n","    if len(filtered_batch) == 0:\n","        return [], []\n","    return tuple(zip(*filtered_batch))\n","\n","dataloader = DataLoader(dataset, batch_size=4, shuffle=True, num_workers=2, collate_fn=collate_fn)\n"],"metadata":{"id":"yEBHOh7NpeNS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"CcjB_GzI3b4-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Dataset 최신 이안류있는 이미지 없는 이미지를 분리\n","import os\n","import json\n","import concurrent.futures\n","from PIL import Image\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","import torchvision.transforms as transforms\n","\n","class RipCurrentDataset(Dataset):\n","    def __init__(self, image_dir, annotation_dir, transform=None, num_samples=20000, include_negative=True):\n","        self.image_dir = image_dir\n","        self.annotation_dir = annotation_dir\n","        self.transform = transform\n","        self.include_negative = include_negative\n","\n","        self.image_files = [f for f in os.listdir(image_dir) if f.endswith('.jpg')]\n","\n","        # Filter out images without corresponding json files\n","        self.image_files = [f for f in self.image_files if os.path.exists(os.path.join(self.annotation_dir, f.replace('.jpg', '.json')))]\n","\n","        self.image_files = self.image_files[:num_samples]  # Limit to num_samples\n","\n","        self.annotations = self.load_annotations(self.image_files)\n","\n","    def load_annotations(self, image_files):\n","        annotations = []\n","        def load_json(img_name):\n","            annotation_path = os.path.join(self.annotation_dir, img_name.replace('.jpg', '.json'))\n","            with open(annotation_path) as f:\n","                return json.load(f)\n","\n","        with concurrent.futures.ThreadPoolExecutor() as executor:\n","            results = list(executor.map(load_json, image_files))\n","\n","        return results\n","\n","    def __len__(self):\n","        return len(self.image_files)\n","\n","    def __getitem__(self, idx):\n","        img_name = self.image_files[idx]\n","        img_path = os.path.join(self.image_dir, img_name)\n","        image = Image.open(img_path).convert(\"RGB\")\n","\n","        annotation = self.annotations[idx]\n","        boxes = annotation['annotations']['drawing']\n","\n","        if annotation['annotations']['class'] == 0:\n","            # If class is 0, there are no bounding boxes\n","            boxes = torch.empty((0, 4), dtype=torch.float32)\n","            labels = torch.tensor([], dtype=torch.int64)\n","        else:\n","            # Convert points to [x_min, y_min, x_max, y_max] format\n","            valid_boxes = []\n","            for box in boxes:\n","                x_min = min(point[0] for point in box)\n","                y_min = min(point[1] for point in box)\n","                x_max = max(point[0] for point in box)\n","                y_max = max(point[1] for point in box)\n","                if x_max > x_min and y_max > y_min:\n","                    valid_boxes.append([x_min, y_min, x_max, y_max])\n","\n","            boxes = torch.tensor(valid_boxes, dtype=torch.float32)\n","            labels = torch.tensor([annotation['annotations']['class']] * len(boxes), dtype=torch.int64)\n","\n","        target = {}\n","        target['boxes'] = boxes\n","        target['labels'] = labels\n","\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        return image, target\n","\n","transform = transforms.Compose([\n","    transforms.ToTensor(),\n","])\n","\n","image_dir = '/content/drive/MyDrive/RIPcurrent/TS_1.image_1.Haeundae_3.PARA2'\n","annotation_dir = '/content/drive/MyDrive/RIPcurrent/TL_1.JSON_1.Haeundae_3.PARA2'\n","\n","dataset = RipCurrentDataset(image_dir=image_dir,\n","                            annotation_dir=annotation_dir,\n","                            transform=transform,\n","                            num_samples=20000,\n","                            include_negative=True)\n","\n","def collate_fn(batch):\n","    # Separate samples with and without bounding boxes\n","    positive_samples = [sample for sample in batch if sample[1]['boxes'].size(0) > 0]\n","    negative_samples = [sample for sample in batch if sample[1]['boxes'].size(0) == 0]\n","\n","    # Ensure all samples are processed\n","    combined_batch = positive_samples + negative_samples\n","\n","    return combined_batch\n","\n","dataloader = DataLoader(dataset, batch_size=4, shuffle=True, num_workers=2, collate_fn=collate_fn)\n"],"metadata":{"id":"zCRjFeBh23nf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"C-Ne8XyF24Rt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torchvision\n","from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n","\n","def get_model(num_classes):\n","    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n","    in_features = model.roi_heads.box_predictor.cls_score.in_features\n","    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n","    return model\n","\n","model = get_model(num_classes=2)"],"metadata":{"id":"lc6rHIZhpeLD","executionInfo":{"status":"ok","timestamp":1720317068055,"user_tz":-540,"elapsed":1720,"user":{"displayName":"양유석","userId":"08992042625290856644"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"af9c9713-84e3-43e3-b2dd-510607e0f48f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.optim as optim\n","import torchvision.transforms as T\n","import os\n","\n","# Define the device and move the model to the device\n","device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","model.to(device)\n","\n","# Define optimizer and learning rate scheduler\n","params = [p for p in model.parameters() if p.requires_grad]\n","optimizer = optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n","lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n","\n","# Load the checkpoint\n","checkpoint_path = '/content/drive/MyDrive/RIPcurrent/model_checkpoints/RCNN_checkpoint_epoch0708PARA2GLORY_10.pth'\n","if os.path.exists(checkpoint_path):\n","    checkpoint = torch.load(checkpoint_path, map_location=device)\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","    start_epoch = checkpoint['epoch']\n","    print(f'Checkpoint loaded: start_epoch = {start_epoch}, loss = {checkpoint[\"loss\"]:.4f}')\n","else:\n","    start_epoch = 0\n","    print('No checkpoint found. Training from scratch.')\n","\n","num_epochs = 5\n","save_dir = '/content/drive/MyDrive/RIPcurrent/model_checkpoints'\n","if not os.path.exists(save_dir):\n","    os.makedirs(save_dir)\n","\n","for epoch in range(start_epoch, start_epoch + num_epochs):\n","    model.train()\n","    i = 0\n","    epoch_losses = []\n","\n","    for images, targets in dataloader:\n","        if len(images) == 0:\n","            print(f'Skipping empty batch at step {i}')\n","            continue  # Skip empty batches\n","\n","        images = list(image.to(device) for image in images)\n","        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n","\n","        loss_dict = model(images, targets)\n","        losses = sum(loss for loss in loss_dict.values())\n","        epoch_losses.append(losses.item())\n","\n","        optimizer.zero_grad()\n","        losses.backward()\n","        optimizer.step()\n","\n","        if i % 10 == 0:\n","            print(f'Epoch [{epoch+1}/{start_epoch + num_epochs}], Step [{i}/{len(dataloader)}], Loss: {losses.item():.4f}')\n","        i += 1\n","\n","    lr_scheduler.step()\n","\n","    # Save the model and optimizer state\n","    if epoch_losses:\n","        avg_loss = sum(epoch_losses) / len(epoch_losses)\n","    else:\n","        avg_loss = float('inf')\n","\n","    checkpoint = {\n","        'epoch': epoch + 1,\n","        'model_state_dict': model.state_dict(),\n","        'optimizer_state_dict': optimizer.state_dict(),\n","        'loss': avg_loss,\n","    }\n","    torch.save(checkpoint, os.path.join(save_dir, f'RCNN_checkpoint_epoch0708PARA2GLORY_{epoch + 1}.pth'))\n","    print(f'Model saved at epoch {epoch + 1}, Loss: {avg_loss:.4f}')\n"],"metadata":{"id":"6Tr5anjYpeIp","colab":{"base_uri":"https://localhost:8080/","height":112},"outputId":"164e5338-b1d0-4c3a-e98e-f2c3bb02831f","executionInfo":{"status":"error","timestamp":1720316960055,"user_tz":-540,"elapsed":516,"user":{"displayName":"양유석","userId":"08992042625290856644"}}},"execution_count":null,"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"invalid decimal literal (<ipython-input-10-79ebbb38bb9e>, line 1)","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-10-79ebbb38bb9e>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    133import torch\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid decimal literal\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"cJrKBrWx96LN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Training loop 최신 이안류 없는 경우도 학습하도록\n","import torch\n","import torch.optim as optim\n","import torchvision.transforms as T\n","import os\n","\n","# Define the device and move the model to the device\n","device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","model.to(device)\n","\n","# Define optimizer and learning rate scheduler\n","params = [p for p in model.parameters() if p.requires_grad]\n","optimizer = optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n","lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n","\n","# Load the checkpoint\n","checkpoint_path = '/content/drive/MyDrive/RIPcurrent/model_checkpoints/RCNN_checkpoint_epoch0708PARA2GLORY_10.pth'\n","if os.path.exists(checkpoint_path):\n","    checkpoint = torch.load(checkpoint_path, map_location=device)\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","    start_epoch = checkpoint['epoch']\n","    print(f'Checkpoint loaded: start_epoch = {start_epoch}, loss = {checkpoint[\"loss\"]:.4f}')\n","else:\n","    start_epoch = 0\n","    print('No checkpoint found. Training from scratch.')\n","\n","num_epochs = 5\n","save_dir = '/content/drive/MyDrive/RIPcurrent/model_checkpoints'\n","if not os.path.exists(save_dir):\n","    os.makedirs(save_dir)\n","\n","# Define DataLoader with collate_fn\n","dataloader = DataLoader(dataset, batch_size=4, shuffle=True, num_workers=2, collate_fn=collate_fn)\n","\n","# Training loop\n","for epoch in range(start_epoch, start_epoch + num_epochs):\n","    model.train()\n","    i = 0\n","    epoch_losses = []\n","\n","    for combined_batch in dataloader:\n","        if len(combined_batch) == 0:\n","            print(f'Skipping empty batch at step {i}')\n","            continue  # Skip empty batches\n","\n","        images, targets = zip(*combined_batch)\n","        images = list(image.to(device) for image in images)\n","        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n","\n","        # Forward pass\n","        loss_dict = model(images, targets)\n","        losses = sum(loss for loss in loss_dict.values())\n","        epoch_losses.append(losses.item())\n","\n","        # Backward pass\n","        optimizer.zero_grad()\n","        losses.backward()\n","        optimizer.step()\n","\n","        if i % 10 == 0:\n","            print(f'Epoch [{epoch+1}/{start_epoch + num_epochs}], Step [{i}/{len(dataloader)}], Loss: {losses.item():.4f}')\n","        i += 1\n","\n","    lr_scheduler.step()\n","\n","    # Save the model and optimizer state\n","    if epoch_losses:\n","        avg_loss = sum(epoch_losses) / len(epoch_losses)\n","    else:\n","        avg_loss = float('inf')\n","\n","    checkpoint = {\n","        'epoch': epoch + 1,\n","        'model_state_dict': model.state_dict(),\n","        'optimizer_state_dict': optimizer.state_dict(),\n","        'loss': avg_loss,\n","    }\n","    torch.save(checkpoint, os.path.join(save_dir, f'RCNN_checkpoint_epoch0708PARA2GLORY_{epoch + 1}.pth'))\n","    print(f'Model saved at epoch {epoch + 1}, Loss: {avg_loss:.4f}')\n"],"metadata":{"id":"YJ61tAk--C8S","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1720346891710,"user_tz":-540,"elapsed":29774465,"user":{"displayName":"양유석","userId":"08992042625290856644"}},"outputId":"2260a39d-c4f7-435b-cd00-84a86a6beba4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Checkpoint loaded: start_epoch = 10, loss = 0.2339\n","Epoch [11/15], Step [0/5000], Loss: 0.2089\n","Epoch [11/15], Step [10/5000], Loss: 0.2011\n","Epoch [11/15], Step [20/5000], Loss: 0.2372\n","Epoch [11/15], Step [30/5000], Loss: 0.1982\n","Epoch [11/15], Step [40/5000], Loss: 0.2154\n","Epoch [11/15], Step [50/5000], Loss: 0.2229\n","Epoch [11/15], Step [60/5000], Loss: 0.2596\n","Epoch [11/15], Step [70/5000], Loss: 0.1954\n","Epoch [11/15], Step [80/5000], Loss: 0.1807\n","Epoch [11/15], Step [90/5000], Loss: 0.1459\n","Epoch [11/15], Step [100/5000], Loss: 0.1580\n","Epoch [11/15], Step [110/5000], Loss: 0.1029\n","Epoch [11/15], Step [120/5000], Loss: 0.2213\n","Epoch [11/15], Step [130/5000], Loss: 0.2083\n","Epoch [11/15], Step [140/5000], Loss: 0.1495\n","Epoch [11/15], Step [150/5000], Loss: 0.2497\n","Epoch [11/15], Step [160/5000], Loss: 0.1587\n","Epoch [11/15], Step [170/5000], Loss: 0.2407\n","Epoch [11/15], Step [180/5000], Loss: 0.1962\n","Epoch [11/15], Step [190/5000], Loss: 0.2459\n","Epoch [11/15], Step [200/5000], Loss: 0.3012\n","Epoch [11/15], Step [210/5000], Loss: 0.1539\n","Epoch [11/15], Step [220/5000], Loss: 0.1250\n","Epoch [11/15], Step [230/5000], Loss: 0.3364\n","Epoch [11/15], Step [240/5000], Loss: 0.2768\n","Epoch [11/15], Step [250/5000], Loss: 0.1573\n","Epoch [11/15], Step [260/5000], Loss: 0.1441\n","Epoch [11/15], Step [270/5000], Loss: 0.1289\n","Epoch [11/15], Step [280/5000], Loss: 0.1593\n","Epoch [11/15], Step [290/5000], Loss: 0.1482\n","Epoch [11/15], Step [300/5000], Loss: 0.2904\n","Epoch [11/15], Step [310/5000], Loss: 0.1209\n","Epoch [11/15], Step [320/5000], Loss: 0.1340\n","Epoch [11/15], Step [330/5000], Loss: 0.1181\n","Epoch [11/15], Step [340/5000], Loss: 0.1555\n","Epoch [11/15], Step [350/5000], Loss: 0.2269\n","Epoch [11/15], Step [360/5000], Loss: 0.2497\n","Epoch [11/15], Step [370/5000], Loss: 0.1696\n","Epoch [11/15], Step [380/5000], Loss: 0.1345\n","Epoch [11/15], Step [390/5000], Loss: 0.2040\n","Epoch [11/15], Step [400/5000], Loss: 0.1747\n","Epoch [11/15], Step [410/5000], Loss: 0.1042\n","Epoch [11/15], Step [420/5000], Loss: 0.2830\n","Epoch [11/15], Step [430/5000], Loss: 0.1972\n","Epoch [11/15], Step [440/5000], Loss: 0.2541\n","Epoch [11/15], Step [450/5000], Loss: 0.1665\n","Epoch [11/15], Step [460/5000], Loss: 0.2471\n","Epoch [11/15], Step [470/5000], Loss: 0.0273\n","Epoch [11/15], Step [480/5000], Loss: 0.1615\n","Epoch [11/15], Step [490/5000], Loss: 0.1653\n","Epoch [11/15], Step [500/5000], Loss: 0.1083\n","Epoch [11/15], Step [510/5000], Loss: 0.0516\n","Epoch [11/15], Step [520/5000], Loss: 0.0630\n","Epoch [11/15], Step [530/5000], Loss: 0.0322\n","Epoch [11/15], Step [540/5000], Loss: 0.2039\n","Epoch [11/15], Step [550/5000], Loss: 0.1680\n","Epoch [11/15], Step [560/5000], Loss: 0.1557\n","Epoch [11/15], Step [570/5000], Loss: 0.1941\n","Epoch [11/15], Step [580/5000], Loss: 0.1722\n","Epoch [11/15], Step [590/5000], Loss: 0.2034\n","Epoch [11/15], Step [600/5000], Loss: 0.2019\n","Epoch [11/15], Step [610/5000], Loss: 0.2271\n","Epoch [11/15], Step [620/5000], Loss: 0.1193\n","Epoch [11/15], Step [630/5000], Loss: 0.1484\n","Epoch [11/15], Step [640/5000], Loss: 0.1350\n","Epoch [11/15], Step [650/5000], Loss: 0.2013\n","Epoch [11/15], Step [660/5000], Loss: 0.2053\n","Epoch [11/15], Step [670/5000], Loss: 0.1631\n","Epoch [11/15], Step [680/5000], Loss: 0.1081\n","Epoch [11/15], Step [690/5000], Loss: 0.2882\n","Epoch [11/15], Step [700/5000], Loss: 0.1958\n","Epoch [11/15], Step [710/5000], Loss: 0.2336\n","Epoch [11/15], Step [720/5000], Loss: 0.0944\n","Epoch [11/15], Step [730/5000], Loss: 0.2256\n","Epoch [11/15], Step [740/5000], Loss: 0.2631\n","Epoch [11/15], Step [750/5000], Loss: 0.2380\n","Epoch [11/15], Step [760/5000], Loss: 0.2584\n","Epoch [11/15], Step [770/5000], Loss: 0.2154\n","Epoch [11/15], Step [780/5000], Loss: 0.1784\n","Epoch [11/15], Step [790/5000], Loss: 0.2258\n","Epoch [11/15], Step [800/5000], Loss: 0.1378\n","Epoch [11/15], Step [810/5000], Loss: 0.1797\n","Epoch [11/15], Step [820/5000], Loss: 0.1256\n","Epoch [11/15], Step [830/5000], Loss: 0.0873\n","Epoch [11/15], Step [840/5000], Loss: 0.0951\n","Epoch [11/15], Step [850/5000], Loss: 0.1710\n","Epoch [11/15], Step [860/5000], Loss: 0.0452\n","Epoch [11/15], Step [870/5000], Loss: 0.0732\n","Epoch [11/15], Step [880/5000], Loss: 0.1357\n","Epoch [11/15], Step [890/5000], Loss: 0.3763\n","Epoch [11/15], Step [900/5000], Loss: 0.1024\n","Epoch [11/15], Step [910/5000], Loss: 0.1086\n","Epoch [11/15], Step [920/5000], Loss: 0.2513\n","Epoch [11/15], Step [930/5000], Loss: 0.1730\n","Epoch [11/15], Step [940/5000], Loss: 0.1488\n","Epoch [11/15], Step [950/5000], Loss: 0.0641\n","Epoch [11/15], Step [960/5000], Loss: 0.1120\n","Epoch [11/15], Step [970/5000], Loss: 0.1983\n","Epoch [11/15], Step [980/5000], Loss: 0.3025\n","Epoch [11/15], Step [990/5000], Loss: 0.1825\n","Epoch [11/15], Step [1000/5000], Loss: 0.2116\n","Epoch [11/15], Step [1010/5000], Loss: 0.3090\n","Epoch [11/15], Step [1020/5000], Loss: 0.2526\n","Epoch [11/15], Step [1030/5000], Loss: 0.1154\n","Epoch [11/15], Step [1040/5000], Loss: 0.0922\n","Epoch [11/15], Step [1050/5000], Loss: 0.2908\n","Epoch [11/15], Step [1060/5000], Loss: 0.1346\n","Epoch [11/15], Step [1070/5000], Loss: 0.1031\n","Epoch [11/15], Step [1080/5000], Loss: 0.2551\n","Epoch [11/15], Step [1090/5000], Loss: 0.1422\n","Epoch [11/15], Step [1100/5000], Loss: 0.1880\n","Epoch [11/15], Step [1110/5000], Loss: 0.1522\n","Epoch [11/15], Step [1120/5000], Loss: 0.2047\n","Epoch [11/15], Step [1130/5000], Loss: 0.0534\n","Epoch [11/15], Step [1140/5000], Loss: 0.0866\n","Epoch [11/15], Step [1150/5000], Loss: 0.1162\n","Epoch [11/15], Step [1160/5000], Loss: 0.3055\n","Epoch [11/15], Step [1170/5000], Loss: 0.0537\n","Epoch [11/15], Step [1180/5000], Loss: 0.1041\n","Epoch [11/15], Step [1190/5000], Loss: 0.1862\n","Epoch [11/15], Step [1200/5000], Loss: 0.1376\n","Epoch [11/15], Step [1210/5000], Loss: 0.3425\n","Epoch [11/15], Step [1220/5000], Loss: 0.2236\n","Epoch [11/15], Step [1230/5000], Loss: 0.1301\n","Epoch [11/15], Step [1240/5000], Loss: 0.1427\n","Epoch [11/15], Step [1250/5000], Loss: 0.1671\n","Epoch [11/15], Step [1260/5000], Loss: 0.1724\n","Epoch [11/15], Step [1270/5000], Loss: 0.2567\n","Epoch [11/15], Step [1280/5000], Loss: 0.3852\n","Epoch [11/15], Step [1290/5000], Loss: 0.0364\n","Epoch [11/15], Step [1300/5000], Loss: 0.0904\n","Epoch [11/15], Step [1310/5000], Loss: 0.1340\n","Epoch [11/15], Step [1320/5000], Loss: 0.1090\n","Epoch [11/15], Step [1330/5000], Loss: 0.2317\n","Epoch [11/15], Step [1340/5000], Loss: 0.2103\n","Epoch [11/15], Step [1350/5000], Loss: 0.1033\n","Epoch [11/15], Step [1360/5000], Loss: 0.1934\n","Epoch [11/15], Step [1370/5000], Loss: 0.0431\n","Epoch [11/15], Step [1380/5000], Loss: 0.1746\n","Epoch [11/15], Step [1390/5000], Loss: 0.2292\n","Epoch [11/15], Step [1400/5000], Loss: 0.1054\n","Epoch [11/15], Step [1410/5000], Loss: 0.1861\n","Epoch [11/15], Step [1420/5000], Loss: 0.1277\n","Epoch [11/15], Step [1430/5000], Loss: 0.1430\n","Epoch [11/15], Step [1440/5000], Loss: 0.2247\n","Epoch [11/15], Step [1450/5000], Loss: 0.1041\n","Epoch [11/15], Step [1460/5000], Loss: 0.2088\n","Epoch [11/15], Step [1470/5000], Loss: 0.0960\n","Epoch [11/15], Step [1480/5000], Loss: 0.1262\n","Epoch [11/15], Step [1490/5000], Loss: 0.1542\n","Epoch [11/15], Step [1500/5000], Loss: 0.1635\n","Epoch [11/15], Step [1510/5000], Loss: 0.3559\n","Epoch [11/15], Step [1520/5000], Loss: 0.1828\n","Epoch [11/15], Step [1530/5000], Loss: 0.1712\n","Epoch [11/15], Step [1540/5000], Loss: 0.1278\n","Epoch [11/15], Step [1550/5000], Loss: 0.1852\n","Epoch [11/15], Step [1560/5000], Loss: 0.1125\n","Epoch [11/15], Step [1570/5000], Loss: 0.0980\n","Epoch [11/15], Step [1580/5000], Loss: 0.1568\n","Epoch [11/15], Step [1590/5000], Loss: 0.0676\n","Epoch [11/15], Step [1600/5000], Loss: 0.2984\n","Epoch [11/15], Step [1610/5000], Loss: 0.0859\n","Epoch [11/15], Step [1620/5000], Loss: 0.1801\n","Epoch [11/15], Step [1630/5000], Loss: 0.1262\n","Epoch [11/15], Step [1640/5000], Loss: 0.1436\n","Epoch [11/15], Step [1650/5000], Loss: 0.0926\n","Epoch [11/15], Step [1660/5000], Loss: 0.1654\n","Epoch [11/15], Step [1670/5000], Loss: 0.1455\n","Epoch [11/15], Step [1680/5000], Loss: 0.2857\n","Epoch [11/15], Step [1690/5000], Loss: 0.2633\n","Epoch [11/15], Step [1700/5000], Loss: 0.3403\n","Epoch [11/15], Step [1710/5000], Loss: 0.0883\n","Epoch [11/15], Step [1720/5000], Loss: 0.1702\n","Epoch [11/15], Step [1730/5000], Loss: 0.1924\n","Epoch [11/15], Step [1740/5000], Loss: 0.1158\n","Epoch [11/15], Step [1750/5000], Loss: 0.2847\n","Epoch [11/15], Step [1760/5000], Loss: 0.3246\n","Epoch [11/15], Step [1770/5000], Loss: 0.1491\n","Epoch [11/15], Step [1780/5000], Loss: 0.1864\n","Epoch [11/15], Step [1790/5000], Loss: 0.1822\n","Epoch [11/15], Step [1800/5000], Loss: 0.2096\n","Epoch [11/15], Step [1810/5000], Loss: 0.1008\n","Epoch [11/15], Step [1820/5000], Loss: 0.1563\n","Epoch [11/15], Step [1830/5000], Loss: 0.2031\n","Epoch [11/15], Step [1840/5000], Loss: 0.2620\n","Epoch [11/15], Step [1850/5000], Loss: 0.1230\n","Epoch [11/15], Step [1860/5000], Loss: 0.1906\n","Epoch [11/15], Step [1870/5000], Loss: 0.2902\n","Epoch [11/15], Step [1880/5000], Loss: 0.1610\n","Epoch [11/15], Step [1890/5000], Loss: 0.1545\n","Epoch [11/15], Step [1900/5000], Loss: 0.2326\n","Epoch [11/15], Step [1910/5000], Loss: 0.2680\n","Epoch [11/15], Step [1920/5000], Loss: 0.1802\n","Epoch [11/15], Step [1930/5000], Loss: 0.0998\n","Epoch [11/15], Step [1940/5000], Loss: 0.1356\n","Epoch [11/15], Step [1950/5000], Loss: 0.1068\n","Epoch [11/15], Step [1960/5000], Loss: 0.1249\n","Epoch [11/15], Step [1970/5000], Loss: 0.2320\n","Epoch [11/15], Step [1980/5000], Loss: 0.2486\n","Epoch [11/15], Step [1990/5000], Loss: 0.2359\n","Epoch [11/15], Step [2000/5000], Loss: 0.2085\n","Epoch [11/15], Step [2010/5000], Loss: 0.0818\n","Epoch [11/15], Step [2020/5000], Loss: 0.1200\n","Epoch [11/15], Step [2030/5000], Loss: 0.1720\n","Epoch [11/15], Step [2040/5000], Loss: 0.1705\n","Epoch [11/15], Step [2050/5000], Loss: 0.2665\n","Epoch [11/15], Step [2060/5000], Loss: 0.1337\n","Epoch [11/15], Step [2070/5000], Loss: 0.0589\n","Epoch [11/15], Step [2080/5000], Loss: 0.1032\n","Epoch [11/15], Step [2090/5000], Loss: 0.1895\n","Epoch [11/15], Step [2100/5000], Loss: 0.1970\n","Epoch [11/15], Step [2110/5000], Loss: 0.1423\n","Epoch [11/15], Step [2120/5000], Loss: 0.1233\n","Epoch [11/15], Step [2130/5000], Loss: 0.1790\n","Epoch [11/15], Step [2140/5000], Loss: 0.3358\n","Epoch [11/15], Step [2150/5000], Loss: 0.0569\n","Epoch [11/15], Step [2160/5000], Loss: 0.1845\n","Epoch [11/15], Step [2170/5000], Loss: 0.2185\n","Epoch [11/15], Step [2180/5000], Loss: 0.2097\n","Epoch [11/15], Step [2190/5000], Loss: 0.2779\n","Epoch [11/15], Step [2200/5000], Loss: 0.1409\n","Epoch [11/15], Step [2210/5000], Loss: 0.2727\n","Epoch [11/15], Step [2220/5000], Loss: 0.2812\n","Epoch [11/15], Step [2230/5000], Loss: 0.1464\n","Epoch [11/15], Step [2240/5000], Loss: 0.1796\n","Epoch [11/15], Step [2250/5000], Loss: 0.1608\n","Epoch [11/15], Step [2260/5000], Loss: 0.1080\n","Epoch [11/15], Step [2270/5000], Loss: 0.1830\n","Epoch [11/15], Step [2280/5000], Loss: 0.3113\n","Epoch [11/15], Step [2290/5000], Loss: 0.0997\n","Epoch [11/15], Step [2300/5000], Loss: 0.1018\n","Epoch [11/15], Step [2310/5000], Loss: 0.2276\n","Epoch [11/15], Step [2320/5000], Loss: 0.1825\n","Epoch [11/15], Step [2330/5000], Loss: 0.1060\n","Epoch [11/15], Step [2340/5000], Loss: 0.1422\n","Epoch [11/15], Step [2350/5000], Loss: 0.1751\n","Epoch [11/15], Step [2360/5000], Loss: 0.1648\n","Epoch [11/15], Step [2370/5000], Loss: 0.2741\n","Epoch [11/15], Step [2380/5000], Loss: 0.2032\n","Epoch [11/15], Step [2390/5000], Loss: 0.1434\n","Epoch [11/15], Step [2400/5000], Loss: 0.1699\n","Epoch [11/15], Step [2410/5000], Loss: 0.2743\n","Epoch [11/15], Step [2420/5000], Loss: 0.2468\n","Epoch [11/15], Step [2430/5000], Loss: 0.0674\n","Epoch [11/15], Step [2440/5000], Loss: 0.1802\n","Epoch [11/15], Step [2450/5000], Loss: 0.1553\n","Epoch [11/15], Step [2460/5000], Loss: 0.1685\n","Epoch [11/15], Step [2470/5000], Loss: 0.1357\n","Epoch [11/15], Step [2480/5000], Loss: 0.2650\n","Epoch [11/15], Step [2490/5000], Loss: 0.2955\n","Epoch [11/15], Step [2500/5000], Loss: 0.0884\n","Epoch [11/15], Step [2510/5000], Loss: 0.2486\n","Epoch [11/15], Step [2520/5000], Loss: 0.2585\n","Epoch [11/15], Step [2530/5000], Loss: 0.1553\n","Epoch [11/15], Step [2540/5000], Loss: 0.1173\n","Epoch [11/15], Step [2550/5000], Loss: 0.1560\n","Epoch [11/15], Step [2560/5000], Loss: 0.1923\n","Epoch [11/15], Step [2570/5000], Loss: 0.0565\n","Epoch [11/15], Step [2580/5000], Loss: 0.2545\n","Epoch [11/15], Step [2590/5000], Loss: 0.1703\n","Epoch [11/15], Step [2600/5000], Loss: 0.3350\n","Epoch [11/15], Step [2610/5000], Loss: 0.2141\n","Epoch [11/15], Step [2620/5000], Loss: 0.1129\n","Epoch [11/15], Step [2630/5000], Loss: 0.2449\n","Epoch [11/15], Step [2640/5000], Loss: 0.1789\n","Epoch [11/15], Step [2650/5000], Loss: 0.2700\n","Epoch [11/15], Step [2660/5000], Loss: 0.1881\n","Epoch [11/15], Step [2670/5000], Loss: 0.1213\n","Epoch [11/15], Step [2680/5000], Loss: 0.0814\n","Epoch [11/15], Step [2690/5000], Loss: 0.1371\n","Epoch [11/15], Step [2700/5000], Loss: 0.1202\n","Epoch [11/15], Step [2710/5000], Loss: 0.1538\n","Epoch [11/15], Step [2720/5000], Loss: 0.2572\n","Epoch [11/15], Step [2730/5000], Loss: 0.2248\n","Epoch [11/15], Step [2740/5000], Loss: 0.1070\n","Epoch [11/15], Step [2750/5000], Loss: 0.1626\n","Epoch [11/15], Step [2760/5000], Loss: 0.2198\n","Epoch [11/15], Step [2770/5000], Loss: 0.2594\n","Epoch [11/15], Step [2780/5000], Loss: 0.1392\n","Epoch [11/15], Step [2790/5000], Loss: 0.1260\n","Epoch [11/15], Step [2800/5000], Loss: 0.0630\n","Epoch [11/15], Step [2810/5000], Loss: 0.1986\n","Epoch [11/15], Step [2820/5000], Loss: 0.1289\n","Epoch [11/15], Step [2830/5000], Loss: 0.2228\n","Epoch [11/15], Step [2840/5000], Loss: 0.2108\n","Epoch [11/15], Step [2850/5000], Loss: 0.1471\n","Epoch [11/15], Step [2860/5000], Loss: 0.2082\n","Epoch [11/15], Step [2870/5000], Loss: 0.0836\n","Epoch [11/15], Step [2880/5000], Loss: 0.1418\n","Epoch [11/15], Step [2890/5000], Loss: 0.0548\n","Epoch [11/15], Step [2900/5000], Loss: 0.2053\n","Epoch [11/15], Step [2910/5000], Loss: 0.1218\n","Epoch [11/15], Step [2920/5000], Loss: 0.1061\n","Epoch [11/15], Step [2930/5000], Loss: 0.1964\n","Epoch [11/15], Step [2940/5000], Loss: 0.2735\n","Epoch [11/15], Step [2950/5000], Loss: 0.0660\n","Epoch [11/15], Step [2960/5000], Loss: 0.1577\n","Epoch [11/15], Step [2970/5000], Loss: 0.1865\n","Epoch [11/15], Step [2980/5000], Loss: 0.1966\n","Epoch [11/15], Step [2990/5000], Loss: 0.2366\n","Epoch [11/15], Step [3000/5000], Loss: 0.0572\n","Epoch [11/15], Step [3010/5000], Loss: 0.1186\n","Epoch [11/15], Step [3020/5000], Loss: 0.2500\n","Epoch [11/15], Step [3030/5000], Loss: 0.1640\n","Epoch [11/15], Step [3040/5000], Loss: 0.1697\n","Epoch [11/15], Step [3050/5000], Loss: 0.2190\n","Epoch [11/15], Step [3060/5000], Loss: 0.1453\n","Epoch [11/15], Step [3070/5000], Loss: 0.1583\n","Epoch [11/15], Step [3080/5000], Loss: 0.0627\n","Epoch [11/15], Step [3090/5000], Loss: 0.1510\n","Epoch [11/15], Step [3100/5000], Loss: 0.2615\n","Epoch [11/15], Step [3110/5000], Loss: 0.1132\n","Epoch [11/15], Step [3120/5000], Loss: 0.1632\n","Epoch [11/15], Step [3130/5000], Loss: 0.1217\n","Epoch [11/15], Step [3140/5000], Loss: 0.2565\n","Epoch [11/15], Step [3150/5000], Loss: 0.2198\n","Epoch [11/15], Step [3160/5000], Loss: 0.2994\n","Epoch [11/15], Step [3170/5000], Loss: 0.0963\n","Epoch [11/15], Step [3180/5000], Loss: 0.0326\n","Epoch [11/15], Step [3190/5000], Loss: 0.1244\n","Epoch [11/15], Step [3200/5000], Loss: 0.1896\n","Epoch [11/15], Step [3210/5000], Loss: 0.1511\n","Epoch [11/15], Step [3220/5000], Loss: 0.1386\n","Epoch [11/15], Step [3230/5000], Loss: 0.1346\n","Epoch [11/15], Step [3240/5000], Loss: 0.1667\n","Epoch [11/15], Step [3250/5000], Loss: 0.2377\n","Epoch [11/15], Step [3260/5000], Loss: 0.1538\n","Epoch [11/15], Step [3270/5000], Loss: 0.1573\n","Epoch [11/15], Step [3280/5000], Loss: 0.1827\n","Epoch [11/15], Step [3290/5000], Loss: 0.1617\n","Epoch [11/15], Step [3300/5000], Loss: 0.0870\n","Epoch [11/15], Step [3310/5000], Loss: 0.2155\n","Epoch [11/15], Step [3320/5000], Loss: 0.2612\n","Epoch [11/15], Step [3330/5000], Loss: 0.1287\n","Epoch [11/15], Step [3340/5000], Loss: 0.1147\n","Epoch [11/15], Step [3350/5000], Loss: 0.1289\n","Epoch [11/15], Step [3360/5000], Loss: 0.2205\n","Epoch [11/15], Step [3370/5000], Loss: 0.1702\n","Epoch [11/15], Step [3380/5000], Loss: 0.2365\n","Epoch [11/15], Step [3390/5000], Loss: 0.1953\n","Epoch [11/15], Step [3400/5000], Loss: 0.1853\n","Epoch [11/15], Step [3410/5000], Loss: 0.1075\n","Epoch [11/15], Step [3420/5000], Loss: 0.1006\n","Epoch [11/15], Step [3430/5000], Loss: 0.1419\n","Epoch [11/15], Step [3440/5000], Loss: 0.1101\n","Epoch [11/15], Step [3450/5000], Loss: 0.1222\n","Epoch [11/15], Step [3460/5000], Loss: 0.1880\n","Epoch [11/15], Step [3470/5000], Loss: 0.1135\n","Epoch [11/15], Step [3480/5000], Loss: 0.1980\n","Epoch [11/15], Step [3490/5000], Loss: 0.3154\n","Epoch [11/15], Step [3500/5000], Loss: 0.1968\n","Epoch [11/15], Step [3510/5000], Loss: 0.1356\n","Epoch [11/15], Step [3520/5000], Loss: 0.2122\n","Epoch [11/15], Step [3530/5000], Loss: 0.1547\n","Epoch [11/15], Step [3540/5000], Loss: 0.1043\n","Epoch [11/15], Step [3550/5000], Loss: 0.1038\n","Epoch [11/15], Step [3560/5000], Loss: 0.1960\n","Epoch [11/15], Step [3570/5000], Loss: 0.3105\n","Epoch [11/15], Step [3580/5000], Loss: 0.0812\n","Epoch [11/15], Step [3590/5000], Loss: 0.1731\n","Epoch [11/15], Step [3600/5000], Loss: 0.1275\n","Epoch [11/15], Step [3610/5000], Loss: 0.2287\n","Epoch [11/15], Step [3620/5000], Loss: 0.2005\n","Epoch [11/15], Step [3630/5000], Loss: 0.0675\n","Epoch [11/15], Step [3640/5000], Loss: 0.1098\n","Epoch [11/15], Step [3650/5000], Loss: 0.3063\n","Epoch [11/15], Step [3660/5000], Loss: 0.2453\n","Epoch [11/15], Step [3670/5000], Loss: 0.1372\n","Epoch [11/15], Step [3680/5000], Loss: 0.1805\n","Epoch [11/15], Step [3690/5000], Loss: 0.1998\n","Epoch [11/15], Step [3700/5000], Loss: 0.1438\n","Epoch [11/15], Step [3710/5000], Loss: 0.1803\n","Epoch [11/15], Step [3720/5000], Loss: 0.0620\n","Epoch [11/15], Step [3730/5000], Loss: 0.0608\n","Epoch [11/15], Step [3740/5000], Loss: 0.0665\n","Epoch [11/15], Step [3750/5000], Loss: 0.1278\n","Epoch [11/15], Step [3760/5000], Loss: 0.2373\n","Epoch [11/15], Step [3770/5000], Loss: 0.1694\n","Epoch [11/15], Step [3780/5000], Loss: 0.2384\n","Epoch [11/15], Step [3790/5000], Loss: 0.1948\n","Epoch [11/15], Step [3800/5000], Loss: 0.1574\n","Epoch [11/15], Step [3810/5000], Loss: 0.1479\n","Epoch [11/15], Step [3820/5000], Loss: 0.2107\n","Epoch [11/15], Step [3830/5000], Loss: 0.3168\n","Epoch [11/15], Step [3840/5000], Loss: 0.2786\n","Epoch [11/15], Step [3850/5000], Loss: 0.1287\n","Epoch [11/15], Step [3860/5000], Loss: 0.1467\n","Epoch [11/15], Step [3870/5000], Loss: 0.2152\n","Epoch [11/15], Step [3880/5000], Loss: 0.0350\n","Epoch [11/15], Step [3890/5000], Loss: 0.2250\n","Epoch [11/15], Step [3900/5000], Loss: 0.1992\n","Epoch [11/15], Step [3910/5000], Loss: 0.2538\n","Epoch [11/15], Step [3920/5000], Loss: 0.2113\n","Epoch [11/15], Step [3930/5000], Loss: 0.1874\n","Epoch [11/15], Step [3940/5000], Loss: 0.1117\n","Epoch [11/15], Step [3950/5000], Loss: 0.2407\n","Epoch [11/15], Step [3960/5000], Loss: 0.2916\n","Epoch [11/15], Step [3970/5000], Loss: 0.1981\n","Epoch [11/15], Step [3980/5000], Loss: 0.2126\n","Epoch [11/15], Step [3990/5000], Loss: 0.1205\n","Epoch [11/15], Step [4000/5000], Loss: 0.0823\n","Epoch [11/15], Step [4010/5000], Loss: 0.1178\n","Epoch [11/15], Step [4020/5000], Loss: 0.2379\n","Epoch [11/15], Step [4030/5000], Loss: 0.2605\n","Epoch [11/15], Step [4040/5000], Loss: 0.1772\n","Epoch [11/15], Step [4050/5000], Loss: 0.2204\n","Epoch [11/15], Step [4060/5000], Loss: 0.1118\n","Epoch [11/15], Step [4070/5000], Loss: 0.1881\n","Epoch [11/15], Step [4080/5000], Loss: 0.2842\n","Epoch [11/15], Step [4090/5000], Loss: 0.2972\n","Epoch [11/15], Step [4100/5000], Loss: 0.1267\n","Epoch [11/15], Step [4110/5000], Loss: 0.0977\n","Epoch [11/15], Step [4120/5000], Loss: 0.1797\n","Epoch [11/15], Step [4130/5000], Loss: 0.3247\n","Epoch [11/15], Step [4140/5000], Loss: 0.1933\n","Epoch [11/15], Step [4150/5000], Loss: 0.1355\n","Epoch [11/15], Step [4160/5000], Loss: 0.1354\n","Epoch [11/15], Step [4170/5000], Loss: 0.1654\n","Epoch [11/15], Step [4180/5000], Loss: 0.0900\n","Epoch [11/15], Step [4190/5000], Loss: 0.1386\n","Epoch [11/15], Step [4200/5000], Loss: 0.1219\n","Epoch [11/15], Step [4210/5000], Loss: 0.2597\n","Epoch [11/15], Step [4220/5000], Loss: 0.1196\n","Epoch [11/15], Step [4230/5000], Loss: 0.1182\n","Epoch [11/15], Step [4240/5000], Loss: 0.0873\n","Epoch [11/15], Step [4250/5000], Loss: 0.2163\n","Epoch [11/15], Step [4260/5000], Loss: 0.1938\n","Epoch [11/15], Step [4270/5000], Loss: 0.1501\n","Epoch [11/15], Step [4280/5000], Loss: 0.1365\n","Epoch [11/15], Step [4290/5000], Loss: 0.1503\n","Epoch [11/15], Step [4300/5000], Loss: 0.1729\n","Epoch [11/15], Step [4310/5000], Loss: 0.1056\n","Epoch [11/15], Step [4320/5000], Loss: 0.1427\n","Epoch [11/15], Step [4330/5000], Loss: 0.2857\n","Epoch [11/15], Step [4340/5000], Loss: 0.2278\n","Epoch [11/15], Step [4350/5000], Loss: 0.0919\n","Epoch [11/15], Step [4360/5000], Loss: 0.1941\n","Epoch [11/15], Step [4370/5000], Loss: 0.1944\n","Epoch [11/15], Step [4380/5000], Loss: 0.0188\n","Epoch [11/15], Step [4390/5000], Loss: 0.2703\n","Epoch [11/15], Step [4400/5000], Loss: 0.2606\n","Epoch [11/15], Step [4410/5000], Loss: 0.1888\n","Epoch [11/15], Step [4420/5000], Loss: 0.2067\n","Epoch [11/15], Step [4430/5000], Loss: 0.1861\n","Epoch [11/15], Step [4440/5000], Loss: 0.1980\n","Epoch [11/15], Step [4450/5000], Loss: 0.1749\n","Epoch [11/15], Step [4460/5000], Loss: 0.3310\n","Epoch [11/15], Step [4470/5000], Loss: 0.1362\n","Epoch [11/15], Step [4480/5000], Loss: 0.2771\n","Epoch [11/15], Step [4490/5000], Loss: 0.0740\n","Epoch [11/15], Step [4500/5000], Loss: 0.1728\n","Epoch [11/15], Step [4510/5000], Loss: 0.1553\n","Epoch [11/15], Step [4520/5000], Loss: 0.1490\n","Epoch [11/15], Step [4530/5000], Loss: 0.2072\n","Epoch [11/15], Step [4540/5000], Loss: 0.2136\n","Epoch [11/15], Step [4550/5000], Loss: 0.1732\n","Epoch [11/15], Step [4560/5000], Loss: 0.1853\n","Epoch [11/15], Step [4570/5000], Loss: 0.0246\n","Epoch [11/15], Step [4580/5000], Loss: 0.2176\n","Epoch [11/15], Step [4590/5000], Loss: 0.1258\n","Epoch [11/15], Step [4600/5000], Loss: 0.1639\n","Epoch [11/15], Step [4610/5000], Loss: 0.2054\n","Epoch [11/15], Step [4620/5000], Loss: 0.1793\n","Epoch [11/15], Step [4630/5000], Loss: 0.3925\n","Epoch [11/15], Step [4640/5000], Loss: 0.1707\n","Epoch [11/15], Step [4650/5000], Loss: 0.2265\n","Epoch [11/15], Step [4660/5000], Loss: 0.0682\n","Epoch [11/15], Step [4670/5000], Loss: 0.1862\n","Epoch [11/15], Step [4680/5000], Loss: 0.1628\n","Epoch [11/15], Step [4690/5000], Loss: 0.1302\n","Epoch [11/15], Step [4700/5000], Loss: 0.2196\n","Epoch [11/15], Step [4710/5000], Loss: 0.2373\n","Epoch [11/15], Step [4720/5000], Loss: 0.1560\n","Epoch [11/15], Step [4730/5000], Loss: 0.1599\n","Epoch [11/15], Step [4740/5000], Loss: 0.2053\n","Epoch [11/15], Step [4750/5000], Loss: 0.0980\n","Epoch [11/15], Step [4760/5000], Loss: 0.1060\n","Epoch [11/15], Step [4770/5000], Loss: 0.0698\n","Epoch [11/15], Step [4780/5000], Loss: 0.2290\n","Epoch [11/15], Step [4790/5000], Loss: 0.3079\n","Epoch [11/15], Step [4800/5000], Loss: 0.2282\n","Epoch [11/15], Step [4810/5000], Loss: 0.3210\n","Epoch [11/15], Step [4820/5000], Loss: 0.2058\n","Epoch [11/15], Step [4830/5000], Loss: 0.2624\n","Epoch [11/15], Step [4840/5000], Loss: 0.1790\n","Epoch [11/15], Step [4850/5000], Loss: 0.1888\n","Epoch [11/15], Step [4860/5000], Loss: 0.1543\n","Epoch [11/15], Step [4870/5000], Loss: 0.1777\n","Epoch [11/15], Step [4880/5000], Loss: 0.0881\n","Epoch [11/15], Step [4890/5000], Loss: 0.4756\n","Epoch [11/15], Step [4900/5000], Loss: 0.2542\n","Epoch [11/15], Step [4910/5000], Loss: 0.1896\n","Epoch [11/15], Step [4920/5000], Loss: 0.1464\n","Epoch [11/15], Step [4930/5000], Loss: 0.1219\n","Epoch [11/15], Step [4940/5000], Loss: 0.2588\n","Epoch [11/15], Step [4950/5000], Loss: 0.1017\n","Epoch [11/15], Step [4960/5000], Loss: 0.1915\n","Epoch [11/15], Step [4970/5000], Loss: 0.0640\n","Epoch [11/15], Step [4980/5000], Loss: 0.2181\n","Epoch [11/15], Step [4990/5000], Loss: 0.1695\n","Model saved at epoch 11, Loss: 0.1827\n","Epoch [12/15], Step [0/5000], Loss: 0.1338\n","Epoch [12/15], Step [10/5000], Loss: 0.1518\n","Epoch [12/15], Step [20/5000], Loss: 0.2288\n","Epoch [12/15], Step [30/5000], Loss: 0.2159\n","Epoch [12/15], Step [40/5000], Loss: 0.0926\n","Epoch [12/15], Step [50/5000], Loss: 0.2613\n","Epoch [12/15], Step [60/5000], Loss: 0.1915\n","Epoch [12/15], Step [70/5000], Loss: 0.2867\n","Epoch [12/15], Step [80/5000], Loss: 0.1636\n","Epoch [12/15], Step [90/5000], Loss: 0.2836\n","Epoch [12/15], Step [100/5000], Loss: 0.1938\n","Epoch [12/15], Step [110/5000], Loss: 0.3067\n","Epoch [12/15], Step [120/5000], Loss: 0.2232\n","Epoch [12/15], Step [130/5000], Loss: 0.2659\n","Epoch [12/15], Step [140/5000], Loss: 0.2267\n","Epoch [12/15], Step [150/5000], Loss: 0.1166\n","Epoch [12/15], Step [160/5000], Loss: 0.1831\n","Epoch [12/15], Step [170/5000], Loss: 0.1769\n","Epoch [12/15], Step [180/5000], Loss: 0.2318\n","Epoch [12/15], Step [190/5000], Loss: 0.2621\n","Epoch [12/15], Step [200/5000], Loss: 0.2093\n","Epoch [12/15], Step [210/5000], Loss: 0.1200\n","Epoch [12/15], Step [220/5000], Loss: 0.1118\n","Epoch [12/15], Step [230/5000], Loss: 0.2372\n","Epoch [12/15], Step [240/5000], Loss: 0.2091\n","Epoch [12/15], Step [250/5000], Loss: 0.1300\n","Epoch [12/15], Step [260/5000], Loss: 0.1574\n","Epoch [12/15], Step [270/5000], Loss: 0.1628\n","Epoch [12/15], Step [280/5000], Loss: 0.1119\n","Epoch [12/15], Step [290/5000], Loss: 0.1451\n","Epoch [12/15], Step [300/5000], Loss: 0.1068\n","Epoch [12/15], Step [310/5000], Loss: 0.2179\n","Epoch [12/15], Step [320/5000], Loss: 0.1694\n","Epoch [12/15], Step [330/5000], Loss: 0.2711\n","Epoch [12/15], Step [340/5000], Loss: 0.2458\n","Epoch [12/15], Step [350/5000], Loss: 0.1238\n","Epoch [12/15], Step [360/5000], Loss: 0.1880\n","Epoch [12/15], Step [370/5000], Loss: 0.1587\n","Epoch [12/15], Step [380/5000], Loss: 0.0774\n","Epoch [12/15], Step [390/5000], Loss: 0.2374\n","Epoch [12/15], Step [400/5000], Loss: 0.2174\n","Epoch [12/15], Step [410/5000], Loss: 0.1884\n","Epoch [12/15], Step [420/5000], Loss: 0.2673\n","Epoch [12/15], Step [430/5000], Loss: 0.1529\n","Epoch [12/15], Step [440/5000], Loss: 0.1624\n","Epoch [12/15], Step [450/5000], Loss: 0.0699\n","Epoch [12/15], Step [460/5000], Loss: 0.1067\n","Epoch [12/15], Step [470/5000], Loss: 0.0886\n","Epoch [12/15], Step [480/5000], Loss: 0.1135\n","Epoch [12/15], Step [490/5000], Loss: 0.1177\n","Epoch [12/15], Step [500/5000], Loss: 0.0686\n","Epoch [12/15], Step [510/5000], Loss: 0.1130\n","Epoch [12/15], Step [520/5000], Loss: 0.1556\n","Epoch [12/15], Step [530/5000], Loss: 0.0965\n","Epoch [12/15], Step [540/5000], Loss: 0.1724\n","Epoch [12/15], Step [550/5000], Loss: 0.1140\n","Epoch [12/15], Step [560/5000], Loss: 0.2741\n","Epoch [12/15], Step [570/5000], Loss: 0.0819\n","Epoch [12/15], Step [580/5000], Loss: 0.1401\n","Epoch [12/15], Step [590/5000], Loss: 0.2819\n","Epoch [12/15], Step [600/5000], Loss: 0.1191\n","Epoch [12/15], Step [610/5000], Loss: 0.2650\n","Epoch [12/15], Step [620/5000], Loss: 0.2237\n","Epoch [12/15], Step [630/5000], Loss: 0.0633\n","Epoch [12/15], Step [640/5000], Loss: 0.1887\n","Epoch [12/15], Step [650/5000], Loss: 0.1747\n","Epoch [12/15], Step [660/5000], Loss: 0.1771\n","Epoch [12/15], Step [670/5000], Loss: 0.1571\n","Epoch [12/15], Step [680/5000], Loss: 0.2685\n","Epoch [12/15], Step [690/5000], Loss: 0.2120\n","Epoch [12/15], Step [700/5000], Loss: 0.1225\n","Epoch [12/15], Step [710/5000], Loss: 0.3010\n","Epoch [12/15], Step [720/5000], Loss: 0.2028\n","Epoch [12/15], Step [730/5000], Loss: 0.1055\n","Epoch [12/15], Step [740/5000], Loss: 0.0696\n","Epoch [12/15], Step [750/5000], Loss: 0.0880\n","Epoch [12/15], Step [760/5000], Loss: 0.0891\n","Epoch [12/15], Step [770/5000], Loss: 0.2357\n","Epoch [12/15], Step [780/5000], Loss: 0.1449\n","Epoch [12/15], Step [790/5000], Loss: 0.2053\n","Epoch [12/15], Step [800/5000], Loss: 0.1887\n","Epoch [12/15], Step [810/5000], Loss: 0.1703\n","Epoch [12/15], Step [820/5000], Loss: 0.1561\n","Epoch [12/15], Step [830/5000], Loss: 0.2345\n","Epoch [12/15], Step [840/5000], Loss: 0.1644\n","Epoch [12/15], Step [850/5000], Loss: 0.2039\n","Epoch [12/15], Step [860/5000], Loss: 0.1797\n","Epoch [12/15], Step [870/5000], Loss: 0.1489\n","Epoch [12/15], Step [880/5000], Loss: 0.1206\n","Epoch [12/15], Step [890/5000], Loss: 0.2871\n","Epoch [12/15], Step [900/5000], Loss: 0.1393\n","Epoch [12/15], Step [910/5000], Loss: 0.0649\n","Epoch [12/15], Step [920/5000], Loss: 0.1402\n","Epoch [12/15], Step [930/5000], Loss: 0.0269\n","Epoch [12/15], Step [940/5000], Loss: 0.1125\n","Epoch [12/15], Step [950/5000], Loss: 0.2777\n","Epoch [12/15], Step [960/5000], Loss: 0.1750\n","Epoch [12/15], Step [970/5000], Loss: 0.1420\n","Epoch [12/15], Step [980/5000], Loss: 0.0634\n","Epoch [12/15], Step [990/5000], Loss: 0.1452\n","Epoch [12/15], Step [1000/5000], Loss: 0.3012\n","Epoch [12/15], Step [1010/5000], Loss: 0.1574\n","Epoch [12/15], Step [1020/5000], Loss: 0.1553\n","Epoch [12/15], Step [1030/5000], Loss: 0.2348\n","Epoch [12/15], Step [1040/5000], Loss: 0.1451\n","Epoch [12/15], Step [1050/5000], Loss: 0.1376\n","Epoch [12/15], Step [1060/5000], Loss: 0.1736\n","Epoch [12/15], Step [1070/5000], Loss: 0.2790\n","Epoch [12/15], Step [1080/5000], Loss: 0.2060\n","Epoch [12/15], Step [1090/5000], Loss: 0.2364\n","Epoch [12/15], Step [1100/5000], Loss: 0.1701\n","Epoch [12/15], Step [1110/5000], Loss: 0.2105\n","Epoch [12/15], Step [1120/5000], Loss: 0.1942\n","Epoch [12/15], Step [1130/5000], Loss: 0.1771\n","Epoch [12/15], Step [1140/5000], Loss: 0.1794\n","Epoch [12/15], Step [1150/5000], Loss: 0.1325\n","Epoch [12/15], Step [1160/5000], Loss: 0.2085\n","Epoch [12/15], Step [1170/5000], Loss: 0.2592\n","Epoch [12/15], Step [1180/5000], Loss: 0.1679\n","Epoch [12/15], Step [1190/5000], Loss: 0.0708\n","Epoch [12/15], Step [1200/5000], Loss: 0.1940\n","Epoch [12/15], Step [1210/5000], Loss: 0.1359\n","Epoch [12/15], Step [1220/5000], Loss: 0.0970\n","Epoch [12/15], Step [1230/5000], Loss: 0.1660\n","Epoch [12/15], Step [1240/5000], Loss: 0.2354\n","Epoch [12/15], Step [1250/5000], Loss: 0.1956\n","Epoch [12/15], Step [1260/5000], Loss: 0.1883\n","Epoch [12/15], Step [1270/5000], Loss: 0.1248\n","Epoch [12/15], Step [1280/5000], Loss: 0.0930\n","Epoch [12/15], Step [1290/5000], Loss: 0.1191\n","Epoch [12/15], Step [1300/5000], Loss: 0.1836\n","Epoch [12/15], Step [1310/5000], Loss: 0.2109\n","Epoch [12/15], Step [1320/5000], Loss: 0.3927\n","Epoch [12/15], Step [1330/5000], Loss: 0.2636\n","Epoch [12/15], Step [1340/5000], Loss: 0.3278\n","Epoch [12/15], Step [1350/5000], Loss: 0.2397\n","Epoch [12/15], Step [1360/5000], Loss: 0.1934\n","Epoch [12/15], Step [1370/5000], Loss: 0.1917\n","Epoch [12/15], Step [1380/5000], Loss: 0.1490\n","Epoch [12/15], Step [1390/5000], Loss: 0.3074\n","Epoch [12/15], Step [1400/5000], Loss: 0.1485\n","Epoch [12/15], Step [1410/5000], Loss: 0.1755\n","Epoch [12/15], Step [1420/5000], Loss: 0.2929\n","Epoch [12/15], Step [1430/5000], Loss: 0.2140\n","Epoch [12/15], Step [1440/5000], Loss: 0.2976\n","Epoch [12/15], Step [1450/5000], Loss: 0.1635\n","Epoch [12/15], Step [1460/5000], Loss: 0.1187\n","Epoch [12/15], Step [1470/5000], Loss: 0.1410\n","Epoch [12/15], Step [1480/5000], Loss: 0.1617\n","Epoch [12/15], Step [1490/5000], Loss: 0.1964\n","Epoch [12/15], Step [1500/5000], Loss: 0.2285\n","Epoch [12/15], Step [1510/5000], Loss: 0.1126\n","Epoch [12/15], Step [1520/5000], Loss: 0.1702\n","Epoch [12/15], Step [1530/5000], Loss: 0.1752\n","Epoch [12/15], Step [1540/5000], Loss: 0.0822\n","Epoch [12/15], Step [1550/5000], Loss: 0.1127\n","Epoch [12/15], Step [1560/5000], Loss: 0.1362\n","Epoch [12/15], Step [1570/5000], Loss: 0.1182\n","Epoch [12/15], Step [1580/5000], Loss: 0.0661\n","Epoch [12/15], Step [1590/5000], Loss: 0.0764\n","Epoch [12/15], Step [1600/5000], Loss: 0.2398\n","Epoch [12/15], Step [1610/5000], Loss: 0.2447\n","Epoch [12/15], Step [1620/5000], Loss: 0.3510\n","Epoch [12/15], Step [1630/5000], Loss: 0.1590\n","Epoch [12/15], Step [1640/5000], Loss: 0.1025\n","Epoch [12/15], Step [1650/5000], Loss: 0.1380\n","Epoch [12/15], Step [1660/5000], Loss: 0.1828\n","Epoch [12/15], Step [1670/5000], Loss: 0.2239\n","Epoch [12/15], Step [1680/5000], Loss: 0.1386\n","Epoch [12/15], Step [1690/5000], Loss: 0.2655\n","Epoch [12/15], Step [1700/5000], Loss: 0.1358\n","Epoch [12/15], Step [1710/5000], Loss: 0.2051\n","Epoch [12/15], Step [1720/5000], Loss: 0.2565\n","Epoch [12/15], Step [1730/5000], Loss: 0.1366\n","Epoch [12/15], Step [1740/5000], Loss: 0.1145\n","Epoch [12/15], Step [1750/5000], Loss: 0.2256\n","Epoch [12/15], Step [1760/5000], Loss: 0.1247\n","Epoch [12/15], Step [1770/5000], Loss: 0.1398\n","Epoch [12/15], Step [1780/5000], Loss: 0.1377\n","Epoch [12/15], Step [1790/5000], Loss: 0.1815\n","Epoch [12/15], Step [1800/5000], Loss: 0.1747\n","Epoch [12/15], Step [1810/5000], Loss: 0.3638\n","Epoch [12/15], Step [1820/5000], Loss: 0.2359\n","Epoch [12/15], Step [1830/5000], Loss: 0.2284\n","Epoch [12/15], Step [1840/5000], Loss: 0.1764\n","Epoch [12/15], Step [1850/5000], Loss: 0.1977\n","Epoch [12/15], Step [1860/5000], Loss: 0.2106\n","Epoch [12/15], Step [1870/5000], Loss: 0.0808\n","Epoch [12/15], Step [1880/5000], Loss: 0.1550\n","Epoch [12/15], Step [1890/5000], Loss: 0.1186\n","Epoch [12/15], Step [1900/5000], Loss: 0.0988\n","Epoch [12/15], Step [1910/5000], Loss: 0.3053\n","Epoch [12/15], Step [1920/5000], Loss: 0.1704\n","Epoch [12/15], Step [1930/5000], Loss: 0.1744\n","Epoch [12/15], Step [1940/5000], Loss: 0.1905\n","Epoch [12/15], Step [1950/5000], Loss: 0.1499\n","Epoch [12/15], Step [1960/5000], Loss: 0.0681\n","Epoch [12/15], Step [1970/5000], Loss: 0.2374\n","Epoch [12/15], Step [1980/5000], Loss: 0.1852\n","Epoch [12/15], Step [1990/5000], Loss: 0.1313\n","Epoch [12/15], Step [2000/5000], Loss: 0.2147\n","Epoch [12/15], Step [2010/5000], Loss: 0.1185\n","Epoch [12/15], Step [2020/5000], Loss: 0.1350\n","Epoch [12/15], Step [2030/5000], Loss: 0.1429\n","Epoch [12/15], Step [2040/5000], Loss: 0.2705\n","Epoch [12/15], Step [2050/5000], Loss: 0.1707\n","Epoch [12/15], Step [2060/5000], Loss: 0.3363\n","Epoch [12/15], Step [2070/5000], Loss: 0.2648\n","Epoch [12/15], Step [2080/5000], Loss: 0.1568\n","Epoch [12/15], Step [2090/5000], Loss: 0.2241\n","Epoch [12/15], Step [2100/5000], Loss: 0.2365\n","Epoch [12/15], Step [2110/5000], Loss: 0.1093\n","Epoch [12/15], Step [2120/5000], Loss: 0.1800\n","Epoch [12/15], Step [2130/5000], Loss: 0.1068\n","Epoch [12/15], Step [2140/5000], Loss: 0.1965\n","Epoch [12/15], Step [2150/5000], Loss: 0.1605\n","Epoch [12/15], Step [2160/5000], Loss: 0.2614\n","Epoch [12/15], Step [2170/5000], Loss: 0.1316\n","Epoch [12/15], Step [2180/5000], Loss: 0.2409\n","Epoch [12/15], Step [2190/5000], Loss: 0.2174\n","Epoch [12/15], Step [2200/5000], Loss: 0.3139\n","Epoch [12/15], Step [2210/5000], Loss: 0.2148\n","Epoch [12/15], Step [2220/5000], Loss: 0.0568\n","Epoch [12/15], Step [2230/5000], Loss: 0.1824\n","Epoch [12/15], Step [2240/5000], Loss: 0.1266\n","Epoch [12/15], Step [2250/5000], Loss: 0.2665\n","Epoch [12/15], Step [2260/5000], Loss: 0.1138\n","Epoch [12/15], Step [2270/5000], Loss: 0.1470\n","Epoch [12/15], Step [2280/5000], Loss: 0.1843\n","Epoch [12/15], Step [2290/5000], Loss: 0.2561\n","Epoch [12/15], Step [2300/5000], Loss: 0.1697\n","Epoch [12/15], Step [2310/5000], Loss: 0.2543\n","Epoch [12/15], Step [2320/5000], Loss: 0.0290\n","Epoch [12/15], Step [2330/5000], Loss: 0.1980\n","Epoch [12/15], Step [2340/5000], Loss: 0.1543\n","Epoch [12/15], Step [2350/5000], Loss: 0.3380\n","Epoch [12/15], Step [2360/5000], Loss: 0.1781\n","Epoch [12/15], Step [2370/5000], Loss: 0.2059\n","Epoch [12/15], Step [2380/5000], Loss: 0.2195\n","Epoch [12/15], Step [2390/5000], Loss: 0.1891\n","Epoch [12/15], Step [2400/5000], Loss: 0.1689\n","Epoch [12/15], Step [2410/5000], Loss: 0.3244\n","Epoch [12/15], Step [2420/5000], Loss: 0.1092\n","Epoch [12/15], Step [2430/5000], Loss: 0.3112\n","Epoch [12/15], Step [2440/5000], Loss: 0.1470\n","Epoch [12/15], Step [2450/5000], Loss: 0.3440\n","Epoch [12/15], Step [2460/5000], Loss: 0.1816\n","Epoch [12/15], Step [2470/5000], Loss: 0.0001\n","Epoch [12/15], Step [2480/5000], Loss: 0.1691\n","Epoch [12/15], Step [2490/5000], Loss: 0.0802\n","Epoch [12/15], Step [2500/5000], Loss: 0.2697\n","Epoch [12/15], Step [2510/5000], Loss: 0.1871\n","Epoch [12/15], Step [2520/5000], Loss: 0.4400\n","Epoch [12/15], Step [2530/5000], Loss: 0.1795\n","Epoch [12/15], Step [2540/5000], Loss: 0.3203\n","Epoch [12/15], Step [2550/5000], Loss: 0.1816\n","Epoch [12/15], Step [2560/5000], Loss: 0.1786\n","Epoch [12/15], Step [2570/5000], Loss: 0.2238\n","Epoch [12/15], Step [2580/5000], Loss: 0.2779\n","Epoch [12/15], Step [2590/5000], Loss: 0.2954\n","Epoch [12/15], Step [2600/5000], Loss: 0.1126\n","Epoch [12/15], Step [2610/5000], Loss: 0.2470\n","Epoch [12/15], Step [2620/5000], Loss: 0.1947\n","Epoch [12/15], Step [2630/5000], Loss: 0.3689\n","Epoch [12/15], Step [2640/5000], Loss: 0.1276\n","Epoch [12/15], Step [2650/5000], Loss: 0.0921\n","Epoch [12/15], Step [2660/5000], Loss: 0.1197\n","Epoch [12/15], Step [2670/5000], Loss: 0.1626\n","Epoch [12/15], Step [2680/5000], Loss: 0.2209\n","Epoch [12/15], Step [2690/5000], Loss: 0.1415\n","Epoch [12/15], Step [2700/5000], Loss: 0.1485\n","Epoch [12/15], Step [2710/5000], Loss: 0.2827\n","Epoch [12/15], Step [2720/5000], Loss: 0.2172\n","Epoch [12/15], Step [2730/5000], Loss: 0.2581\n","Epoch [12/15], Step [2740/5000], Loss: 0.2520\n","Epoch [12/15], Step [2750/5000], Loss: 0.0948\n","Epoch [12/15], Step [2760/5000], Loss: 0.0961\n","Epoch [12/15], Step [2770/5000], Loss: 0.1720\n","Epoch [12/15], Step [2780/5000], Loss: 0.1885\n","Epoch [12/15], Step [2790/5000], Loss: 0.3483\n","Epoch [12/15], Step [2800/5000], Loss: 0.1910\n","Epoch [12/15], Step [2810/5000], Loss: 0.2354\n","Epoch [12/15], Step [2820/5000], Loss: 0.2117\n","Epoch [12/15], Step [2830/5000], Loss: 0.1384\n","Epoch [12/15], Step [2840/5000], Loss: 0.0672\n","Epoch [12/15], Step [2850/5000], Loss: 0.1125\n","Epoch [12/15], Step [2860/5000], Loss: 0.3067\n","Epoch [12/15], Step [2870/5000], Loss: 0.2073\n","Epoch [12/15], Step [2880/5000], Loss: 0.1504\n","Epoch [12/15], Step [2890/5000], Loss: 0.2193\n","Epoch [12/15], Step [2900/5000], Loss: 0.2445\n","Epoch [12/15], Step [2910/5000], Loss: 0.2288\n","Epoch [12/15], Step [2920/5000], Loss: 0.1205\n","Epoch [12/15], Step [2930/5000], Loss: 0.3546\n","Epoch [12/15], Step [2940/5000], Loss: 0.1727\n","Epoch [12/15], Step [2950/5000], Loss: 0.1817\n","Epoch [12/15], Step [2960/5000], Loss: 0.1505\n","Epoch [12/15], Step [2970/5000], Loss: 0.2214\n","Epoch [12/15], Step [2980/5000], Loss: 0.2168\n","Epoch [12/15], Step [2990/5000], Loss: 0.3029\n","Epoch [12/15], Step [3000/5000], Loss: 0.1911\n","Epoch [12/15], Step [3010/5000], Loss: 0.1152\n","Epoch [12/15], Step [3020/5000], Loss: 0.1149\n","Epoch [12/15], Step [3030/5000], Loss: 0.1796\n","Epoch [12/15], Step [3040/5000], Loss: 0.1375\n","Epoch [12/15], Step [3050/5000], Loss: 0.0995\n","Epoch [12/15], Step [3060/5000], Loss: 0.2469\n","Epoch [12/15], Step [3070/5000], Loss: 0.2647\n","Epoch [12/15], Step [3080/5000], Loss: 0.1232\n","Epoch [12/15], Step [3090/5000], Loss: 0.2719\n","Epoch [12/15], Step [3100/5000], Loss: 0.0799\n","Epoch [12/15], Step [3110/5000], Loss: 0.1077\n","Epoch [12/15], Step [3120/5000], Loss: 0.1674\n","Epoch [12/15], Step [3130/5000], Loss: 0.2491\n","Epoch [12/15], Step [3140/5000], Loss: 0.1912\n","Epoch [12/15], Step [3150/5000], Loss: 0.1741\n","Epoch [12/15], Step [3160/5000], Loss: 0.2593\n","Epoch [12/15], Step [3170/5000], Loss: 0.2429\n","Epoch [12/15], Step [3180/5000], Loss: 0.1839\n","Epoch [12/15], Step [3190/5000], Loss: 0.1976\n","Epoch [12/15], Step [3200/5000], Loss: 0.2638\n","Epoch [12/15], Step [3210/5000], Loss: 0.1477\n","Epoch [12/15], Step [3220/5000], Loss: 0.0901\n","Epoch [12/15], Step [3230/5000], Loss: 0.1840\n","Epoch [12/15], Step [3240/5000], Loss: 0.2560\n","Epoch [12/15], Step [3250/5000], Loss: 0.2941\n","Epoch [12/15], Step [3260/5000], Loss: 0.2127\n","Epoch [12/15], Step [3270/5000], Loss: 0.1606\n","Epoch [12/15], Step [3280/5000], Loss: 0.1312\n","Epoch [12/15], Step [3290/5000], Loss: 0.2123\n","Epoch [12/15], Step [3300/5000], Loss: 0.2055\n","Epoch [12/15], Step [3310/5000], Loss: 0.1569\n","Epoch [12/15], Step [3320/5000], Loss: 0.2238\n","Epoch [12/15], Step [3330/5000], Loss: 0.2585\n","Epoch [12/15], Step [3340/5000], Loss: 0.0913\n","Epoch [12/15], Step [3350/5000], Loss: 0.3024\n","Epoch [12/15], Step [3360/5000], Loss: 0.1443\n","Epoch [12/15], Step [3370/5000], Loss: 0.2454\n","Epoch [12/15], Step [3380/5000], Loss: 0.2860\n","Epoch [12/15], Step [3390/5000], Loss: 0.2061\n","Epoch [12/15], Step [3400/5000], Loss: 0.2198\n","Epoch [12/15], Step [3410/5000], Loss: 0.1362\n","Epoch [12/15], Step [3420/5000], Loss: 0.1236\n","Epoch [12/15], Step [3430/5000], Loss: 0.2299\n","Epoch [12/15], Step [3440/5000], Loss: 0.2699\n","Epoch [12/15], Step [3450/5000], Loss: 0.1211\n","Epoch [12/15], Step [3460/5000], Loss: 0.1905\n","Epoch [12/15], Step [3470/5000], Loss: 0.1862\n","Epoch [12/15], Step [3480/5000], Loss: 0.0887\n","Epoch [12/15], Step [3490/5000], Loss: 0.1927\n","Epoch [12/15], Step [3500/5000], Loss: 0.2035\n","Epoch [12/15], Step [3510/5000], Loss: 0.2035\n","Epoch [12/15], Step [3520/5000], Loss: 0.1880\n","Epoch [12/15], Step [3530/5000], Loss: 0.1779\n","Epoch [12/15], Step [3540/5000], Loss: 0.1430\n","Epoch [12/15], Step [3550/5000], Loss: 0.2135\n","Epoch [12/15], Step [3560/5000], Loss: 0.0529\n","Epoch [12/15], Step [3570/5000], Loss: 0.2079\n","Epoch [12/15], Step [3580/5000], Loss: 0.2110\n","Epoch [12/15], Step [3590/5000], Loss: 0.1366\n","Epoch [12/15], Step [3600/5000], Loss: 0.2187\n","Epoch [12/15], Step [3610/5000], Loss: 0.0422\n","Epoch [12/15], Step [3620/5000], Loss: 0.1872\n","Epoch [12/15], Step [3630/5000], Loss: 0.2019\n","Epoch [12/15], Step [3640/5000], Loss: 0.1683\n","Epoch [12/15], Step [3650/5000], Loss: 0.1042\n","Epoch [12/15], Step [3660/5000], Loss: 0.1885\n","Epoch [12/15], Step [3670/5000], Loss: 0.1384\n","Epoch [12/15], Step [3680/5000], Loss: 0.1641\n","Epoch [12/15], Step [3690/5000], Loss: 0.2237\n","Epoch [12/15], Step [3700/5000], Loss: 0.1664\n","Epoch [12/15], Step [3710/5000], Loss: 0.1994\n","Epoch [12/15], Step [3720/5000], Loss: 0.2025\n","Epoch [12/15], Step [3730/5000], Loss: 0.2672\n","Epoch [12/15], Step [3740/5000], Loss: 0.1258\n","Epoch [12/15], Step [3750/5000], Loss: 0.1265\n","Epoch [12/15], Step [3760/5000], Loss: 0.1124\n","Epoch [12/15], Step [3770/5000], Loss: 0.1426\n","Epoch [12/15], Step [3780/5000], Loss: 0.2495\n","Epoch [12/15], Step [3790/5000], Loss: 0.0851\n","Epoch [12/15], Step [3800/5000], Loss: 0.0957\n","Epoch [12/15], Step [3810/5000], Loss: 0.2317\n","Epoch [12/15], Step [3820/5000], Loss: 0.1063\n","Epoch [12/15], Step [3830/5000], Loss: 0.3265\n","Epoch [12/15], Step [3840/5000], Loss: 0.0684\n","Epoch [12/15], Step [3850/5000], Loss: 0.1613\n","Epoch [12/15], Step [3860/5000], Loss: 0.1253\n","Epoch [12/15], Step [3870/5000], Loss: 0.1169\n","Epoch [12/15], Step [3880/5000], Loss: 0.3332\n","Epoch [12/15], Step [3890/5000], Loss: 0.2956\n","Epoch [12/15], Step [3900/5000], Loss: 0.2498\n","Epoch [12/15], Step [3910/5000], Loss: 0.1495\n","Epoch [12/15], Step [3920/5000], Loss: 0.2746\n","Epoch [12/15], Step [3930/5000], Loss: 0.1550\n","Epoch [12/15], Step [3940/5000], Loss: 0.1779\n","Epoch [12/15], Step [3950/5000], Loss: 0.1554\n","Epoch [12/15], Step [3960/5000], Loss: 0.0860\n","Epoch [12/15], Step [3970/5000], Loss: 0.1847\n","Epoch [12/15], Step [3980/5000], Loss: 0.1392\n","Epoch [12/15], Step [3990/5000], Loss: 0.0826\n","Epoch [12/15], Step [4000/5000], Loss: 0.2724\n","Epoch [12/15], Step [4010/5000], Loss: 0.2199\n","Epoch [12/15], Step [4020/5000], Loss: 0.0922\n","Epoch [12/15], Step [4030/5000], Loss: 0.1587\n","Epoch [12/15], Step [4040/5000], Loss: 0.2391\n","Epoch [12/15], Step [4050/5000], Loss: 0.1969\n","Epoch [12/15], Step [4060/5000], Loss: 0.2038\n","Epoch [12/15], Step [4070/5000], Loss: 0.1569\n","Epoch [12/15], Step [4080/5000], Loss: 0.2460\n","Epoch [12/15], Step [4090/5000], Loss: 0.2911\n","Epoch [12/15], Step [4100/5000], Loss: 0.1268\n","Epoch [12/15], Step [4110/5000], Loss: 0.1131\n","Epoch [12/15], Step [4120/5000], Loss: 0.2554\n","Epoch [12/15], Step [4130/5000], Loss: 0.0975\n","Epoch [12/15], Step [4140/5000], Loss: 0.2362\n","Epoch [12/15], Step [4150/5000], Loss: 0.1436\n","Epoch [12/15], Step [4160/5000], Loss: 0.1436\n","Epoch [12/15], Step [4170/5000], Loss: 0.1252\n","Epoch [12/15], Step [4180/5000], Loss: 0.2003\n","Epoch [12/15], Step [4190/5000], Loss: 0.1714\n","Epoch [12/15], Step [4200/5000], Loss: 0.2429\n","Epoch [12/15], Step [4210/5000], Loss: 0.2676\n","Epoch [12/15], Step [4220/5000], Loss: 0.2321\n","Epoch [12/15], Step [4230/5000], Loss: 0.0570\n","Epoch [12/15], Step [4240/5000], Loss: 0.1992\n","Epoch [12/15], Step [4250/5000], Loss: 0.0798\n","Epoch [12/15], Step [4260/5000], Loss: 0.1538\n","Epoch [12/15], Step [4270/5000], Loss: 0.2245\n","Epoch [12/15], Step [4280/5000], Loss: 0.2474\n","Epoch [12/15], Step [4290/5000], Loss: 0.2456\n","Epoch [12/15], Step [4300/5000], Loss: 0.1397\n","Epoch [12/15], Step [4310/5000], Loss: 0.2452\n","Epoch [12/15], Step [4320/5000], Loss: 0.2052\n","Epoch [12/15], Step [4330/5000], Loss: 0.3398\n","Epoch [12/15], Step [4340/5000], Loss: 0.2967\n","Epoch [12/15], Step [4350/5000], Loss: 0.1493\n","Epoch [12/15], Step [4360/5000], Loss: 0.2124\n","Epoch [12/15], Step [4370/5000], Loss: 0.1624\n","Epoch [12/15], Step [4380/5000], Loss: 0.1642\n","Epoch [12/15], Step [4390/5000], Loss: 0.1514\n","Epoch [12/15], Step [4400/5000], Loss: 0.2549\n","Epoch [12/15], Step [4410/5000], Loss: 0.1654\n","Epoch [12/15], Step [4420/5000], Loss: 0.0741\n","Epoch [12/15], Step [4430/5000], Loss: 0.2526\n","Epoch [12/15], Step [4440/5000], Loss: 0.1118\n","Epoch [12/15], Step [4450/5000], Loss: 0.2789\n","Epoch [12/15], Step [4460/5000], Loss: 0.0632\n","Epoch [12/15], Step [4470/5000], Loss: 0.1812\n","Epoch [12/15], Step [4480/5000], Loss: 0.1900\n","Epoch [12/15], Step [4490/5000], Loss: 0.2335\n","Epoch [12/15], Step [4500/5000], Loss: 0.2820\n","Epoch [12/15], Step [4510/5000], Loss: 0.1431\n","Epoch [12/15], Step [4520/5000], Loss: 0.2335\n","Epoch [12/15], Step [4530/5000], Loss: 0.1954\n","Epoch [12/15], Step [4540/5000], Loss: 0.1778\n","Epoch [12/15], Step [4550/5000], Loss: 0.0892\n","Epoch [12/15], Step [4560/5000], Loss: 0.1587\n","Epoch [12/15], Step [4570/5000], Loss: 0.1110\n","Epoch [12/15], Step [4580/5000], Loss: 0.1319\n","Epoch [12/15], Step [4590/5000], Loss: 0.2596\n","Epoch [12/15], Step [4600/5000], Loss: 0.2844\n","Epoch [12/15], Step [4610/5000], Loss: 0.1879\n","Epoch [12/15], Step [4620/5000], Loss: 0.1231\n","Epoch [12/15], Step [4630/5000], Loss: 0.1834\n","Epoch [12/15], Step [4640/5000], Loss: 0.0755\n","Epoch [12/15], Step [4650/5000], Loss: 0.2504\n","Epoch [12/15], Step [4660/5000], Loss: 0.2727\n","Epoch [12/15], Step [4670/5000], Loss: 0.1024\n","Epoch [12/15], Step [4680/5000], Loss: 0.1699\n","Epoch [12/15], Step [4690/5000], Loss: 0.2207\n","Epoch [12/15], Step [4700/5000], Loss: 0.1721\n","Epoch [12/15], Step [4710/5000], Loss: 0.2251\n","Epoch [12/15], Step [4720/5000], Loss: 0.0979\n","Epoch [12/15], Step [4730/5000], Loss: 0.1491\n","Epoch [12/15], Step [4740/5000], Loss: 0.1364\n","Epoch [12/15], Step [4750/5000], Loss: 0.0760\n","Epoch [12/15], Step [4760/5000], Loss: 0.2942\n","Epoch [12/15], Step [4770/5000], Loss: 0.2215\n","Epoch [12/15], Step [4780/5000], Loss: 0.1040\n","Epoch [12/15], Step [4790/5000], Loss: 0.1389\n","Epoch [12/15], Step [4800/5000], Loss: 0.2365\n","Epoch [12/15], Step [4810/5000], Loss: 0.1618\n","Epoch [12/15], Step [4820/5000], Loss: 0.1310\n","Epoch [12/15], Step [4830/5000], Loss: 0.1152\n","Epoch [12/15], Step [4840/5000], Loss: 0.1443\n","Epoch [12/15], Step [4850/5000], Loss: 0.1985\n","Epoch [12/15], Step [4860/5000], Loss: 0.1005\n","Epoch [12/15], Step [4870/5000], Loss: 0.0687\n","Epoch [12/15], Step [4880/5000], Loss: 0.1667\n","Epoch [12/15], Step [4890/5000], Loss: 0.1337\n","Epoch [12/15], Step [4900/5000], Loss: 0.1516\n","Epoch [12/15], Step [4910/5000], Loss: 0.2196\n","Epoch [12/15], Step [4920/5000], Loss: 0.3070\n","Epoch [12/15], Step [4930/5000], Loss: 0.1483\n","Epoch [12/15], Step [4940/5000], Loss: 0.1233\n","Epoch [12/15], Step [4950/5000], Loss: 0.2553\n","Epoch [12/15], Step [4960/5000], Loss: 0.1494\n","Epoch [12/15], Step [4970/5000], Loss: 0.1665\n","Epoch [12/15], Step [4980/5000], Loss: 0.0851\n","Epoch [12/15], Step [4990/5000], Loss: 0.1986\n","Model saved at epoch 12, Loss: 0.1807\n","Epoch [13/15], Step [0/5000], Loss: 0.1844\n","Epoch [13/15], Step [10/5000], Loss: 0.1254\n","Epoch [13/15], Step [20/5000], Loss: 0.3401\n","Epoch [13/15], Step [30/5000], Loss: 0.1874\n","Epoch [13/15], Step [40/5000], Loss: 0.1831\n","Epoch [13/15], Step [50/5000], Loss: 0.2316\n","Epoch [13/15], Step [60/5000], Loss: 0.1644\n","Epoch [13/15], Step [70/5000], Loss: 0.2111\n","Epoch [13/15], Step [80/5000], Loss: 0.2463\n","Epoch [13/15], Step [90/5000], Loss: 0.1300\n","Epoch [13/15], Step [100/5000], Loss: 0.2045\n","Epoch [13/15], Step [110/5000], Loss: 0.1954\n","Epoch [13/15], Step [120/5000], Loss: 0.1230\n","Epoch [13/15], Step [130/5000], Loss: 0.0622\n","Epoch [13/15], Step [140/5000], Loss: 0.1427\n","Epoch [13/15], Step [150/5000], Loss: 0.1961\n","Epoch [13/15], Step [160/5000], Loss: 0.1269\n","Epoch [13/15], Step [170/5000], Loss: 0.2670\n","Epoch [13/15], Step [180/5000], Loss: 0.1552\n","Epoch [13/15], Step [190/5000], Loss: 0.2389\n","Epoch [13/15], Step [200/5000], Loss: 0.3210\n","Epoch [13/15], Step [210/5000], Loss: 0.1502\n","Epoch [13/15], Step [220/5000], Loss: 0.1160\n","Epoch [13/15], Step [230/5000], Loss: 0.0862\n","Epoch [13/15], Step [240/5000], Loss: 0.1671\n","Epoch [13/15], Step [250/5000], Loss: 0.2871\n","Epoch [13/15], Step [260/5000], Loss: 0.0995\n","Epoch [13/15], Step [270/5000], Loss: 0.1634\n","Epoch [13/15], Step [280/5000], Loss: 0.1999\n","Epoch [13/15], Step [290/5000], Loss: 0.1563\n","Epoch [13/15], Step [300/5000], Loss: 0.1548\n","Epoch [13/15], Step [310/5000], Loss: 0.2441\n","Epoch [13/15], Step [320/5000], Loss: 0.1216\n","Epoch [13/15], Step [330/5000], Loss: 0.1512\n","Epoch [13/15], Step [340/5000], Loss: 0.1665\n","Epoch [13/15], Step [350/5000], Loss: 0.1937\n","Epoch [13/15], Step [360/5000], Loss: 0.1348\n","Epoch [13/15], Step [370/5000], Loss: 0.0557\n","Epoch [13/15], Step [380/5000], Loss: 0.1854\n","Epoch [13/15], Step [390/5000], Loss: 0.2012\n","Epoch [13/15], Step [400/5000], Loss: 0.0674\n","Epoch [13/15], Step [410/5000], Loss: 0.1163\n","Epoch [13/15], Step [420/5000], Loss: 0.0941\n","Epoch [13/15], Step [430/5000], Loss: 0.1962\n","Epoch [13/15], Step [440/5000], Loss: 0.2467\n","Epoch [13/15], Step [450/5000], Loss: 0.1120\n","Epoch [13/15], Step [460/5000], Loss: 0.1288\n","Epoch [13/15], Step [470/5000], Loss: 0.1935\n","Epoch [13/15], Step [480/5000], Loss: 0.1267\n","Epoch [13/15], Step [490/5000], Loss: 0.2241\n","Epoch [13/15], Step [500/5000], Loss: 0.1974\n","Epoch [13/15], Step [510/5000], Loss: 0.2838\n","Epoch [13/15], Step [520/5000], Loss: 0.1276\n","Epoch [13/15], Step [530/5000], Loss: 0.2257\n","Epoch [13/15], Step [540/5000], Loss: 0.1537\n","Epoch [13/15], Step [550/5000], Loss: 0.1962\n","Epoch [13/15], Step [560/5000], Loss: 0.1599\n","Epoch [13/15], Step [570/5000], Loss: 0.2325\n","Epoch [13/15], Step [580/5000], Loss: 0.3014\n","Epoch [13/15], Step [590/5000], Loss: 0.1233\n","Epoch [13/15], Step [600/5000], Loss: 0.1861\n","Epoch [13/15], Step [610/5000], Loss: 0.3978\n","Epoch [13/15], Step [620/5000], Loss: 0.2394\n","Epoch [13/15], Step [630/5000], Loss: 0.1420\n","Epoch [13/15], Step [640/5000], Loss: 0.2223\n","Epoch [13/15], Step [650/5000], Loss: 0.2121\n","Epoch [13/15], Step [660/5000], Loss: 0.0580\n","Epoch [13/15], Step [670/5000], Loss: 0.0610\n","Epoch [13/15], Step [680/5000], Loss: 0.2390\n","Epoch [13/15], Step [690/5000], Loss: 0.2266\n","Epoch [13/15], Step [700/5000], Loss: 0.2182\n","Epoch [13/15], Step [710/5000], Loss: 0.2435\n","Epoch [13/15], Step [720/5000], Loss: 0.2877\n","Epoch [13/15], Step [730/5000], Loss: 0.2179\n","Epoch [13/15], Step [740/5000], Loss: 0.2020\n","Epoch [13/15], Step [750/5000], Loss: 0.1753\n","Epoch [13/15], Step [760/5000], Loss: 0.2991\n","Epoch [13/15], Step [770/5000], Loss: 0.1462\n","Epoch [13/15], Step [780/5000], Loss: 0.0452\n","Epoch [13/15], Step [790/5000], Loss: 0.1954\n","Epoch [13/15], Step [800/5000], Loss: 0.0971\n","Epoch [13/15], Step [810/5000], Loss: 0.1311\n","Epoch [13/15], Step [820/5000], Loss: 0.2232\n","Epoch [13/15], Step [830/5000], Loss: 0.1634\n","Epoch [13/15], Step [840/5000], Loss: 0.1046\n","Epoch [13/15], Step [850/5000], Loss: 0.2423\n","Epoch [13/15], Step [860/5000], Loss: 0.2044\n","Epoch [13/15], Step [870/5000], Loss: 0.1784\n","Epoch [13/15], Step [880/5000], Loss: 0.2409\n","Epoch [13/15], Step [890/5000], Loss: 0.1532\n","Epoch [13/15], Step [900/5000], Loss: 0.0755\n","Epoch [13/15], Step [910/5000], Loss: 0.1532\n","Epoch [13/15], Step [920/5000], Loss: 0.1347\n","Epoch [13/15], Step [930/5000], Loss: 0.1674\n","Epoch [13/15], Step [940/5000], Loss: 0.0589\n","Epoch [13/15], Step [950/5000], Loss: 0.0430\n","Epoch [13/15], Step [960/5000], Loss: 0.0995\n","Epoch [13/15], Step [970/5000], Loss: 0.2089\n","Epoch [13/15], Step [980/5000], Loss: 0.0769\n","Epoch [13/15], Step [990/5000], Loss: 0.1398\n","Epoch [13/15], Step [1000/5000], Loss: 0.2472\n","Epoch [13/15], Step [1010/5000], Loss: 0.2936\n","Epoch [13/15], Step [1020/5000], Loss: 0.1743\n","Epoch [13/15], Step [1030/5000], Loss: 0.3008\n","Epoch [13/15], Step [1040/5000], Loss: 0.2381\n","Epoch [13/15], Step [1050/5000], Loss: 0.2537\n","Epoch [13/15], Step [1060/5000], Loss: 0.1084\n","Epoch [13/15], Step [1070/5000], Loss: 0.0977\n","Epoch [13/15], Step [1080/5000], Loss: 0.3198\n","Epoch [13/15], Step [1090/5000], Loss: 0.1136\n","Epoch [13/15], Step [1100/5000], Loss: 0.1778\n","Epoch [13/15], Step [1110/5000], Loss: 0.1310\n","Epoch [13/15], Step [1120/5000], Loss: 0.1596\n","Epoch [13/15], Step [1130/5000], Loss: 0.1353\n","Epoch [13/15], Step [1140/5000], Loss: 0.1477\n","Epoch [13/15], Step [1150/5000], Loss: 0.1585\n","Epoch [13/15], Step [1160/5000], Loss: 0.1158\n","Epoch [13/15], Step [1170/5000], Loss: 0.2032\n","Epoch [13/15], Step [1180/5000], Loss: 0.0725\n","Epoch [13/15], Step [1190/5000], Loss: 0.1785\n","Epoch [13/15], Step [1200/5000], Loss: 0.2241\n","Epoch [13/15], Step [1210/5000], Loss: 0.2059\n","Epoch [13/15], Step [1220/5000], Loss: 0.2414\n","Epoch [13/15], Step [1230/5000], Loss: 0.2170\n","Epoch [13/15], Step [1240/5000], Loss: 0.1504\n","Epoch [13/15], Step [1250/5000], Loss: 0.1473\n","Epoch [13/15], Step [1260/5000], Loss: 0.1589\n","Epoch [13/15], Step [1270/5000], Loss: 0.2496\n","Epoch [13/15], Step [1280/5000], Loss: 0.3069\n","Epoch [13/15], Step [1290/5000], Loss: 0.1681\n","Epoch [13/15], Step [1300/5000], Loss: 0.1527\n","Epoch [13/15], Step [1310/5000], Loss: 0.1815\n","Epoch [13/15], Step [1320/5000], Loss: 0.3530\n","Epoch [13/15], Step [1330/5000], Loss: 0.2123\n","Epoch [13/15], Step [1340/5000], Loss: 0.1751\n","Epoch [13/15], Step [1350/5000], Loss: 0.1500\n","Epoch [13/15], Step [1360/5000], Loss: 0.1242\n","Epoch [13/15], Step [1370/5000], Loss: 0.1022\n","Epoch [13/15], Step [1380/5000], Loss: 0.1836\n","Epoch [13/15], Step [1390/5000], Loss: 0.1883\n","Epoch [13/15], Step [1400/5000], Loss: 0.1452\n","Epoch [13/15], Step [1410/5000], Loss: 0.1346\n","Epoch [13/15], Step [1420/5000], Loss: 0.2094\n","Epoch [13/15], Step [1430/5000], Loss: 0.3632\n","Epoch [13/15], Step [1440/5000], Loss: 0.2275\n","Epoch [13/15], Step [1450/5000], Loss: 0.0711\n","Epoch [13/15], Step [1460/5000], Loss: 0.1749\n","Epoch [13/15], Step [1470/5000], Loss: 0.0565\n","Epoch [13/15], Step [1480/5000], Loss: 0.1876\n","Epoch [13/15], Step [1490/5000], Loss: 0.1747\n","Epoch [13/15], Step [1500/5000], Loss: 0.1392\n","Epoch [13/15], Step [1510/5000], Loss: 0.1987\n","Epoch [13/15], Step [1520/5000], Loss: 0.2514\n","Epoch [13/15], Step [1530/5000], Loss: 0.2299\n","Epoch [13/15], Step [1540/5000], Loss: 0.0835\n","Epoch [13/15], Step [1550/5000], Loss: 0.1046\n","Epoch [13/15], Step [1560/5000], Loss: 0.1090\n","Epoch [13/15], Step [1570/5000], Loss: 0.1285\n","Epoch [13/15], Step [1580/5000], Loss: 0.1861\n","Epoch [13/15], Step [1590/5000], Loss: 0.1090\n","Epoch [13/15], Step [1600/5000], Loss: 0.2345\n","Epoch [13/15], Step [1610/5000], Loss: 0.1212\n","Epoch [13/15], Step [1620/5000], Loss: 0.1014\n","Epoch [13/15], Step [1630/5000], Loss: 0.0774\n","Epoch [13/15], Step [1640/5000], Loss: 0.1201\n","Epoch [13/15], Step [1650/5000], Loss: 0.2606\n","Epoch [13/15], Step [1660/5000], Loss: 0.1801\n","Epoch [13/15], Step [1670/5000], Loss: 0.1071\n","Epoch [13/15], Step [1680/5000], Loss: 0.1679\n","Epoch [13/15], Step [1690/5000], Loss: 0.1520\n","Epoch [13/15], Step [1700/5000], Loss: 0.2247\n","Epoch [13/15], Step [1710/5000], Loss: 0.2337\n","Epoch [13/15], Step [1720/5000], Loss: 0.1765\n","Epoch [13/15], Step [1730/5000], Loss: 0.1314\n","Epoch [13/15], Step [1740/5000], Loss: 0.1270\n","Epoch [13/15], Step [1750/5000], Loss: 0.1606\n","Epoch [13/15], Step [1760/5000], Loss: 0.0681\n","Epoch [13/15], Step [1770/5000], Loss: 0.2733\n","Epoch [13/15], Step [1780/5000], Loss: 0.3147\n","Epoch [13/15], Step [1790/5000], Loss: 0.1245\n","Epoch [13/15], Step [1800/5000], Loss: 0.0945\n","Epoch [13/15], Step [1810/5000], Loss: 0.3121\n","Epoch [13/15], Step [1820/5000], Loss: 0.1279\n","Epoch [13/15], Step [1830/5000], Loss: 0.2344\n","Epoch [13/15], Step [1840/5000], Loss: 0.1613\n","Epoch [13/15], Step [1850/5000], Loss: 0.1865\n","Epoch [13/15], Step [1860/5000], Loss: 0.0733\n","Epoch [13/15], Step [1870/5000], Loss: 0.1078\n","Epoch [13/15], Step [1880/5000], Loss: 0.1392\n","Epoch [13/15], Step [1890/5000], Loss: 0.1184\n","Epoch [13/15], Step [1900/5000], Loss: 0.1668\n","Epoch [13/15], Step [1910/5000], Loss: 0.2244\n","Epoch [13/15], Step [1920/5000], Loss: 0.2555\n","Epoch [13/15], Step [1930/5000], Loss: 0.1899\n","Epoch [13/15], Step [1940/5000], Loss: 0.1695\n","Epoch [13/15], Step [1950/5000], Loss: 0.2411\n","Epoch [13/15], Step [1960/5000], Loss: 0.1268\n","Epoch [13/15], Step [1970/5000], Loss: 0.1161\n","Epoch [13/15], Step [1980/5000], Loss: 0.0979\n","Epoch [13/15], Step [1990/5000], Loss: 0.0973\n","Epoch [13/15], Step [2000/5000], Loss: 0.0813\n","Epoch [13/15], Step [2010/5000], Loss: 0.2452\n","Epoch [13/15], Step [2020/5000], Loss: 0.1483\n","Epoch [13/15], Step [2030/5000], Loss: 0.1397\n","Epoch [13/15], Step [2040/5000], Loss: 0.1404\n","Epoch [13/15], Step [2050/5000], Loss: 0.1071\n","Epoch [13/15], Step [2060/5000], Loss: 0.2321\n","Epoch [13/15], Step [2070/5000], Loss: 0.3124\n","Epoch [13/15], Step [2080/5000], Loss: 0.1162\n","Epoch [13/15], Step [2090/5000], Loss: 0.2855\n","Epoch [13/15], Step [2100/5000], Loss: 0.1598\n","Epoch [13/15], Step [2110/5000], Loss: 0.2193\n","Epoch [13/15], Step [2120/5000], Loss: 0.2680\n","Epoch [13/15], Step [2130/5000], Loss: 0.0298\n","Epoch [13/15], Step [2140/5000], Loss: 0.1766\n","Epoch [13/15], Step [2150/5000], Loss: 0.0615\n","Epoch [13/15], Step [2160/5000], Loss: 0.2140\n","Epoch [13/15], Step [2170/5000], Loss: 0.2216\n","Epoch [13/15], Step [2180/5000], Loss: 0.1281\n","Epoch [13/15], Step [2190/5000], Loss: 0.1745\n","Epoch [13/15], Step [2200/5000], Loss: 0.1420\n","Epoch [13/15], Step [2210/5000], Loss: 0.1767\n","Epoch [13/15], Step [2220/5000], Loss: 0.2260\n","Epoch [13/15], Step [2230/5000], Loss: 0.1130\n","Epoch [13/15], Step [2240/5000], Loss: 0.2047\n","Epoch [13/15], Step [2250/5000], Loss: 0.1899\n","Epoch [13/15], Step [2260/5000], Loss: 0.2346\n","Epoch [13/15], Step [2270/5000], Loss: 0.1747\n","Epoch [13/15], Step [2280/5000], Loss: 0.0726\n","Epoch [13/15], Step [2290/5000], Loss: 0.1384\n","Epoch [13/15], Step [2300/5000], Loss: 0.2443\n","Epoch [13/15], Step [2310/5000], Loss: 0.0911\n","Epoch [13/15], Step [2320/5000], Loss: 0.1680\n","Epoch [13/15], Step [2330/5000], Loss: 0.1182\n","Epoch [13/15], Step [2340/5000], Loss: 0.2309\n","Epoch [13/15], Step [2350/5000], Loss: 0.1611\n","Epoch [13/15], Step [2360/5000], Loss: 0.2496\n","Epoch [13/15], Step [2370/5000], Loss: 0.3909\n","Epoch [13/15], Step [2380/5000], Loss: 0.1045\n","Epoch [13/15], Step [2390/5000], Loss: 0.0781\n","Epoch [13/15], Step [2400/5000], Loss: 0.0856\n","Epoch [13/15], Step [2410/5000], Loss: 0.1521\n","Epoch [13/15], Step [2420/5000], Loss: 0.1805\n","Epoch [13/15], Step [2430/5000], Loss: 0.2333\n","Epoch [13/15], Step [2440/5000], Loss: 0.2287\n","Epoch [13/15], Step [2450/5000], Loss: 0.1182\n","Epoch [13/15], Step [2460/5000], Loss: 0.2039\n","Epoch [13/15], Step [2470/5000], Loss: 0.2714\n","Epoch [13/15], Step [2480/5000], Loss: 0.1340\n","Epoch [13/15], Step [2490/5000], Loss: 0.1821\n","Epoch [13/15], Step [2500/5000], Loss: 0.1723\n","Epoch [13/15], Step [2510/5000], Loss: 0.1832\n","Epoch [13/15], Step [2520/5000], Loss: 0.1828\n","Epoch [13/15], Step [2530/5000], Loss: 0.1827\n","Epoch [13/15], Step [2540/5000], Loss: 0.1014\n","Epoch [13/15], Step [2550/5000], Loss: 0.1360\n","Epoch [13/15], Step [2560/5000], Loss: 0.3194\n","Epoch [13/15], Step [2570/5000], Loss: 0.1235\n","Epoch [13/15], Step [2580/5000], Loss: 0.0707\n","Epoch [13/15], Step [2590/5000], Loss: 0.1957\n","Epoch [13/15], Step [2600/5000], Loss: 0.2325\n","Epoch [13/15], Step [2610/5000], Loss: 0.2575\n","Epoch [13/15], Step [2620/5000], Loss: 0.2654\n","Epoch [13/15], Step [2630/5000], Loss: 0.0985\n","Epoch [13/15], Step [2640/5000], Loss: 0.1284\n","Epoch [13/15], Step [2650/5000], Loss: 0.1187\n","Epoch [13/15], Step [2660/5000], Loss: 0.1870\n","Epoch [13/15], Step [2670/5000], Loss: 0.2143\n","Epoch [13/15], Step [2680/5000], Loss: 0.2171\n","Epoch [13/15], Step [2690/5000], Loss: 0.0505\n","Epoch [13/15], Step [2700/5000], Loss: 0.2851\n","Epoch [13/15], Step [2710/5000], Loss: 0.1264\n","Epoch [13/15], Step [2720/5000], Loss: 0.2949\n","Epoch [13/15], Step [2730/5000], Loss: 0.1993\n","Epoch [13/15], Step [2740/5000], Loss: 0.1790\n","Epoch [13/15], Step [2750/5000], Loss: 0.1092\n","Epoch [13/15], Step [2760/5000], Loss: 0.1974\n","Epoch [13/15], Step [2770/5000], Loss: 0.0774\n","Epoch [13/15], Step [2780/5000], Loss: 0.1159\n","Epoch [13/15], Step [2790/5000], Loss: 0.1675\n","Epoch [13/15], Step [2800/5000], Loss: 0.1736\n","Epoch [13/15], Step [2810/5000], Loss: 0.1643\n","Epoch [13/15], Step [2820/5000], Loss: 0.1574\n","Epoch [13/15], Step [2830/5000], Loss: 0.1308\n","Epoch [13/15], Step [2840/5000], Loss: 0.1956\n","Epoch [13/15], Step [2850/5000], Loss: 0.2298\n","Epoch [13/15], Step [2860/5000], Loss: 0.2904\n","Epoch [13/15], Step [2870/5000], Loss: 0.2552\n","Epoch [13/15], Step [2880/5000], Loss: 0.1388\n","Epoch [13/15], Step [2890/5000], Loss: 0.1217\n","Epoch [13/15], Step [2900/5000], Loss: 0.2043\n","Epoch [13/15], Step [2910/5000], Loss: 0.1883\n","Epoch [13/15], Step [2920/5000], Loss: 0.1490\n","Epoch [13/15], Step [2930/5000], Loss: 0.1066\n","Epoch [13/15], Step [2940/5000], Loss: 0.2199\n","Epoch [13/15], Step [2950/5000], Loss: 0.2090\n","Epoch [13/15], Step [2960/5000], Loss: 0.1560\n","Epoch [13/15], Step [2970/5000], Loss: 0.1981\n","Epoch [13/15], Step [2980/5000], Loss: 0.1357\n","Epoch [13/15], Step [2990/5000], Loss: 0.2075\n","Epoch [13/15], Step [3000/5000], Loss: 0.2103\n","Epoch [13/15], Step [3010/5000], Loss: 0.1647\n","Epoch [13/15], Step [3020/5000], Loss: 0.1550\n","Epoch [13/15], Step [3030/5000], Loss: 0.2815\n","Epoch [13/15], Step [3040/5000], Loss: 0.1409\n","Epoch [13/15], Step [3050/5000], Loss: 0.1626\n","Epoch [13/15], Step [3060/5000], Loss: 0.2056\n","Epoch [13/15], Step [3070/5000], Loss: 0.0251\n","Epoch [13/15], Step [3080/5000], Loss: 0.2093\n","Epoch [13/15], Step [3090/5000], Loss: 0.0620\n","Epoch [13/15], Step [3100/5000], Loss: 0.1936\n","Epoch [13/15], Step [3110/5000], Loss: 0.1319\n","Epoch [13/15], Step [3120/5000], Loss: 0.1761\n","Epoch [13/15], Step [3130/5000], Loss: 0.1332\n","Epoch [13/15], Step [3140/5000], Loss: 0.1856\n","Epoch [13/15], Step [3150/5000], Loss: 0.2654\n","Epoch [13/15], Step [3160/5000], Loss: 0.2669\n","Epoch [13/15], Step [3170/5000], Loss: 0.1214\n","Epoch [13/15], Step [3180/5000], Loss: 0.1694\n","Epoch [13/15], Step [3190/5000], Loss: 0.1665\n","Epoch [13/15], Step [3200/5000], Loss: 0.0361\n","Epoch [13/15], Step [3210/5000], Loss: 0.1197\n","Epoch [13/15], Step [3220/5000], Loss: 0.3188\n","Epoch [13/15], Step [3230/5000], Loss: 0.1736\n","Epoch [13/15], Step [3240/5000], Loss: 0.1532\n","Epoch [13/15], Step [3250/5000], Loss: 0.1553\n","Epoch [13/15], Step [3260/5000], Loss: 0.2163\n","Epoch [13/15], Step [3270/5000], Loss: 0.2324\n","Epoch [13/15], Step [3280/5000], Loss: 0.1199\n","Epoch [13/15], Step [3290/5000], Loss: 0.2992\n","Epoch [13/15], Step [3300/5000], Loss: 0.1889\n","Epoch [13/15], Step [3310/5000], Loss: 0.2629\n","Epoch [13/15], Step [3320/5000], Loss: 0.1001\n","Epoch [13/15], Step [3330/5000], Loss: 0.0990\n","Epoch [13/15], Step [3340/5000], Loss: 0.1083\n","Epoch [13/15], Step [3350/5000], Loss: 0.1614\n","Epoch [13/15], Step [3360/5000], Loss: 0.3350\n","Epoch [13/15], Step [3370/5000], Loss: 0.0490\n","Epoch [13/15], Step [3380/5000], Loss: 0.2085\n","Epoch [13/15], Step [3390/5000], Loss: 0.1683\n","Epoch [13/15], Step [3400/5000], Loss: 0.1653\n","Epoch [13/15], Step [3410/5000], Loss: 0.1837\n","Epoch [13/15], Step [3420/5000], Loss: 0.1977\n","Epoch [13/15], Step [3430/5000], Loss: 0.1954\n","Epoch [13/15], Step [3440/5000], Loss: 0.1434\n","Epoch [13/15], Step [3450/5000], Loss: 0.1926\n","Epoch [13/15], Step [3460/5000], Loss: 0.4752\n","Epoch [13/15], Step [3470/5000], Loss: 0.1641\n","Epoch [13/15], Step [3480/5000], Loss: 0.0953\n","Epoch [13/15], Step [3490/5000], Loss: 0.1217\n","Epoch [13/15], Step [3500/5000], Loss: 0.0954\n","Epoch [13/15], Step [3510/5000], Loss: 0.2153\n","Epoch [13/15], Step [3520/5000], Loss: 0.2426\n","Epoch [13/15], Step [3530/5000], Loss: 0.1765\n","Epoch [13/15], Step [3540/5000], Loss: 0.1442\n","Epoch [13/15], Step [3550/5000], Loss: 0.1557\n","Epoch [13/15], Step [3560/5000], Loss: 0.1504\n","Epoch [13/15], Step [3570/5000], Loss: 0.0940\n","Epoch [13/15], Step [3580/5000], Loss: 0.2173\n","Epoch [13/15], Step [3590/5000], Loss: 0.1767\n","Epoch [13/15], Step [3600/5000], Loss: 0.1425\n","Epoch [13/15], Step [3610/5000], Loss: 0.2185\n","Epoch [13/15], Step [3620/5000], Loss: 0.1744\n","Epoch [13/15], Step [3630/5000], Loss: 0.3361\n","Epoch [13/15], Step [3640/5000], Loss: 0.0889\n","Epoch [13/15], Step [3650/5000], Loss: 0.1884\n","Epoch [13/15], Step [3660/5000], Loss: 0.0481\n","Epoch [13/15], Step [3670/5000], Loss: 0.2278\n","Epoch [13/15], Step [3680/5000], Loss: 0.0709\n","Epoch [13/15], Step [3690/5000], Loss: 0.3045\n","Epoch [13/15], Step [3700/5000], Loss: 0.2133\n","Epoch [13/15], Step [3710/5000], Loss: 0.0894\n","Epoch [13/15], Step [3720/5000], Loss: 0.2042\n","Epoch [13/15], Step [3730/5000], Loss: 0.1613\n","Epoch [13/15], Step [3740/5000], Loss: 0.2738\n","Epoch [13/15], Step [3750/5000], Loss: 0.0371\n","Epoch [13/15], Step [3760/5000], Loss: 0.2489\n","Epoch [13/15], Step [3770/5000], Loss: 0.1452\n","Epoch [13/15], Step [3780/5000], Loss: 0.1712\n","Epoch [13/15], Step [3790/5000], Loss: 0.1817\n","Epoch [13/15], Step [3800/5000], Loss: 0.1220\n","Epoch [13/15], Step [3810/5000], Loss: 0.1758\n","Epoch [13/15], Step [3820/5000], Loss: 0.1641\n","Epoch [13/15], Step [3830/5000], Loss: 0.1914\n","Epoch [13/15], Step [3840/5000], Loss: 0.1393\n","Epoch [13/15], Step [3850/5000], Loss: 0.1212\n","Epoch [13/15], Step [3860/5000], Loss: 0.2400\n","Epoch [13/15], Step [3870/5000], Loss: 0.1779\n","Epoch [13/15], Step [3880/5000], Loss: 0.1525\n","Epoch [13/15], Step [3890/5000], Loss: 0.0807\n","Epoch [13/15], Step [3900/5000], Loss: 0.1636\n","Epoch [13/15], Step [3910/5000], Loss: 0.4006\n","Epoch [13/15], Step [3920/5000], Loss: 0.1232\n","Epoch [13/15], Step [3930/5000], Loss: 0.2017\n","Epoch [13/15], Step [3940/5000], Loss: 0.0599\n","Epoch [13/15], Step [3950/5000], Loss: 0.3650\n","Epoch [13/15], Step [3960/5000], Loss: 0.1358\n","Epoch [13/15], Step [3970/5000], Loss: 0.2902\n","Epoch [13/15], Step [3980/5000], Loss: 0.2175\n","Epoch [13/15], Step [3990/5000], Loss: 0.1111\n","Epoch [13/15], Step [4000/5000], Loss: 0.0881\n","Epoch [13/15], Step [4010/5000], Loss: 0.1066\n","Epoch [13/15], Step [4020/5000], Loss: 0.1601\n","Epoch [13/15], Step [4030/5000], Loss: 0.1059\n","Epoch [13/15], Step [4040/5000], Loss: 0.1133\n","Epoch [13/15], Step [4050/5000], Loss: 0.1817\n","Epoch [13/15], Step [4060/5000], Loss: 0.3125\n","Epoch [13/15], Step [4070/5000], Loss: 0.1994\n","Epoch [13/15], Step [4080/5000], Loss: 0.1493\n","Epoch [13/15], Step [4090/5000], Loss: 0.2613\n","Epoch [13/15], Step [4100/5000], Loss: 0.1305\n","Epoch [13/15], Step [4110/5000], Loss: 0.1174\n","Epoch [13/15], Step [4120/5000], Loss: 0.1932\n","Epoch [13/15], Step [4130/5000], Loss: 0.0875\n","Epoch [13/15], Step [4140/5000], Loss: 0.3006\n","Epoch [13/15], Step [4150/5000], Loss: 0.1825\n","Epoch [13/15], Step [4160/5000], Loss: 0.1230\n","Epoch [13/15], Step [4170/5000], Loss: 0.1771\n","Epoch [13/15], Step [4180/5000], Loss: 0.1943\n","Epoch [13/15], Step [4190/5000], Loss: 0.1814\n","Epoch [13/15], Step [4200/5000], Loss: 0.1570\n","Epoch [13/15], Step [4210/5000], Loss: 0.1817\n","Epoch [13/15], Step [4220/5000], Loss: 0.2809\n","Epoch [13/15], Step [4230/5000], Loss: 0.1024\n","Epoch [13/15], Step [4240/5000], Loss: 0.0971\n","Epoch [13/15], Step [4250/5000], Loss: 0.0826\n","Epoch [13/15], Step [4260/5000], Loss: 0.2295\n","Epoch [13/15], Step [4270/5000], Loss: 0.1725\n","Epoch [13/15], Step [4280/5000], Loss: 0.1196\n","Epoch [13/15], Step [4290/5000], Loss: 0.1878\n","Epoch [13/15], Step [4300/5000], Loss: 0.1165\n","Epoch [13/15], Step [4310/5000], Loss: 0.2020\n","Epoch [13/15], Step [4320/5000], Loss: 0.0738\n","Epoch [13/15], Step [4330/5000], Loss: 0.1677\n","Epoch [13/15], Step [4340/5000], Loss: 0.1247\n","Epoch [13/15], Step [4350/5000], Loss: 0.2707\n","Epoch [13/15], Step [4360/5000], Loss: 0.2453\n","Epoch [13/15], Step [4370/5000], Loss: 0.1197\n","Epoch [13/15], Step [4380/5000], Loss: 0.1774\n","Epoch [13/15], Step [4390/5000], Loss: 0.2413\n","Epoch [13/15], Step [4400/5000], Loss: 0.3980\n","Epoch [13/15], Step [4410/5000], Loss: 0.1266\n","Epoch [13/15], Step [4420/5000], Loss: 0.1885\n","Epoch [13/15], Step [4430/5000], Loss: 0.1010\n","Epoch [13/15], Step [4440/5000], Loss: 0.2044\n","Epoch [13/15], Step [4450/5000], Loss: 0.2025\n","Epoch [13/15], Step [4460/5000], Loss: 0.1087\n","Epoch [13/15], Step [4470/5000], Loss: 0.1542\n","Epoch [13/15], Step [4480/5000], Loss: 0.1717\n","Epoch [13/15], Step [4490/5000], Loss: 0.1134\n","Epoch [13/15], Step [4500/5000], Loss: 0.1179\n","Epoch [13/15], Step [4510/5000], Loss: 0.2107\n","Epoch [13/15], Step [4520/5000], Loss: 0.1422\n","Epoch [13/15], Step [4530/5000], Loss: 0.1668\n","Epoch [13/15], Step [4540/5000], Loss: 0.1072\n","Epoch [13/15], Step [4550/5000], Loss: 0.2454\n","Epoch [13/15], Step [4560/5000], Loss: 0.1125\n","Epoch [13/15], Step [4570/5000], Loss: 0.2020\n","Epoch [13/15], Step [4580/5000], Loss: 0.1296\n","Epoch [13/15], Step [4590/5000], Loss: 0.0277\n","Epoch [13/15], Step [4600/5000], Loss: 0.2538\n","Epoch [13/15], Step [4610/5000], Loss: 0.2884\n","Epoch [13/15], Step [4620/5000], Loss: 0.3426\n","Epoch [13/15], Step [4630/5000], Loss: 0.1818\n","Epoch [13/15], Step [4640/5000], Loss: 0.1013\n","Epoch [13/15], Step [4650/5000], Loss: 0.1482\n","Epoch [13/15], Step [4660/5000], Loss: 0.2236\n","Epoch [13/15], Step [4670/5000], Loss: 0.1584\n","Epoch [13/15], Step [4680/5000], Loss: 0.1744\n","Epoch [13/15], Step [4690/5000], Loss: 0.1219\n","Epoch [13/15], Step [4700/5000], Loss: 0.1995\n","Epoch [13/15], Step [4710/5000], Loss: 0.1789\n","Epoch [13/15], Step [4720/5000], Loss: 0.0487\n","Epoch [13/15], Step [4730/5000], Loss: 0.0971\n","Epoch [13/15], Step [4740/5000], Loss: 0.2030\n","Epoch [13/15], Step [4750/5000], Loss: 0.2436\n","Epoch [13/15], Step [4760/5000], Loss: 0.1680\n","Epoch [13/15], Step [4770/5000], Loss: 0.3337\n","Epoch [13/15], Step [4780/5000], Loss: 0.2018\n","Epoch [13/15], Step [4790/5000], Loss: 0.2802\n","Epoch [13/15], Step [4800/5000], Loss: 0.2434\n","Epoch [13/15], Step [4810/5000], Loss: 0.1763\n","Epoch [13/15], Step [4820/5000], Loss: 0.0851\n","Epoch [13/15], Step [4830/5000], Loss: 0.1066\n","Epoch [13/15], Step [4840/5000], Loss: 0.2379\n","Epoch [13/15], Step [4850/5000], Loss: 0.2037\n","Epoch [13/15], Step [4860/5000], Loss: 0.4216\n","Epoch [13/15], Step [4870/5000], Loss: 0.2029\n","Epoch [13/15], Step [4880/5000], Loss: 0.2434\n","Epoch [13/15], Step [4890/5000], Loss: 0.1246\n","Epoch [13/15], Step [4900/5000], Loss: 0.1271\n","Epoch [13/15], Step [4910/5000], Loss: 0.0640\n","Epoch [13/15], Step [4920/5000], Loss: 0.1566\n","Epoch [13/15], Step [4930/5000], Loss: 0.1141\n","Epoch [13/15], Step [4940/5000], Loss: 0.1237\n","Epoch [13/15], Step [4950/5000], Loss: 0.2173\n","Epoch [13/15], Step [4960/5000], Loss: 0.1099\n","Epoch [13/15], Step [4970/5000], Loss: 0.0978\n","Epoch [13/15], Step [4980/5000], Loss: 0.2844\n","Epoch [13/15], Step [4990/5000], Loss: 0.1109\n","Model saved at epoch 13, Loss: 0.1772\n","Epoch [14/15], Step [0/5000], Loss: 0.2103\n","Epoch [14/15], Step [10/5000], Loss: 0.1472\n","Epoch [14/15], Step [20/5000], Loss: 0.1512\n","Epoch [14/15], Step [30/5000], Loss: 0.0918\n","Epoch [14/15], Step [40/5000], Loss: 0.1397\n","Epoch [14/15], Step [50/5000], Loss: 0.2264\n","Epoch [14/15], Step [60/5000], Loss: 0.2289\n","Epoch [14/15], Step [70/5000], Loss: 0.0663\n","Epoch [14/15], Step [80/5000], Loss: 0.1851\n","Epoch [14/15], Step [90/5000], Loss: 0.2131\n","Epoch [14/15], Step [100/5000], Loss: 0.1835\n","Epoch [14/15], Step [110/5000], Loss: 0.1934\n","Epoch [14/15], Step [120/5000], Loss: 0.1817\n","Epoch [14/15], Step [130/5000], Loss: 0.1466\n","Epoch [14/15], Step [140/5000], Loss: 0.2297\n","Epoch [14/15], Step [150/5000], Loss: 0.1819\n","Epoch [14/15], Step [160/5000], Loss: 0.2487\n","Epoch [14/15], Step [170/5000], Loss: 0.1888\n","Epoch [14/15], Step [180/5000], Loss: 0.2970\n","Epoch [14/15], Step [190/5000], Loss: 0.1264\n","Epoch [14/15], Step [200/5000], Loss: 0.1922\n","Epoch [14/15], Step [210/5000], Loss: 0.2378\n","Epoch [14/15], Step [220/5000], Loss: 0.2710\n","Epoch [14/15], Step [230/5000], Loss: 0.1656\n","Epoch [14/15], Step [240/5000], Loss: 0.0951\n","Epoch [14/15], Step [250/5000], Loss: 0.0708\n","Epoch [14/15], Step [260/5000], Loss: 0.0500\n","Epoch [14/15], Step [270/5000], Loss: 0.2002\n","Epoch [14/15], Step [280/5000], Loss: 0.1743\n","Epoch [14/15], Step [290/5000], Loss: 0.1246\n","Epoch [14/15], Step [300/5000], Loss: 0.2080\n","Epoch [14/15], Step [310/5000], Loss: 0.0363\n","Epoch [14/15], Step [320/5000], Loss: 0.1390\n","Epoch [14/15], Step [330/5000], Loss: 0.1402\n","Epoch [14/15], Step [340/5000], Loss: 0.2691\n","Epoch [14/15], Step [350/5000], Loss: 0.2013\n","Epoch [14/15], Step [360/5000], Loss: 0.1445\n","Epoch [14/15], Step [370/5000], Loss: 0.2024\n","Epoch [14/15], Step [380/5000], Loss: 0.1797\n","Epoch [14/15], Step [390/5000], Loss: 0.1147\n","Epoch [14/15], Step [400/5000], Loss: 0.2501\n","Epoch [14/15], Step [410/5000], Loss: 0.3585\n","Epoch [14/15], Step [420/5000], Loss: 0.1015\n","Epoch [14/15], Step [430/5000], Loss: 0.1314\n","Epoch [14/15], Step [440/5000], Loss: 0.2921\n","Epoch [14/15], Step [450/5000], Loss: 0.2136\n","Epoch [14/15], Step [460/5000], Loss: 0.0415\n","Epoch [14/15], Step [470/5000], Loss: 0.1969\n","Epoch [14/15], Step [480/5000], Loss: 0.1063\n","Epoch [14/15], Step [490/5000], Loss: 0.1287\n","Epoch [14/15], Step [500/5000], Loss: 0.1965\n","Epoch [14/15], Step [510/5000], Loss: 0.0890\n","Epoch [14/15], Step [520/5000], Loss: 0.1137\n","Epoch [14/15], Step [530/5000], Loss: 0.1586\n","Epoch [14/15], Step [540/5000], Loss: 0.3912\n","Epoch [14/15], Step [550/5000], Loss: 0.1270\n","Epoch [14/15], Step [560/5000], Loss: 0.1958\n","Epoch [14/15], Step [570/5000], Loss: 0.0530\n","Epoch [14/15], Step [580/5000], Loss: 0.1524\n","Epoch [14/15], Step [590/5000], Loss: 0.1197\n","Epoch [14/15], Step [600/5000], Loss: 0.0932\n","Epoch [14/15], Step [610/5000], Loss: 0.1213\n","Epoch [14/15], Step [620/5000], Loss: 0.1577\n","Epoch [14/15], Step [630/5000], Loss: 0.1223\n","Epoch [14/15], Step [640/5000], Loss: 0.0844\n","Epoch [14/15], Step [650/5000], Loss: 0.1159\n","Epoch [14/15], Step [660/5000], Loss: 0.0789\n","Epoch [14/15], Step [670/5000], Loss: 0.1473\n","Epoch [14/15], Step [680/5000], Loss: 0.2188\n","Epoch [14/15], Step [690/5000], Loss: 0.2985\n","Epoch [14/15], Step [700/5000], Loss: 0.1457\n","Epoch [14/15], Step [710/5000], Loss: 0.2498\n","Epoch [14/15], Step [720/5000], Loss: 0.1724\n","Epoch [14/15], Step [730/5000], Loss: 0.0760\n","Epoch [14/15], Step [740/5000], Loss: 0.2023\n","Epoch [14/15], Step [750/5000], Loss: 0.2447\n","Epoch [14/15], Step [760/5000], Loss: 0.1234\n","Epoch [14/15], Step [770/5000], Loss: 0.1674\n","Epoch [14/15], Step [780/5000], Loss: 0.1659\n","Epoch [14/15], Step [790/5000], Loss: 0.2286\n","Epoch [14/15], Step [800/5000], Loss: 0.2964\n","Epoch [14/15], Step [810/5000], Loss: 0.2320\n","Epoch [14/15], Step [820/5000], Loss: 0.0404\n","Epoch [14/15], Step [830/5000], Loss: 0.2143\n","Epoch [14/15], Step [840/5000], Loss: 0.2794\n","Epoch [14/15], Step [850/5000], Loss: 0.1974\n","Epoch [14/15], Step [860/5000], Loss: 0.2559\n","Epoch [14/15], Step [870/5000], Loss: 0.0407\n","Epoch [14/15], Step [880/5000], Loss: 0.1932\n","Epoch [14/15], Step [890/5000], Loss: 0.0552\n","Epoch [14/15], Step [900/5000], Loss: 0.1945\n","Epoch [14/15], Step [910/5000], Loss: 0.3164\n","Epoch [14/15], Step [920/5000], Loss: 0.1300\n","Epoch [14/15], Step [930/5000], Loss: 0.2648\n","Epoch [14/15], Step [940/5000], Loss: 0.2432\n","Epoch [14/15], Step [950/5000], Loss: 0.1386\n","Epoch [14/15], Step [960/5000], Loss: 0.2240\n","Epoch [14/15], Step [970/5000], Loss: 0.0892\n","Epoch [14/15], Step [980/5000], Loss: 0.1046\n","Epoch [14/15], Step [990/5000], Loss: 0.2272\n","Epoch [14/15], Step [1000/5000], Loss: 0.1994\n","Epoch [14/15], Step [1010/5000], Loss: 0.0807\n","Epoch [14/15], Step [1020/5000], Loss: 0.1384\n","Epoch [14/15], Step [1030/5000], Loss: 0.1626\n","Epoch [14/15], Step [1040/5000], Loss: 0.0601\n","Epoch [14/15], Step [1050/5000], Loss: 0.1960\n","Epoch [14/15], Step [1060/5000], Loss: 0.2479\n","Epoch [14/15], Step [1070/5000], Loss: 0.2595\n","Epoch [14/15], Step [1080/5000], Loss: 0.2815\n","Epoch [14/15], Step [1090/5000], Loss: 0.1590\n","Epoch [14/15], Step [1100/5000], Loss: 0.0612\n","Epoch [14/15], Step [1110/5000], Loss: 0.1485\n","Epoch [14/15], Step [1120/5000], Loss: 0.1655\n","Epoch [14/15], Step [1130/5000], Loss: 0.2612\n","Epoch [14/15], Step [1140/5000], Loss: 0.1577\n","Epoch [14/15], Step [1150/5000], Loss: 0.1527\n","Epoch [14/15], Step [1160/5000], Loss: 0.0824\n","Epoch [14/15], Step [1170/5000], Loss: 0.0945\n","Epoch [14/15], Step [1180/5000], Loss: 0.2018\n","Epoch [14/15], Step [1190/5000], Loss: 0.2281\n","Epoch [14/15], Step [1200/5000], Loss: 0.0596\n","Epoch [14/15], Step [1210/5000], Loss: 0.1507\n","Epoch [14/15], Step [1220/5000], Loss: 0.0823\n","Epoch [14/15], Step [1230/5000], Loss: 0.3445\n","Epoch [14/15], Step [1240/5000], Loss: 0.1690\n","Epoch [14/15], Step [1250/5000], Loss: 0.2321\n","Epoch [14/15], Step [1260/5000], Loss: 0.0779\n","Epoch [14/15], Step [1270/5000], Loss: 0.3016\n","Epoch [14/15], Step [1280/5000], Loss: 0.1216\n","Epoch [14/15], Step [1290/5000], Loss: 0.2088\n","Epoch [14/15], Step [1300/5000], Loss: 0.1492\n","Epoch [14/15], Step [1310/5000], Loss: 0.1687\n","Epoch [14/15], Step [1320/5000], Loss: 0.0521\n","Epoch [14/15], Step [1330/5000], Loss: 0.0627\n","Epoch [14/15], Step [1340/5000], Loss: 0.0595\n","Epoch [14/15], Step [1350/5000], Loss: 0.0697\n","Epoch [14/15], Step [1360/5000], Loss: 0.1583\n","Epoch [14/15], Step [1370/5000], Loss: 0.3111\n","Epoch [14/15], Step [1380/5000], Loss: 0.0947\n","Epoch [14/15], Step [1390/5000], Loss: 0.1606\n","Epoch [14/15], Step [1400/5000], Loss: 0.0985\n","Epoch [14/15], Step [1410/5000], Loss: 0.1690\n","Epoch [14/15], Step [1420/5000], Loss: 0.2538\n","Epoch [14/15], Step [1430/5000], Loss: 0.1653\n","Epoch [14/15], Step [1440/5000], Loss: 0.1356\n","Epoch [14/15], Step [1450/5000], Loss: 0.1503\n","Epoch [14/15], Step [1460/5000], Loss: 0.1152\n","Epoch [14/15], Step [1470/5000], Loss: 0.0856\n","Epoch [14/15], Step [1480/5000], Loss: 0.1290\n","Epoch [14/15], Step [1490/5000], Loss: 0.1900\n","Epoch [14/15], Step [1500/5000], Loss: 0.2107\n","Epoch [14/15], Step [1510/5000], Loss: 0.1673\n","Epoch [14/15], Step [1520/5000], Loss: 0.0809\n","Epoch [14/15], Step [1530/5000], Loss: 0.2388\n","Epoch [14/15], Step [1540/5000], Loss: 0.2008\n","Epoch [14/15], Step [1550/5000], Loss: 0.2509\n","Epoch [14/15], Step [1560/5000], Loss: 0.2233\n","Epoch [14/15], Step [1570/5000], Loss: 0.1117\n","Epoch [14/15], Step [1580/5000], Loss: 0.0943\n","Epoch [14/15], Step [1590/5000], Loss: 0.0937\n","Epoch [14/15], Step [1600/5000], Loss: 0.0203\n","Epoch [14/15], Step [1610/5000], Loss: 0.2241\n","Epoch [14/15], Step [1620/5000], Loss: 0.1738\n","Epoch [14/15], Step [1630/5000], Loss: 0.0330\n","Epoch [14/15], Step [1640/5000], Loss: 0.1261\n","Epoch [14/15], Step [1650/5000], Loss: 0.2545\n","Epoch [14/15], Step [1660/5000], Loss: 0.1215\n","Epoch [14/15], Step [1670/5000], Loss: 0.1729\n","Epoch [14/15], Step [1680/5000], Loss: 0.2031\n","Epoch [14/15], Step [1690/5000], Loss: 0.2168\n","Epoch [14/15], Step [1700/5000], Loss: 0.1546\n","Epoch [14/15], Step [1710/5000], Loss: 0.3024\n","Epoch [14/15], Step [1720/5000], Loss: 0.0971\n","Epoch [14/15], Step [1730/5000], Loss: 0.1108\n","Epoch [14/15], Step [1740/5000], Loss: 0.2390\n","Epoch [14/15], Step [1750/5000], Loss: 0.1065\n","Epoch [14/15], Step [1760/5000], Loss: 0.1247\n","Epoch [14/15], Step [1770/5000], Loss: 0.1252\n","Epoch [14/15], Step [1780/5000], Loss: 0.0536\n","Epoch [14/15], Step [1790/5000], Loss: 0.1636\n","Epoch [14/15], Step [1800/5000], Loss: 0.0994\n","Epoch [14/15], Step [1810/5000], Loss: 0.1692\n","Epoch [14/15], Step [1820/5000], Loss: 0.2197\n","Epoch [14/15], Step [1830/5000], Loss: 0.1570\n","Epoch [14/15], Step [1840/5000], Loss: 0.2599\n","Epoch [14/15], Step [1850/5000], Loss: 0.1984\n","Epoch [14/15], Step [1860/5000], Loss: 0.1408\n","Epoch [14/15], Step [1870/5000], Loss: 0.2485\n","Epoch [14/15], Step [1880/5000], Loss: 0.1598\n","Epoch [14/15], Step [1890/5000], Loss: 0.1040\n","Epoch [14/15], Step [1900/5000], Loss: 0.1994\n","Epoch [14/15], Step [1910/5000], Loss: 0.2663\n","Epoch [14/15], Step [1920/5000], Loss: 0.1169\n","Epoch [14/15], Step [1930/5000], Loss: 0.1579\n","Epoch [14/15], Step [1940/5000], Loss: 0.1484\n","Epoch [14/15], Step [1950/5000], Loss: 0.1542\n","Epoch [14/15], Step [1960/5000], Loss: 0.1765\n","Epoch [14/15], Step [1970/5000], Loss: 0.1730\n","Epoch [14/15], Step [1980/5000], Loss: 0.2804\n","Epoch [14/15], Step [1990/5000], Loss: 0.2863\n","Epoch [14/15], Step [2000/5000], Loss: 0.2525\n","Epoch [14/15], Step [2010/5000], Loss: 0.1308\n","Epoch [14/15], Step [2020/5000], Loss: 0.3092\n","Epoch [14/15], Step [2030/5000], Loss: 0.0916\n","Epoch [14/15], Step [2040/5000], Loss: 0.1934\n","Epoch [14/15], Step [2050/5000], Loss: 0.0229\n","Epoch [14/15], Step [2060/5000], Loss: 0.3031\n","Epoch [14/15], Step [2070/5000], Loss: 0.0711\n","Epoch [14/15], Step [2080/5000], Loss: 0.1824\n","Epoch [14/15], Step [2090/5000], Loss: 0.1543\n","Epoch [14/15], Step [2100/5000], Loss: 0.0770\n","Epoch [14/15], Step [2110/5000], Loss: 0.1758\n","Epoch [14/15], Step [2120/5000], Loss: 0.2077\n","Epoch [14/15], Step [2130/5000], Loss: 0.2850\n","Epoch [14/15], Step [2140/5000], Loss: 0.1019\n","Epoch [14/15], Step [2150/5000], Loss: 0.1140\n","Epoch [14/15], Step [2160/5000], Loss: 0.1211\n","Epoch [14/15], Step [2170/5000], Loss: 0.1136\n","Epoch [14/15], Step [2180/5000], Loss: 0.2044\n","Epoch [14/15], Step [2190/5000], Loss: 0.0721\n","Epoch [14/15], Step [2200/5000], Loss: 0.0589\n","Epoch [14/15], Step [2210/5000], Loss: 0.2003\n","Epoch [14/15], Step [2220/5000], Loss: 0.2092\n","Epoch [14/15], Step [2230/5000], Loss: 0.1356\n","Epoch [14/15], Step [2240/5000], Loss: 0.2795\n","Epoch [14/15], Step [2250/5000], Loss: 0.1613\n","Epoch [14/15], Step [2260/5000], Loss: 0.1642\n","Epoch [14/15], Step [2270/5000], Loss: 0.2048\n","Epoch [14/15], Step [2280/5000], Loss: 0.1741\n","Epoch [14/15], Step [2290/5000], Loss: 0.0902\n","Epoch [14/15], Step [2300/5000], Loss: 0.1042\n","Epoch [14/15], Step [2310/5000], Loss: 0.2308\n","Epoch [14/15], Step [2320/5000], Loss: 0.2630\n","Epoch [14/15], Step [2330/5000], Loss: 0.1351\n","Epoch [14/15], Step [2340/5000], Loss: 0.1735\n","Epoch [14/15], Step [2350/5000], Loss: 0.2072\n","Epoch [14/15], Step [2360/5000], Loss: 0.1728\n","Epoch [14/15], Step [2370/5000], Loss: 0.0756\n","Epoch [14/15], Step [2380/5000], Loss: 0.0400\n","Epoch [14/15], Step [2390/5000], Loss: 0.0872\n","Epoch [14/15], Step [2400/5000], Loss: 0.2374\n","Epoch [14/15], Step [2410/5000], Loss: 0.2493\n","Epoch [14/15], Step [2420/5000], Loss: 0.2444\n","Epoch [14/15], Step [2430/5000], Loss: 0.1577\n","Epoch [14/15], Step [2440/5000], Loss: 0.2428\n","Epoch [14/15], Step [2450/5000], Loss: 0.2077\n","Epoch [14/15], Step [2460/5000], Loss: 0.1340\n","Epoch [14/15], Step [2470/5000], Loss: 0.2030\n","Epoch [14/15], Step [2480/5000], Loss: 0.0831\n","Epoch [14/15], Step [2490/5000], Loss: 0.1215\n","Epoch [14/15], Step [2500/5000], Loss: 0.1225\n","Epoch [14/15], Step [2510/5000], Loss: 0.0944\n","Epoch [14/15], Step [2520/5000], Loss: 0.2484\n","Epoch [14/15], Step [2530/5000], Loss: 0.2389\n","Epoch [14/15], Step [2540/5000], Loss: 0.1219\n","Epoch [14/15], Step [2550/5000], Loss: 0.1557\n","Epoch [14/15], Step [2560/5000], Loss: 0.1456\n","Epoch [14/15], Step [2570/5000], Loss: 0.1439\n","Epoch [14/15], Step [2580/5000], Loss: 0.1166\n","Epoch [14/15], Step [2590/5000], Loss: 0.2567\n","Epoch [14/15], Step [2600/5000], Loss: 0.1078\n","Epoch [14/15], Step [2610/5000], Loss: 0.2043\n","Epoch [14/15], Step [2620/5000], Loss: 0.1622\n","Epoch [14/15], Step [2630/5000], Loss: 0.3033\n","Epoch [14/15], Step [2640/5000], Loss: 0.2051\n","Epoch [14/15], Step [2650/5000], Loss: 0.2598\n","Epoch [14/15], Step [2660/5000], Loss: 0.1355\n","Epoch [14/15], Step [2670/5000], Loss: 0.3482\n","Epoch [14/15], Step [2680/5000], Loss: 0.2388\n","Epoch [14/15], Step [2690/5000], Loss: 0.0823\n","Epoch [14/15], Step [2700/5000], Loss: 0.2947\n","Epoch [14/15], Step [2710/5000], Loss: 0.2121\n","Epoch [14/15], Step [2720/5000], Loss: 0.1712\n","Epoch [14/15], Step [2730/5000], Loss: 0.1044\n","Epoch [14/15], Step [2740/5000], Loss: 0.1811\n","Epoch [14/15], Step [2750/5000], Loss: 0.1783\n","Epoch [14/15], Step [2760/5000], Loss: 0.1415\n","Epoch [14/15], Step [2770/5000], Loss: 0.1562\n","Epoch [14/15], Step [2780/5000], Loss: 0.0601\n","Epoch [14/15], Step [2790/5000], Loss: 0.0635\n","Epoch [14/15], Step [2800/5000], Loss: 0.2443\n","Epoch [14/15], Step [2810/5000], Loss: 0.0175\n","Epoch [14/15], Step [2820/5000], Loss: 0.2827\n","Epoch [14/15], Step [2830/5000], Loss: 0.2573\n","Epoch [14/15], Step [2840/5000], Loss: 0.1315\n","Epoch [14/15], Step [2850/5000], Loss: 0.1765\n","Epoch [14/15], Step [2860/5000], Loss: 0.2133\n","Epoch [14/15], Step [2870/5000], Loss: 0.1902\n","Epoch [14/15], Step [2880/5000], Loss: 0.2941\n","Epoch [14/15], Step [2890/5000], Loss: 0.1547\n","Epoch [14/15], Step [2900/5000], Loss: 0.1126\n","Epoch [14/15], Step [2910/5000], Loss: 0.1395\n","Epoch [14/15], Step [2920/5000], Loss: 0.1498\n","Epoch [14/15], Step [2930/5000], Loss: 0.1389\n","Epoch [14/15], Step [2940/5000], Loss: 0.0894\n","Epoch [14/15], Step [2950/5000], Loss: 0.1649\n","Epoch [14/15], Step [2960/5000], Loss: 0.1378\n","Epoch [14/15], Step [2970/5000], Loss: 0.0766\n","Epoch [14/15], Step [2980/5000], Loss: 0.3692\n","Epoch [14/15], Step [2990/5000], Loss: 0.1393\n","Epoch [14/15], Step [3000/5000], Loss: 0.1904\n","Epoch [14/15], Step [3010/5000], Loss: 0.1730\n","Epoch [14/15], Step [3020/5000], Loss: 0.1617\n","Epoch [14/15], Step [3030/5000], Loss: 0.1239\n","Epoch [14/15], Step [3040/5000], Loss: 0.1418\n","Epoch [14/15], Step [3050/5000], Loss: 0.1351\n","Epoch [14/15], Step [3060/5000], Loss: 0.0792\n","Epoch [14/15], Step [3070/5000], Loss: 0.0930\n","Epoch [14/15], Step [3080/5000], Loss: 0.0804\n","Epoch [14/15], Step [3090/5000], Loss: 0.0946\n","Epoch [14/15], Step [3100/5000], Loss: 0.2991\n","Epoch [14/15], Step [3110/5000], Loss: 0.2007\n","Epoch [14/15], Step [3120/5000], Loss: 0.2037\n","Epoch [14/15], Step [3130/5000], Loss: 0.0681\n","Epoch [14/15], Step [3140/5000], Loss: 0.2140\n","Epoch [14/15], Step [3150/5000], Loss: 0.0630\n","Epoch [14/15], Step [3160/5000], Loss: 0.2193\n","Epoch [14/15], Step [3170/5000], Loss: 0.2630\n","Epoch [14/15], Step [3180/5000], Loss: 0.2234\n","Epoch [14/15], Step [3190/5000], Loss: 0.2036\n","Epoch [14/15], Step [3200/5000], Loss: 0.2064\n","Epoch [14/15], Step [3210/5000], Loss: 0.1108\n","Epoch [14/15], Step [3220/5000], Loss: 0.1937\n","Epoch [14/15], Step [3230/5000], Loss: 0.1992\n","Epoch [14/15], Step [3240/5000], Loss: 0.0809\n","Epoch [14/15], Step [3250/5000], Loss: 0.1929\n","Epoch [14/15], Step [3260/5000], Loss: 0.1377\n","Epoch [14/15], Step [3270/5000], Loss: 0.1408\n","Epoch [14/15], Step [3280/5000], Loss: 0.2120\n","Epoch [14/15], Step [3290/5000], Loss: 0.2468\n","Epoch [14/15], Step [3300/5000], Loss: 0.2246\n","Epoch [14/15], Step [3310/5000], Loss: 0.1840\n","Epoch [14/15], Step [3320/5000], Loss: 0.1448\n","Epoch [14/15], Step [3330/5000], Loss: 0.2355\n","Epoch [14/15], Step [3340/5000], Loss: 0.2075\n","Epoch [14/15], Step [3350/5000], Loss: 0.2358\n","Epoch [14/15], Step [3360/5000], Loss: 0.1334\n","Epoch [14/15], Step [3370/5000], Loss: 0.2100\n","Epoch [14/15], Step [3380/5000], Loss: 0.3773\n","Epoch [14/15], Step [3390/5000], Loss: 0.2446\n","Epoch [14/15], Step [3400/5000], Loss: 0.3735\n","Epoch [14/15], Step [3410/5000], Loss: 0.1907\n","Epoch [14/15], Step [3420/5000], Loss: 0.1811\n","Epoch [14/15], Step [3430/5000], Loss: 0.1132\n","Epoch [14/15], Step [3440/5000], Loss: 0.1936\n","Epoch [14/15], Step [3450/5000], Loss: 0.1091\n","Epoch [14/15], Step [3460/5000], Loss: 0.1672\n","Epoch [14/15], Step [3470/5000], Loss: 0.0910\n","Epoch [14/15], Step [3480/5000], Loss: 0.2661\n","Epoch [14/15], Step [3490/5000], Loss: 0.0334\n","Epoch [14/15], Step [3500/5000], Loss: 0.1005\n","Epoch [14/15], Step [3510/5000], Loss: 0.1931\n","Epoch [14/15], Step [3520/5000], Loss: 0.1622\n","Epoch [14/15], Step [3530/5000], Loss: 0.1940\n","Epoch [14/15], Step [3540/5000], Loss: 0.1524\n","Epoch [14/15], Step [3550/5000], Loss: 0.1553\n","Epoch [14/15], Step [3560/5000], Loss: 0.1558\n","Epoch [14/15], Step [3570/5000], Loss: 0.1042\n","Epoch [14/15], Step [3580/5000], Loss: 0.1496\n","Epoch [14/15], Step [3590/5000], Loss: 0.1222\n","Epoch [14/15], Step [3600/5000], Loss: 0.1361\n","Epoch [14/15], Step [3610/5000], Loss: 0.1508\n","Epoch [14/15], Step [3620/5000], Loss: 0.1434\n","Epoch [14/15], Step [3630/5000], Loss: 0.1221\n","Epoch [14/15], Step [3640/5000], Loss: 0.1859\n","Epoch [14/15], Step [3650/5000], Loss: 0.1715\n","Epoch [14/15], Step [3660/5000], Loss: 0.1569\n","Epoch [14/15], Step [3670/5000], Loss: 0.1947\n","Epoch [14/15], Step [3680/5000], Loss: 0.0795\n","Epoch [14/15], Step [3690/5000], Loss: 0.2495\n","Epoch [14/15], Step [3700/5000], Loss: 0.0968\n","Epoch [14/15], Step [3710/5000], Loss: 0.2048\n","Epoch [14/15], Step [3720/5000], Loss: 0.1283\n","Epoch [14/15], Step [3730/5000], Loss: 0.1353\n","Epoch [14/15], Step [3740/5000], Loss: 0.2127\n","Epoch [14/15], Step [3750/5000], Loss: 0.1200\n","Epoch [14/15], Step [3760/5000], Loss: 0.1548\n","Epoch [14/15], Step [3770/5000], Loss: 0.1708\n","Epoch [14/15], Step [3780/5000], Loss: 0.1072\n","Epoch [14/15], Step [3790/5000], Loss: 0.2079\n","Epoch [14/15], Step [3800/5000], Loss: 0.2980\n","Epoch [14/15], Step [3810/5000], Loss: 0.1848\n","Epoch [14/15], Step [3820/5000], Loss: 0.1786\n","Epoch [14/15], Step [3830/5000], Loss: 0.2831\n","Epoch [14/15], Step [3840/5000], Loss: 0.1705\n","Epoch [14/15], Step [3850/5000], Loss: 0.1468\n","Epoch [14/15], Step [3860/5000], Loss: 0.3116\n","Epoch [14/15], Step [3870/5000], Loss: 0.1572\n","Epoch [14/15], Step [3880/5000], Loss: 0.1654\n","Epoch [14/15], Step [3890/5000], Loss: 0.2275\n","Epoch [14/15], Step [3900/5000], Loss: 0.1666\n","Epoch [14/15], Step [3910/5000], Loss: 0.1603\n","Epoch [14/15], Step [3920/5000], Loss: 0.1041\n","Epoch [14/15], Step [3930/5000], Loss: 0.1792\n","Epoch [14/15], Step [3940/5000], Loss: 0.0807\n","Epoch [14/15], Step [3950/5000], Loss: 0.1495\n","Epoch [14/15], Step [3960/5000], Loss: 0.2396\n","Epoch [14/15], Step [3970/5000], Loss: 0.1583\n","Epoch [14/15], Step [3980/5000], Loss: 0.0288\n","Epoch [14/15], Step [3990/5000], Loss: 0.1225\n","Epoch [14/15], Step [4000/5000], Loss: 0.2744\n","Epoch [14/15], Step [4010/5000], Loss: 0.3069\n","Epoch [14/15], Step [4020/5000], Loss: 0.0147\n","Epoch [14/15], Step [4030/5000], Loss: 0.2557\n","Epoch [14/15], Step [4040/5000], Loss: 0.0900\n","Epoch [14/15], Step [4050/5000], Loss: 0.1946\n","Epoch [14/15], Step [4060/5000], Loss: 0.0917\n","Epoch [14/15], Step [4070/5000], Loss: 0.1688\n","Epoch [14/15], Step [4080/5000], Loss: 0.0673\n","Epoch [14/15], Step [4090/5000], Loss: 0.1258\n","Epoch [14/15], Step [4100/5000], Loss: 0.0762\n","Epoch [14/15], Step [4110/5000], Loss: 0.0871\n","Epoch [14/15], Step [4120/5000], Loss: 0.1160\n","Epoch [14/15], Step [4130/5000], Loss: 0.1453\n","Epoch [14/15], Step [4140/5000], Loss: 0.1415\n","Epoch [14/15], Step [4150/5000], Loss: 0.2634\n","Epoch [14/15], Step [4160/5000], Loss: 0.1116\n","Epoch [14/15], Step [4170/5000], Loss: 0.1499\n","Epoch [14/15], Step [4180/5000], Loss: 0.1958\n","Epoch [14/15], Step [4190/5000], Loss: 0.0348\n","Epoch [14/15], Step [4200/5000], Loss: 0.1111\n","Epoch [14/15], Step [4210/5000], Loss: 0.1746\n","Epoch [14/15], Step [4220/5000], Loss: 0.1502\n","Epoch [14/15], Step [4230/5000], Loss: 0.1863\n","Epoch [14/15], Step [4240/5000], Loss: 0.2521\n","Epoch [14/15], Step [4250/5000], Loss: 0.1909\n","Epoch [14/15], Step [4260/5000], Loss: 0.2316\n","Epoch [14/15], Step [4270/5000], Loss: 0.2289\n","Epoch [14/15], Step [4280/5000], Loss: 0.1436\n","Epoch [14/15], Step [4290/5000], Loss: 0.0870\n","Epoch [14/15], Step [4300/5000], Loss: 0.0656\n","Epoch [14/15], Step [4310/5000], Loss: 0.1688\n","Epoch [14/15], Step [4320/5000], Loss: 0.1331\n","Epoch [14/15], Step [4330/5000], Loss: 0.0799\n","Epoch [14/15], Step [4340/5000], Loss: 0.0696\n","Epoch [14/15], Step [4350/5000], Loss: 0.1701\n","Epoch [14/15], Step [4360/5000], Loss: 0.2036\n","Epoch [14/15], Step [4370/5000], Loss: 0.1225\n","Epoch [14/15], Step [4380/5000], Loss: 0.1356\n","Epoch [14/15], Step [4390/5000], Loss: 0.2878\n","Epoch [14/15], Step [4400/5000], Loss: 0.0596\n","Epoch [14/15], Step [4410/5000], Loss: 0.3312\n","Epoch [14/15], Step [4420/5000], Loss: 0.2709\n","Epoch [14/15], Step [4430/5000], Loss: 0.2429\n","Epoch [14/15], Step [4440/5000], Loss: 0.2060\n","Epoch [14/15], Step [4450/5000], Loss: 0.1262\n","Epoch [14/15], Step [4460/5000], Loss: 0.1151\n","Epoch [14/15], Step [4470/5000], Loss: 0.2020\n","Epoch [14/15], Step [4480/5000], Loss: 0.0986\n","Epoch [14/15], Step [4490/5000], Loss: 0.0696\n","Epoch [14/15], Step [4500/5000], Loss: 0.1859\n","Epoch [14/15], Step [4510/5000], Loss: 0.3112\n","Epoch [14/15], Step [4520/5000], Loss: 0.1616\n","Epoch [14/15], Step [4530/5000], Loss: 0.1259\n","Epoch [14/15], Step [4540/5000], Loss: 0.1735\n","Epoch [14/15], Step [4550/5000], Loss: 0.1428\n","Epoch [14/15], Step [4560/5000], Loss: 0.0711\n","Epoch [14/15], Step [4570/5000], Loss: 0.1479\n","Epoch [14/15], Step [4580/5000], Loss: 0.0748\n","Epoch [14/15], Step [4590/5000], Loss: 0.1969\n","Epoch [14/15], Step [4600/5000], Loss: 0.0923\n","Epoch [14/15], Step [4610/5000], Loss: 0.1905\n","Epoch [14/15], Step [4620/5000], Loss: 0.1693\n","Epoch [14/15], Step [4630/5000], Loss: 0.1400\n","Epoch [14/15], Step [4640/5000], Loss: 0.1276\n","Epoch [14/15], Step [4650/5000], Loss: 0.1868\n","Epoch [14/15], Step [4660/5000], Loss: 0.2087\n","Epoch [14/15], Step [4670/5000], Loss: 0.1232\n","Epoch [14/15], Step [4680/5000], Loss: 0.0863\n","Epoch [14/15], Step [4690/5000], Loss: 0.1048\n","Epoch [14/15], Step [4700/5000], Loss: 0.0844\n","Epoch [14/15], Step [4710/5000], Loss: 0.0882\n","Epoch [14/15], Step [4720/5000], Loss: 0.1986\n","Epoch [14/15], Step [4730/5000], Loss: 0.0943\n","Epoch [14/15], Step [4740/5000], Loss: 0.1675\n","Epoch [14/15], Step [4750/5000], Loss: 0.1407\n","Epoch [14/15], Step [4760/5000], Loss: 0.2394\n","Epoch [14/15], Step [4770/5000], Loss: 0.1415\n","Epoch [14/15], Step [4780/5000], Loss: 0.2465\n","Epoch [14/15], Step [4790/5000], Loss: 0.0865\n","Epoch [14/15], Step [4800/5000], Loss: 0.1472\n","Epoch [14/15], Step [4810/5000], Loss: 0.2040\n","Epoch [14/15], Step [4820/5000], Loss: 0.2108\n","Epoch [14/15], Step [4830/5000], Loss: 0.2365\n","Epoch [14/15], Step [4840/5000], Loss: 0.1719\n","Epoch [14/15], Step [4850/5000], Loss: 0.2444\n","Epoch [14/15], Step [4860/5000], Loss: 0.1376\n","Epoch [14/15], Step [4870/5000], Loss: 0.1215\n","Epoch [14/15], Step [4880/5000], Loss: 0.3764\n","Epoch [14/15], Step [4890/5000], Loss: 0.1998\n","Epoch [14/15], Step [4900/5000], Loss: 0.1255\n","Epoch [14/15], Step [4910/5000], Loss: 0.1590\n","Epoch [14/15], Step [4920/5000], Loss: 0.2424\n","Epoch [14/15], Step [4930/5000], Loss: 0.1056\n","Epoch [14/15], Step [4940/5000], Loss: 0.1728\n","Epoch [14/15], Step [4950/5000], Loss: 0.1090\n","Epoch [14/15], Step [4960/5000], Loss: 0.2277\n","Epoch [14/15], Step [4970/5000], Loss: 0.3503\n","Epoch [14/15], Step [4980/5000], Loss: 0.1900\n","Epoch [14/15], Step [4990/5000], Loss: 0.1582\n","Model saved at epoch 14, Loss: 0.1724\n","Epoch [15/15], Step [0/5000], Loss: 0.1644\n","Epoch [15/15], Step [10/5000], Loss: 0.1459\n","Epoch [15/15], Step [20/5000], Loss: 0.2803\n","Epoch [15/15], Step [30/5000], Loss: 0.2226\n","Epoch [15/15], Step [40/5000], Loss: 0.1391\n","Epoch [15/15], Step [50/5000], Loss: 0.3066\n","Epoch [15/15], Step [60/5000], Loss: 0.1093\n","Epoch [15/15], Step [70/5000], Loss: 0.1827\n","Epoch [15/15], Step [80/5000], Loss: 0.1282\n","Epoch [15/15], Step [90/5000], Loss: 0.0694\n","Epoch [15/15], Step [100/5000], Loss: 0.1427\n","Epoch [15/15], Step [110/5000], Loss: 0.0784\n","Epoch [15/15], Step [120/5000], Loss: 0.1553\n","Epoch [15/15], Step [130/5000], Loss: 0.1442\n","Epoch [15/15], Step [140/5000], Loss: 0.1310\n","Epoch [15/15], Step [150/5000], Loss: 0.1590\n","Epoch [15/15], Step [160/5000], Loss: 0.0722\n","Epoch [15/15], Step [170/5000], Loss: 0.1769\n","Epoch [15/15], Step [180/5000], Loss: 0.1967\n","Epoch [15/15], Step [190/5000], Loss: 0.2193\n","Epoch [15/15], Step [200/5000], Loss: 0.1697\n","Epoch [15/15], Step [210/5000], Loss: 0.1405\n","Epoch [15/15], Step [220/5000], Loss: 0.1769\n","Epoch [15/15], Step [230/5000], Loss: 0.2869\n","Epoch [15/15], Step [240/5000], Loss: 0.1762\n","Epoch [15/15], Step [250/5000], Loss: 0.1939\n","Epoch [15/15], Step [260/5000], Loss: 0.3084\n","Epoch [15/15], Step [270/5000], Loss: 0.1515\n","Epoch [15/15], Step [280/5000], Loss: 0.2567\n","Epoch [15/15], Step [290/5000], Loss: 0.0929\n","Epoch [15/15], Step [300/5000], Loss: 0.1361\n","Epoch [15/15], Step [310/5000], Loss: 0.1702\n","Epoch [15/15], Step [320/5000], Loss: 0.1729\n","Epoch [15/15], Step [330/5000], Loss: 0.2690\n","Epoch [15/15], Step [340/5000], Loss: 0.2132\n","Epoch [15/15], Step [350/5000], Loss: 0.1459\n","Epoch [15/15], Step [360/5000], Loss: 0.2021\n","Epoch [15/15], Step [370/5000], Loss: 0.2218\n","Epoch [15/15], Step [380/5000], Loss: 0.0892\n","Epoch [15/15], Step [390/5000], Loss: 0.1793\n","Epoch [15/15], Step [400/5000], Loss: 0.2366\n","Epoch [15/15], Step [410/5000], Loss: 0.2198\n","Epoch [15/15], Step [420/5000], Loss: 0.2029\n","Epoch [15/15], Step [430/5000], Loss: 0.1752\n","Epoch [15/15], Step [440/5000], Loss: 0.0854\n","Epoch [15/15], Step [450/5000], Loss: 0.2954\n","Epoch [15/15], Step [460/5000], Loss: 0.1160\n","Epoch [15/15], Step [470/5000], Loss: 0.2183\n","Epoch [15/15], Step [480/5000], Loss: 0.2557\n","Epoch [15/15], Step [490/5000], Loss: 0.0620\n","Epoch [15/15], Step [500/5000], Loss: 0.1975\n","Epoch [15/15], Step [510/5000], Loss: 0.2586\n","Epoch [15/15], Step [520/5000], Loss: 0.2988\n","Epoch [15/15], Step [530/5000], Loss: 0.1343\n","Epoch [15/15], Step [540/5000], Loss: 0.1821\n","Epoch [15/15], Step [550/5000], Loss: 0.2194\n","Epoch [15/15], Step [560/5000], Loss: 0.3117\n","Epoch [15/15], Step [570/5000], Loss: 0.1675\n","Epoch [15/15], Step [580/5000], Loss: 0.3073\n","Epoch [15/15], Step [590/5000], Loss: 0.0727\n","Epoch [15/15], Step [600/5000], Loss: 0.0773\n","Epoch [15/15], Step [610/5000], Loss: 0.1813\n","Epoch [15/15], Step [620/5000], Loss: 0.1621\n","Epoch [15/15], Step [630/5000], Loss: 0.1818\n","Epoch [15/15], Step [640/5000], Loss: 0.2097\n","Epoch [15/15], Step [650/5000], Loss: 0.1834\n","Epoch [15/15], Step [660/5000], Loss: 0.1683\n","Epoch [15/15], Step [670/5000], Loss: 0.0430\n","Epoch [15/15], Step [680/5000], Loss: 0.0644\n","Epoch [15/15], Step [690/5000], Loss: 0.2081\n","Epoch [15/15], Step [700/5000], Loss: 0.1005\n","Epoch [15/15], Step [710/5000], Loss: 0.1141\n","Epoch [15/15], Step [720/5000], Loss: 0.2452\n","Epoch [15/15], Step [730/5000], Loss: 0.0906\n","Epoch [15/15], Step [740/5000], Loss: 0.3340\n","Epoch [15/15], Step [750/5000], Loss: 0.1907\n","Epoch [15/15], Step [760/5000], Loss: 0.1219\n","Epoch [15/15], Step [770/5000], Loss: 0.1435\n","Epoch [15/15], Step [780/5000], Loss: 0.2007\n","Epoch [15/15], Step [790/5000], Loss: 0.0752\n","Epoch [15/15], Step [800/5000], Loss: 0.0968\n","Epoch [15/15], Step [810/5000], Loss: 0.2037\n","Epoch [15/15], Step [820/5000], Loss: 0.2013\n","Epoch [15/15], Step [830/5000], Loss: 0.1439\n","Epoch [15/15], Step [840/5000], Loss: 0.1568\n","Epoch [15/15], Step [850/5000], Loss: 0.3196\n","Epoch [15/15], Step [860/5000], Loss: 0.3414\n","Epoch [15/15], Step [870/5000], Loss: 0.1791\n","Epoch [15/15], Step [880/5000], Loss: 0.1824\n","Epoch [15/15], Step [890/5000], Loss: 0.1343\n","Epoch [15/15], Step [900/5000], Loss: 0.2196\n","Epoch [15/15], Step [910/5000], Loss: 0.1194\n","Epoch [15/15], Step [920/5000], Loss: 0.3127\n","Epoch [15/15], Step [930/5000], Loss: 0.2120\n","Epoch [15/15], Step [940/5000], Loss: 0.2158\n","Epoch [15/15], Step [950/5000], Loss: 0.0710\n","Epoch [15/15], Step [960/5000], Loss: 0.2404\n","Epoch [15/15], Step [970/5000], Loss: 0.2317\n","Epoch [15/15], Step [980/5000], Loss: 0.1261\n","Epoch [15/15], Step [990/5000], Loss: 0.1229\n","Epoch [15/15], Step [1000/5000], Loss: 0.1887\n","Epoch [15/15], Step [1010/5000], Loss: 0.0254\n","Epoch [15/15], Step [1020/5000], Loss: 0.2501\n","Epoch [15/15], Step [1030/5000], Loss: 0.2030\n","Epoch [15/15], Step [1040/5000], Loss: 0.1462\n","Epoch [15/15], Step [1050/5000], Loss: 0.1857\n","Epoch [15/15], Step [1060/5000], Loss: 0.1551\n","Epoch [15/15], Step [1070/5000], Loss: 0.1977\n","Epoch [15/15], Step [1080/5000], Loss: 0.1680\n","Epoch [15/15], Step [1090/5000], Loss: 0.2268\n","Epoch [15/15], Step [1100/5000], Loss: 0.0643\n","Epoch [15/15], Step [1110/5000], Loss: 0.1764\n","Epoch [15/15], Step [1120/5000], Loss: 0.2225\n","Epoch [15/15], Step [1130/5000], Loss: 0.0921\n","Epoch [15/15], Step [1140/5000], Loss: 0.2227\n","Epoch [15/15], Step [1150/5000], Loss: 0.1094\n","Epoch [15/15], Step [1160/5000], Loss: 0.1739\n","Epoch [15/15], Step [1170/5000], Loss: 0.2123\n","Epoch [15/15], Step [1180/5000], Loss: 0.1840\n","Epoch [15/15], Step [1190/5000], Loss: 0.1039\n","Epoch [15/15], Step [1200/5000], Loss: 0.3037\n","Epoch [15/15], Step [1210/5000], Loss: 0.1867\n","Epoch [15/15], Step [1220/5000], Loss: 0.3578\n","Epoch [15/15], Step [1230/5000], Loss: 0.2684\n","Epoch [15/15], Step [1240/5000], Loss: 0.1713\n","Epoch [15/15], Step [1250/5000], Loss: 0.1468\n","Epoch [15/15], Step [1260/5000], Loss: 0.1738\n","Epoch [15/15], Step [1270/5000], Loss: 0.1050\n","Epoch [15/15], Step [1280/5000], Loss: 0.1599\n","Epoch [15/15], Step [1290/5000], Loss: 0.1797\n","Epoch [15/15], Step [1300/5000], Loss: 0.2021\n","Epoch [15/15], Step [1310/5000], Loss: 0.0989\n","Epoch [15/15], Step [1320/5000], Loss: 0.1466\n","Epoch [15/15], Step [1330/5000], Loss: 0.3526\n","Epoch [15/15], Step [1340/5000], Loss: 0.2864\n","Epoch [15/15], Step [1350/5000], Loss: 0.1555\n","Epoch [15/15], Step [1360/5000], Loss: 0.3201\n","Epoch [15/15], Step [1370/5000], Loss: 0.0881\n","Epoch [15/15], Step [1380/5000], Loss: 0.3974\n","Epoch [15/15], Step [1390/5000], Loss: 0.2426\n","Epoch [15/15], Step [1400/5000], Loss: 0.1149\n","Epoch [15/15], Step [1410/5000], Loss: 0.1593\n","Epoch [15/15], Step [1420/5000], Loss: 0.2687\n","Epoch [15/15], Step [1430/5000], Loss: 0.2443\n","Epoch [15/15], Step [1440/5000], Loss: 0.2611\n","Epoch [15/15], Step [1450/5000], Loss: 0.1605\n","Epoch [15/15], Step [1460/5000], Loss: 0.2525\n","Epoch [15/15], Step [1470/5000], Loss: 0.2790\n","Epoch [15/15], Step [1480/5000], Loss: 0.1420\n","Epoch [15/15], Step [1490/5000], Loss: 0.2148\n","Epoch [15/15], Step [1500/5000], Loss: 0.2340\n","Epoch [15/15], Step [1510/5000], Loss: 0.1823\n","Epoch [15/15], Step [1520/5000], Loss: 0.1527\n","Epoch [15/15], Step [1530/5000], Loss: 0.2423\n","Epoch [15/15], Step [1540/5000], Loss: 0.2576\n","Epoch [15/15], Step [1550/5000], Loss: 0.0604\n","Epoch [15/15], Step [1560/5000], Loss: 0.2773\n","Epoch [15/15], Step [1570/5000], Loss: 0.1742\n","Epoch [15/15], Step [1580/5000], Loss: 0.1351\n","Epoch [15/15], Step [1590/5000], Loss: 0.1754\n","Epoch [15/15], Step [1600/5000], Loss: 0.2151\n","Epoch [15/15], Step [1610/5000], Loss: 0.1387\n","Epoch [15/15], Step [1620/5000], Loss: 0.1469\n","Epoch [15/15], Step [1630/5000], Loss: 0.1647\n","Epoch [15/15], Step [1640/5000], Loss: 0.0405\n","Epoch [15/15], Step [1650/5000], Loss: 0.1456\n","Epoch [15/15], Step [1660/5000], Loss: 0.1438\n","Epoch [15/15], Step [1670/5000], Loss: 0.1167\n","Epoch [15/15], Step [1680/5000], Loss: 0.1395\n","Epoch [15/15], Step [1690/5000], Loss: 0.1549\n","Epoch [15/15], Step [1700/5000], Loss: 0.2704\n","Epoch [15/15], Step [1710/5000], Loss: 0.3037\n","Epoch [15/15], Step [1720/5000], Loss: 0.0945\n","Epoch [15/15], Step [1730/5000], Loss: 0.1095\n","Epoch [15/15], Step [1740/5000], Loss: 0.2288\n","Epoch [15/15], Step [1750/5000], Loss: 0.1400\n","Epoch [15/15], Step [1760/5000], Loss: 0.1082\n","Epoch [15/15], Step [1770/5000], Loss: 0.1558\n","Epoch [15/15], Step [1780/5000], Loss: 0.1191\n","Epoch [15/15], Step [1790/5000], Loss: 0.2109\n","Epoch [15/15], Step [1800/5000], Loss: 0.2092\n","Epoch [15/15], Step [1810/5000], Loss: 0.1382\n","Epoch [15/15], Step [1820/5000], Loss: 0.1128\n","Epoch [15/15], Step [1830/5000], Loss: 0.1350\n","Epoch [15/15], Step [1840/5000], Loss: 0.0502\n","Epoch [15/15], Step [1850/5000], Loss: 0.1459\n","Epoch [15/15], Step [1860/5000], Loss: 0.1943\n","Epoch [15/15], Step [1870/5000], Loss: 0.2115\n","Epoch [15/15], Step [1880/5000], Loss: 0.1707\n","Epoch [15/15], Step [1890/5000], Loss: 0.1325\n","Epoch [15/15], Step [1900/5000], Loss: 0.0802\n","Epoch [15/15], Step [1910/5000], Loss: 0.2832\n","Epoch [15/15], Step [1920/5000], Loss: 0.0921\n","Epoch [15/15], Step [1930/5000], Loss: 0.1045\n","Epoch [15/15], Step [1940/5000], Loss: 0.1696\n","Epoch [15/15], Step [1950/5000], Loss: 0.1663\n","Epoch [15/15], Step [1960/5000], Loss: 0.1656\n","Epoch [15/15], Step [1970/5000], Loss: 0.0989\n","Epoch [15/15], Step [1980/5000], Loss: 0.2737\n","Epoch [15/15], Step [1990/5000], Loss: 0.1082\n","Epoch [15/15], Step [2000/5000], Loss: 0.2393\n","Epoch [15/15], Step [2010/5000], Loss: 0.2474\n","Epoch [15/15], Step [2020/5000], Loss: 0.1373\n","Epoch [15/15], Step [2030/5000], Loss: 0.2056\n","Epoch [15/15], Step [2040/5000], Loss: 0.5308\n","Epoch [15/15], Step [2050/5000], Loss: 0.1881\n","Epoch [15/15], Step [2060/5000], Loss: 0.1083\n","Epoch [15/15], Step [2070/5000], Loss: 0.1295\n","Epoch [15/15], Step [2080/5000], Loss: 0.1576\n","Epoch [15/15], Step [2090/5000], Loss: 0.1388\n","Epoch [15/15], Step [2100/5000], Loss: 0.1089\n","Epoch [15/15], Step [2110/5000], Loss: 0.3448\n","Epoch [15/15], Step [2120/5000], Loss: 0.3977\n","Epoch [15/15], Step [2130/5000], Loss: 0.0760\n","Epoch [15/15], Step [2140/5000], Loss: 0.2136\n","Epoch [15/15], Step [2150/5000], Loss: 0.1366\n","Epoch [15/15], Step [2160/5000], Loss: 0.1708\n","Epoch [15/15], Step [2170/5000], Loss: 0.2462\n","Epoch [15/15], Step [2180/5000], Loss: 0.2350\n","Epoch [15/15], Step [2190/5000], Loss: 0.2526\n","Epoch [15/15], Step [2200/5000], Loss: 0.0945\n","Epoch [15/15], Step [2210/5000], Loss: 0.2265\n","Epoch [15/15], Step [2220/5000], Loss: 0.2420\n","Epoch [15/15], Step [2230/5000], Loss: 0.1516\n","Epoch [15/15], Step [2240/5000], Loss: 0.2409\n","Epoch [15/15], Step [2250/5000], Loss: 0.1428\n","Epoch [15/15], Step [2260/5000], Loss: 0.2018\n","Epoch [15/15], Step [2270/5000], Loss: 0.0453\n","Epoch [15/15], Step [2280/5000], Loss: 0.1290\n","Epoch [15/15], Step [2290/5000], Loss: 0.2401\n","Epoch [15/15], Step [2300/5000], Loss: 0.1319\n","Epoch [15/15], Step [2310/5000], Loss: 0.2709\n","Epoch [15/15], Step [2320/5000], Loss: 0.1610\n","Epoch [15/15], Step [2330/5000], Loss: 0.1965\n","Epoch [15/15], Step [2340/5000], Loss: 0.2323\n","Epoch [15/15], Step [2350/5000], Loss: 0.1136\n","Epoch [15/15], Step [2360/5000], Loss: 0.2809\n","Epoch [15/15], Step [2370/5000], Loss: 0.0977\n","Epoch [15/15], Step [2380/5000], Loss: 0.1247\n","Epoch [15/15], Step [2390/5000], Loss: 0.2223\n","Epoch [15/15], Step [2400/5000], Loss: 0.1791\n","Epoch [15/15], Step [2410/5000], Loss: 0.0484\n","Epoch [15/15], Step [2420/5000], Loss: 0.0172\n","Epoch [15/15], Step [2430/5000], Loss: 0.1195\n","Epoch [15/15], Step [2440/5000], Loss: 0.0960\n","Epoch [15/15], Step [2450/5000], Loss: 0.0540\n","Epoch [15/15], Step [2460/5000], Loss: 0.2856\n","Epoch [15/15], Step [2470/5000], Loss: 0.2019\n","Epoch [15/15], Step [2480/5000], Loss: 0.1518\n","Epoch [15/15], Step [2490/5000], Loss: 0.1193\n","Epoch [15/15], Step [2500/5000], Loss: 0.2724\n","Epoch [15/15], Step [2510/5000], Loss: 0.2260\n","Epoch [15/15], Step [2520/5000], Loss: 0.1915\n","Epoch [15/15], Step [2530/5000], Loss: 0.1198\n","Epoch [15/15], Step [2540/5000], Loss: 0.0917\n","Epoch [15/15], Step [2550/5000], Loss: 0.2008\n","Epoch [15/15], Step [2560/5000], Loss: 0.2143\n","Epoch [15/15], Step [2570/5000], Loss: 0.1427\n","Epoch [15/15], Step [2580/5000], Loss: 0.0884\n","Epoch [15/15], Step [2590/5000], Loss: 0.1134\n","Epoch [15/15], Step [2600/5000], Loss: 0.1098\n","Epoch [15/15], Step [2610/5000], Loss: 0.1271\n","Epoch [15/15], Step [2620/5000], Loss: 0.2117\n","Epoch [15/15], Step [2630/5000], Loss: 0.1406\n","Epoch [15/15], Step [2640/5000], Loss: 0.1469\n","Epoch [15/15], Step [2650/5000], Loss: 0.0833\n","Epoch [15/15], Step [2660/5000], Loss: 0.1923\n","Epoch [15/15], Step [2670/5000], Loss: 0.1964\n","Epoch [15/15], Step [2680/5000], Loss: 0.0950\n","Epoch [15/15], Step [2690/5000], Loss: 0.1830\n","Epoch [15/15], Step [2700/5000], Loss: 0.1381\n","Epoch [15/15], Step [2710/5000], Loss: 0.2504\n","Epoch [15/15], Step [2720/5000], Loss: 0.1172\n","Epoch [15/15], Step [2730/5000], Loss: 0.1221\n","Epoch [15/15], Step [2740/5000], Loss: 0.3373\n","Epoch [15/15], Step [2750/5000], Loss: 0.1957\n","Epoch [15/15], Step [2760/5000], Loss: 0.1780\n","Epoch [15/15], Step [2770/5000], Loss: 0.1209\n","Epoch [15/15], Step [2780/5000], Loss: 0.0724\n","Epoch [15/15], Step [2790/5000], Loss: 0.1203\n","Epoch [15/15], Step [2800/5000], Loss: 0.1329\n","Epoch [15/15], Step [2810/5000], Loss: 0.0434\n","Epoch [15/15], Step [2820/5000], Loss: 0.0472\n","Epoch [15/15], Step [2830/5000], Loss: 0.1763\n","Epoch [15/15], Step [2840/5000], Loss: 0.2721\n","Epoch [15/15], Step [2850/5000], Loss: 0.1817\n","Epoch [15/15], Step [2860/5000], Loss: 0.2341\n","Epoch [15/15], Step [2870/5000], Loss: 0.1520\n","Epoch [15/15], Step [2880/5000], Loss: 0.1520\n","Epoch [15/15], Step [2890/5000], Loss: 0.2042\n","Epoch [15/15], Step [2900/5000], Loss: 0.2697\n","Epoch [15/15], Step [2910/5000], Loss: 0.0458\n","Epoch [15/15], Step [2920/5000], Loss: 0.1823\n","Epoch [15/15], Step [2930/5000], Loss: 0.1421\n","Epoch [15/15], Step [2940/5000], Loss: 0.1256\n","Epoch [15/15], Step [2950/5000], Loss: 0.1048\n","Epoch [15/15], Step [2960/5000], Loss: 0.1845\n","Epoch [15/15], Step [2970/5000], Loss: 0.0538\n","Epoch [15/15], Step [2980/5000], Loss: 0.2346\n","Epoch [15/15], Step [2990/5000], Loss: 0.0968\n","Epoch [15/15], Step [3000/5000], Loss: 0.1405\n","Epoch [15/15], Step [3010/5000], Loss: 0.1158\n","Epoch [15/15], Step [3020/5000], Loss: 0.2575\n","Epoch [15/15], Step [3030/5000], Loss: 0.1731\n","Epoch [15/15], Step [3040/5000], Loss: 0.1535\n","Epoch [15/15], Step [3050/5000], Loss: 0.2174\n","Epoch [15/15], Step [3060/5000], Loss: 0.1344\n","Epoch [15/15], Step [3070/5000], Loss: 0.1347\n","Epoch [15/15], Step [3080/5000], Loss: 0.1971\n","Epoch [15/15], Step [3090/5000], Loss: 0.1388\n","Epoch [15/15], Step [3100/5000], Loss: 0.0785\n","Epoch [15/15], Step [3110/5000], Loss: 0.0868\n","Epoch [15/15], Step [3120/5000], Loss: 0.1737\n","Epoch [15/15], Step [3130/5000], Loss: 0.1874\n","Epoch [15/15], Step [3140/5000], Loss: 0.1894\n","Epoch [15/15], Step [3150/5000], Loss: 0.1269\n","Epoch [15/15], Step [3160/5000], Loss: 0.1900\n","Epoch [15/15], Step [3170/5000], Loss: 0.2877\n","Epoch [15/15], Step [3180/5000], Loss: 0.1259\n","Epoch [15/15], Step [3190/5000], Loss: 0.0962\n","Epoch [15/15], Step [3200/5000], Loss: 0.0915\n","Epoch [15/15], Step [3210/5000], Loss: 0.2078\n","Epoch [15/15], Step [3220/5000], Loss: 0.1857\n","Epoch [15/15], Step [3230/5000], Loss: 0.1750\n","Epoch [15/15], Step [3240/5000], Loss: 0.2251\n","Epoch [15/15], Step [3250/5000], Loss: 0.1136\n","Epoch [15/15], Step [3260/5000], Loss: 0.1195\n","Epoch [15/15], Step [3270/5000], Loss: 0.0678\n","Epoch [15/15], Step [3280/5000], Loss: 0.3810\n","Epoch [15/15], Step [3290/5000], Loss: 0.1126\n","Epoch [15/15], Step [3300/5000], Loss: 0.1519\n","Epoch [15/15], Step [3310/5000], Loss: 0.2353\n","Epoch [15/15], Step [3320/5000], Loss: 0.2155\n","Epoch [15/15], Step [3330/5000], Loss: 0.0413\n","Epoch [15/15], Step [3340/5000], Loss: 0.1214\n","Epoch [15/15], Step [3350/5000], Loss: 0.1386\n","Epoch [15/15], Step [3360/5000], Loss: 0.2578\n","Epoch [15/15], Step [3370/5000], Loss: 0.1828\n","Epoch [15/15], Step [3380/5000], Loss: 0.1838\n","Epoch [15/15], Step [3390/5000], Loss: 0.1296\n","Epoch [15/15], Step [3400/5000], Loss: 0.1862\n","Epoch [15/15], Step [3410/5000], Loss: 0.1761\n","Epoch [15/15], Step [3420/5000], Loss: 0.2880\n","Epoch [15/15], Step [3430/5000], Loss: 0.0931\n","Epoch [15/15], Step [3440/5000], Loss: 0.0481\n","Epoch [15/15], Step [3450/5000], Loss: 0.3047\n","Epoch [15/15], Step [3460/5000], Loss: 0.1132\n","Epoch [15/15], Step [3470/5000], Loss: 0.2027\n","Epoch [15/15], Step [3480/5000], Loss: 0.1511\n","Epoch [15/15], Step [3490/5000], Loss: 0.0635\n","Epoch [15/15], Step [3500/5000], Loss: 0.1261\n","Epoch [15/15], Step [3510/5000], Loss: 0.1436\n","Epoch [15/15], Step [3520/5000], Loss: 0.3397\n","Epoch [15/15], Step [3530/5000], Loss: 0.2062\n","Epoch [15/15], Step [3540/5000], Loss: 0.2304\n","Epoch [15/15], Step [3550/5000], Loss: 0.1822\n","Epoch [15/15], Step [3560/5000], Loss: 0.1662\n","Epoch [15/15], Step [3570/5000], Loss: 0.1292\n","Epoch [15/15], Step [3580/5000], Loss: 0.1487\n","Epoch [15/15], Step [3590/5000], Loss: 0.2372\n","Epoch [15/15], Step [3600/5000], Loss: 0.1548\n","Epoch [15/15], Step [3610/5000], Loss: 0.0405\n","Epoch [15/15], Step [3620/5000], Loss: 0.1579\n","Epoch [15/15], Step [3630/5000], Loss: 0.2386\n","Epoch [15/15], Step [3640/5000], Loss: 0.1822\n","Epoch [15/15], Step [3650/5000], Loss: 0.1487\n","Epoch [15/15], Step [3660/5000], Loss: 0.1323\n","Epoch [15/15], Step [3670/5000], Loss: 0.1442\n","Epoch [15/15], Step [3680/5000], Loss: 0.0509\n","Epoch [15/15], Step [3690/5000], Loss: 0.1290\n","Epoch [15/15], Step [3700/5000], Loss: 0.2138\n","Epoch [15/15], Step [3710/5000], Loss: 0.0536\n","Epoch [15/15], Step [3720/5000], Loss: 0.0600\n","Epoch [15/15], Step [3730/5000], Loss: 0.1655\n","Epoch [15/15], Step [3740/5000], Loss: 0.1307\n","Epoch [15/15], Step [3750/5000], Loss: 0.0973\n","Epoch [15/15], Step [3760/5000], Loss: 0.1212\n","Epoch [15/15], Step [3770/5000], Loss: 0.2072\n","Epoch [15/15], Step [3780/5000], Loss: 0.2826\n","Epoch [15/15], Step [3790/5000], Loss: 0.1775\n","Epoch [15/15], Step [3800/5000], Loss: 0.2230\n","Epoch [15/15], Step [3810/5000], Loss: 0.1089\n","Epoch [15/15], Step [3820/5000], Loss: 0.1688\n","Epoch [15/15], Step [3830/5000], Loss: 0.1590\n","Epoch [15/15], Step [3840/5000], Loss: 0.1707\n","Epoch [15/15], Step [3850/5000], Loss: 0.1883\n","Epoch [15/15], Step [3860/5000], Loss: 0.2034\n","Epoch [15/15], Step [3870/5000], Loss: 0.1135\n","Epoch [15/15], Step [3880/5000], Loss: 0.1638\n","Epoch [15/15], Step [3890/5000], Loss: 0.1241\n","Epoch [15/15], Step [3900/5000], Loss: 0.2050\n","Epoch [15/15], Step [3910/5000], Loss: 0.2508\n","Epoch [15/15], Step [3920/5000], Loss: 0.1961\n","Epoch [15/15], Step [3930/5000], Loss: 0.1732\n","Epoch [15/15], Step [3940/5000], Loss: 0.2973\n","Epoch [15/15], Step [3950/5000], Loss: 0.1823\n","Epoch [15/15], Step [3960/5000], Loss: 0.2168\n","Epoch [15/15], Step [3970/5000], Loss: 0.2844\n","Epoch [15/15], Step [3980/5000], Loss: 0.3227\n","Epoch [15/15], Step [3990/5000], Loss: 0.1924\n","Epoch [15/15], Step [4000/5000], Loss: 0.2299\n","Epoch [15/15], Step [4010/5000], Loss: 0.1251\n","Epoch [15/15], Step [4020/5000], Loss: 0.3556\n","Epoch [15/15], Step [4030/5000], Loss: 0.2945\n","Epoch [15/15], Step [4040/5000], Loss: 0.0811\n","Epoch [15/15], Step [4050/5000], Loss: 0.1737\n","Epoch [15/15], Step [4060/5000], Loss: 0.1510\n","Epoch [15/15], Step [4070/5000], Loss: 0.1722\n","Epoch [15/15], Step [4080/5000], Loss: 0.2009\n","Epoch [15/15], Step [4090/5000], Loss: 0.2294\n","Epoch [15/15], Step [4100/5000], Loss: 0.1819\n","Epoch [15/15], Step [4110/5000], Loss: 0.1227\n","Epoch [15/15], Step [4120/5000], Loss: 0.1862\n","Epoch [15/15], Step [4130/5000], Loss: 0.1630\n","Epoch [15/15], Step [4140/5000], Loss: 0.0937\n","Epoch [15/15], Step [4150/5000], Loss: 0.2158\n","Epoch [15/15], Step [4160/5000], Loss: 0.1118\n","Epoch [15/15], Step [4170/5000], Loss: 0.0686\n","Epoch [15/15], Step [4180/5000], Loss: 0.1623\n","Epoch [15/15], Step [4190/5000], Loss: 0.0906\n","Epoch [15/15], Step [4200/5000], Loss: 0.0820\n","Epoch [15/15], Step [4210/5000], Loss: 0.2391\n","Epoch [15/15], Step [4220/5000], Loss: 0.2689\n","Epoch [15/15], Step [4230/5000], Loss: 0.1376\n","Epoch [15/15], Step [4240/5000], Loss: 0.1841\n","Epoch [15/15], Step [4250/5000], Loss: 0.1827\n","Epoch [15/15], Step [4260/5000], Loss: 0.1488\n","Epoch [15/15], Step [4270/5000], Loss: 0.0909\n","Epoch [15/15], Step [4280/5000], Loss: 0.2198\n","Epoch [15/15], Step [4290/5000], Loss: 0.2317\n","Epoch [15/15], Step [4300/5000], Loss: 0.1639\n","Epoch [15/15], Step [4310/5000], Loss: 0.1970\n","Epoch [15/15], Step [4320/5000], Loss: 0.1416\n","Epoch [15/15], Step [4330/5000], Loss: 0.1418\n","Epoch [15/15], Step [4340/5000], Loss: 0.2992\n","Epoch [15/15], Step [4350/5000], Loss: 0.2363\n","Epoch [15/15], Step [4360/5000], Loss: 0.2951\n","Epoch [15/15], Step [4370/5000], Loss: 0.1833\n","Epoch [15/15], Step [4380/5000], Loss: 0.1400\n","Epoch [15/15], Step [4390/5000], Loss: 0.1878\n","Epoch [15/15], Step [4400/5000], Loss: 0.2319\n","Epoch [15/15], Step [4410/5000], Loss: 0.1661\n","Epoch [15/15], Step [4420/5000], Loss: 0.1900\n","Epoch [15/15], Step [4430/5000], Loss: 0.2025\n","Epoch [15/15], Step [4440/5000], Loss: 0.1685\n","Epoch [15/15], Step [4450/5000], Loss: 0.1512\n","Epoch [15/15], Step [4460/5000], Loss: 0.3013\n","Epoch [15/15], Step [4470/5000], Loss: 0.1709\n","Epoch [15/15], Step [4480/5000], Loss: 0.1851\n","Epoch [15/15], Step [4490/5000], Loss: 0.2580\n","Epoch [15/15], Step [4500/5000], Loss: 0.2011\n","Epoch [15/15], Step [4510/5000], Loss: 0.1138\n","Epoch [15/15], Step [4520/5000], Loss: 0.2676\n","Epoch [15/15], Step [4530/5000], Loss: 0.1715\n","Epoch [15/15], Step [4540/5000], Loss: 0.1501\n","Epoch [15/15], Step [4550/5000], Loss: 0.2342\n","Epoch [15/15], Step [4560/5000], Loss: 0.2111\n","Epoch [15/15], Step [4570/5000], Loss: 0.1366\n","Epoch [15/15], Step [4580/5000], Loss: 0.1774\n","Epoch [15/15], Step [4590/5000], Loss: 0.1973\n","Epoch [15/15], Step [4600/5000], Loss: 0.2458\n","Epoch [15/15], Step [4610/5000], Loss: 0.0189\n","Epoch [15/15], Step [4620/5000], Loss: 0.2039\n","Epoch [15/15], Step [4630/5000], Loss: 0.1472\n","Epoch [15/15], Step [4640/5000], Loss: 0.1455\n","Epoch [15/15], Step [4650/5000], Loss: 0.1015\n","Epoch [15/15], Step [4660/5000], Loss: 0.1234\n","Epoch [15/15], Step [4670/5000], Loss: 0.1625\n","Epoch [15/15], Step [4680/5000], Loss: 0.1615\n","Epoch [15/15], Step [4690/5000], Loss: 0.1551\n","Epoch [15/15], Step [4700/5000], Loss: 0.2020\n","Epoch [15/15], Step [4710/5000], Loss: 0.1361\n","Epoch [15/15], Step [4720/5000], Loss: 0.2429\n","Epoch [15/15], Step [4730/5000], Loss: 0.1941\n","Epoch [15/15], Step [4740/5000], Loss: 0.1299\n","Epoch [15/15], Step [4750/5000], Loss: 0.0698\n","Epoch [15/15], Step [4760/5000], Loss: 0.1831\n","Epoch [15/15], Step [4770/5000], Loss: 0.0255\n","Epoch [15/15], Step [4780/5000], Loss: 0.0681\n","Epoch [15/15], Step [4790/5000], Loss: 0.0695\n","Epoch [15/15], Step [4800/5000], Loss: 0.2105\n","Epoch [15/15], Step [4810/5000], Loss: 0.2097\n","Epoch [15/15], Step [4820/5000], Loss: 0.1864\n","Epoch [15/15], Step [4830/5000], Loss: 0.1392\n","Epoch [15/15], Step [4840/5000], Loss: 0.1838\n","Epoch [15/15], Step [4850/5000], Loss: 0.1667\n","Epoch [15/15], Step [4860/5000], Loss: 0.1036\n","Epoch [15/15], Step [4870/5000], Loss: 0.1342\n","Epoch [15/15], Step [4880/5000], Loss: 0.1198\n","Epoch [15/15], Step [4890/5000], Loss: 0.1000\n","Epoch [15/15], Step [4900/5000], Loss: 0.2848\n","Epoch [15/15], Step [4910/5000], Loss: 0.2243\n","Epoch [15/15], Step [4920/5000], Loss: 0.2046\n","Epoch [15/15], Step [4930/5000], Loss: 0.2157\n","Epoch [15/15], Step [4940/5000], Loss: 0.1514\n","Epoch [15/15], Step [4950/5000], Loss: 0.1630\n","Epoch [15/15], Step [4960/5000], Loss: 0.1059\n","Epoch [15/15], Step [4970/5000], Loss: 0.2367\n","Epoch [15/15], Step [4980/5000], Loss: 0.3257\n","Epoch [15/15], Step [4990/5000], Loss: 0.0836\n","Model saved at epoch 15, Loss: 0.1722\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"Spce7S76-C4y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","#예측만 도출 밑 셀은 정답까지 비교\n","import os\n","import json\n","import torch\n","import torchvision\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","from torch.utils.data import Dataset, DataLoader\n","import torchvision.transforms as transforms\n","from torchvision.models.detection.faster_rcnn import FastRCNNPredictor, FasterRCNN_ResNet50_FPN_Weights\n","from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n","\n","# Load trained model checkpoint\n","def load_model(checkpoint_path, num_classes):\n","    weights = FasterRCNN_ResNet50_FPN_Weights.DEFAULT\n","    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=weights)\n","    in_features = model.roi_heads.box_predictor.cls_score.in_features\n","    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n","\n","    checkpoint = torch.load(checkpoint_path)\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","\n","    return model\n","\n","# Define the dataset class for evaluation\n","class RipCurrentTestDataset(Dataset):\n","    def __init__(self, image_dir, annotation_dir, transform=None, num_samples=10):\n","        self.image_dir = image_dir\n","        self.annotation_dir = annotation_dir\n","        self.transform = transform\n","\n","        self.image_files = [f for f in os.listdir(image_dir) if f.endswith('.jpg')]\n","        self.image_files = self.image_files[:num_samples]  # Limit to num_samples\n","\n","        self.annotations = self.load_annotations(self.image_files)\n","\n","    def load_annotations(self, image_files):\n","        annotations = []\n","        for img_name in image_files:\n","            annotation_path = os.path.join(self.annotation_dir, img_name.replace('.jpg', '.json'))\n","            with open(annotation_path) as f:\n","                annotations.append(json.load(f))\n","        return annotations\n","\n","    def __len__(self):\n","        return len(self.image_files)\n","\n","    def __getitem__(self, idx):\n","        img_name = self.image_files[idx]\n","        img_path = os.path.join(self.image_dir, img_name)\n","        image = Image.open(img_path).convert(\"RGB\")\n","\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        annotation = self.annotations[idx]\n","        boxes = annotation['annotations']['drawing']\n","        if annotation['annotations']['class'] == 0:\n","            # If class is 0, there are no bounding boxes\n","            boxes = torch.empty((0, 4), dtype=torch.float32)\n","            labels = torch.tensor([], dtype=torch.int64)\n","        else:\n","            # Convert points to [x_min, y_min, x_max, y_max] format\n","            valid_boxes = []\n","            for box in boxes:\n","                x_min = min(point[0] for point in box)\n","                y_min = min(point[1] for point in box)\n","                x_max = max(point[0] for point in box)\n","                y_max = max(point[1] for point in box)\n","                if x_max > x_min and y_max > y_min:\n","                    valid_boxes.append([x_min, y_min, x_max, y_max])\n","\n","            boxes = torch.tensor(valid_boxes, dtype=torch.float32)\n","            labels = torch.tensor([annotation['annotations']['class']] * len(boxes), dtype=torch.int64)\n","\n","        target = {}\n","        target['boxes'] = boxes\n","        target['labels'] = labels\n","        target['image_id'] = img_name\n","\n","        return image, target\n","\n","# Define collate_fn to handle batch of targets correctly\n","def collate_fn(batch):\n","    images, targets = zip(*batch)\n","    return list(images), list(targets)\n","\n","transform = transforms.Compose([\n","    transforms.ToTensor(),\n","])\n","\n","test_image_dir = '/content/drive/MyDrive/RIPcurrent/TEST_image_Haeundae_1.GLORY'\n","test_annotation_dir = '/content/drive/MyDrive/RIPcurrent/TEST_json_Haeundae_1.GLORY'\n","checkpoint_path = '/content/drive/MyDrive/RIPcurrent/model_checkpoints/RCNN_checkpoint_epoch_5.pth'\n","\n","test_dataset = RipCurrentTestDataset(image_dir=test_image_dir,\n","                                     annotation_dir=test_annotation_dir,\n","                                     transform=transform,\n","                                     num_samples=10)\n","\n","test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)\n","\n","device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","\n","model = load_model(checkpoint_path, num_classes=2)\n","model.to(device)\n","model.eval()\n","\n","# Function to plot the bounding boxes\n","def plot_image_with_boxes(image, boxes, labels):\n","    plt.imshow(image.permute(1, 2, 0).cpu().numpy())\n","    ax = plt.gca()\n","    for box, label in zip(boxes, labels):\n","        x_min, y_min, x_max, y_max = box\n","        rect = plt.Rectangle((x_min, y_min), x_max - x_min, y_max - y_min, fill=False, color='red', linewidth=2)\n","        ax.add_patch(rect)\n","        ax.text(x_min, y_min, f'Class: {label}', bbox={'facecolor': 'yellow', 'alpha': 0.5})\n","    plt.show()\n","\n","# Initialize lists to store evaluation metrics\n","all_preds = []\n","all_gts = []\n","\n","# Evaluate the model and save predictions\n","results = []\n","\n","for images, targets in test_dataloader:\n","    images = list(image.to(device) for image in images)\n","    outputs = model(images)\n","\n","    for image, output, target in zip(images, outputs, targets):\n","        if isinstance(target, dict):\n","            gt_boxes = target['boxes']\n","            gt_labels = target['labels']\n","        else:\n","            raise TypeError(f\"Unexpected target type: {type(target)} with content: {target}\")\n","\n","        # Collect predictions and ground truths\n","        pred_boxes = output['boxes'].detach().cpu()\n","        pred_labels = output['labels'].detach().cpu()\n","\n","        all_preds.extend(pred_labels.numpy())\n","        all_gts.extend(gt_labels.numpy())\n","\n","        # Save predictions to results list\n","        result = {\n","            'image_id': target['image_id'],\n","            'predictions': [{'box': box.tolist(), 'label': label.item()} for box, label in zip(pred_boxes, pred_labels)],\n","            'ground_truths': [{'box': box.tolist(), 'label': label.item()} for box, label in zip(gt_boxes, gt_labels)]\n","        }\n","        results.append(result)\n","\n","        # Plot the image with bounding boxes\n","        plot_image_with_boxes(image, pred_boxes, pred_labels)\n","\n","# Save results to a JSON file\n","output_file = '/content/drive/MyDrive/RIPcurrent/predictions.json'\n","with open(output_file, 'w') as f:\n","    json.dump(results, f)\n","print(f'Saved predictions to {output_file}')\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":224},"id":"gqJuf5qB-C2C","executionInfo":{"status":"error","timestamp":1719914834398,"user_tz":-540,"elapsed":312,"user":{"displayName":"양유석","userId":"08992042625290856644"}},"outputId":"daae38ab-f350-44df-c1ce-0fc6f8fa2a81"},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'asdadada' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-9d724328ee82>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0masdadada\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#예측만 도출 밑 셀은 정답까지 비교\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'asdadada' is not defined"]}]},{"cell_type":"code","source":["#예측과 정답까지 비교\n","import os\n","import json\n","import torch\n","import torchvision\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","from torch.utils.data import Dataset, DataLoader\n","import torchvision.transforms as transforms\n","from torchvision.models.detection.faster_rcnn import FastRCNNPredictor, FasterRCNN_ResNet50_FPN_Weights\n","from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n","\n","# Load trained model checkpoint\n","def load_model(checkpoint_path, num_classes):\n","    weights = FasterRCNN_ResNet50_FPN_Weights.DEFAULT\n","    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=weights)\n","    in_features = model.roi_heads.box_predictor.cls_score.in_features\n","    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n","\n","    checkpoint = torch.load(checkpoint_path)\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","\n","    return model\n","\n","# Define the dataset class for evaluation\n","class RipCurrentTestDataset(Dataset):\n","    def __init__(self, image_dir, annotation_dir, transform=None, num_samples=20):\n","        self.image_dir = image_dir\n","        self.annotation_dir = annotation_dir\n","        self.transform = transform\n","\n","        self.image_files = [f for f in os.listdir(image_dir) if f.endswith('.jpg')]\n","        self.image_files = self.image_files[:num_samples]  # Limit to num_samples\n","\n","        self.annotations = self.load_annotations(self.image_files)\n","\n","    def load_annotations(self, image_files):\n","        annotations = []\n","        for img_name in image_files:\n","            annotation_path = os.path.join(self.annotation_dir, img_name.replace('.jpg', '.json'))\n","            with open(annotation_path) as f:\n","                annotations.append(json.load(f))\n","        return annotations\n","\n","    def __len__(self):\n","        return len(self.image_files)\n","\n","    def __getitem__(self, idx):\n","        img_name = self.image_files[idx]\n","        img_path = os.path.join(self.image_dir, img_name)\n","        image = Image.open(img_path).convert(\"RGB\")\n","\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        annotation = self.annotations[idx]\n","        boxes = annotation['annotations']['drawing']\n","        if annotation['annotations']['class'] == 0:\n","            # If class is 0, there are no bounding boxes\n","            boxes = torch.empty((0, 4), dtype=torch.float32)\n","            labels = torch.tensor([], dtype=torch.int64)\n","        else:\n","            # Convert points to [x_min, y_min, x_max, y_max] format\n","            valid_boxes = []\n","            for box in boxes:\n","                x_min = min(point[0] for point in box)\n","                y_min = min(point[1] for point in box)\n","                x_max = max(point[0] for point in box)\n","                y_max = max(point[1] for point in box)\n","                if x_max > x_min and y_max > y_min:\n","                    valid_boxes.append([x_min, y_min, x_max, y_max])\n","\n","            boxes = torch.tensor(valid_boxes, dtype=torch.float32)\n","            labels = torch.tensor([annotation['annotations']['class']] * len(boxes), dtype=torch.int64)\n","\n","        target = {}\n","        target['boxes'] = boxes\n","        target['labels'] = labels\n","        target['image_id'] = img_name\n","\n","        return image, target\n","\n","# Define collate_fn to handle batch of targets correctly\n","def collate_fn(batch):\n","    images, targets = zip(*batch)\n","    return list(images), list(targets)\n","\n","transform = transforms.Compose([\n","    transforms.ToTensor(),\n","])\n","\n","test_image_dir = '/content/drive/MyDrive/RIPcurrent/TS_1.Image_4.Daecheon_2.ZIPTR'\n","test_annotation_dir = '/content/drive/MyDrive/RIPcurrent/TL_1.JSON_4.Daecheon_2.ZIPTR'\n","checkpoint_path = '/content/drive/MyDrive/RIPcurrent/model_checkpoints/RCNN_checkpoint_epoch_9.pth'\n","\n","test_dataset = RipCurrentTestDataset(image_dir=test_image_dir,\n","                                     annotation_dir=test_annotation_dir,\n","                                     transform=transform,\n","                                     num_samples=20)\n","\n","test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)\n","\n","device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","\n","model = load_model(checkpoint_path, num_classes=2)\n","model.to(device)\n","model.eval()\n","\n","# Function to plot the bounding boxes\n","def plot_image_with_boxes(image, pred_boxes, pred_labels, gt_boxes, gt_labels):\n","    fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n","\n","    # Plot predicted boxes\n","    ax[0].imshow(image.permute(1, 2, 0).cpu().numpy())\n","    for box, label in zip(pred_boxes, pred_labels):\n","        x_min, y_min, x_max, y_max = box\n","        rect = plt.Rectangle((x_min, y_min), x_max - x_min, y_max - y_min, fill=False, color='red', linewidth=2)\n","        ax[0].add_patch(rect)\n","        ax[0].text(x_min, y_min, f'Pred: {label}', bbox={'facecolor': 'yellow', 'alpha': 0.5})\n","    ax[0].set_title('Predicted Bounding Boxes')\n","\n","    # Plot ground truth boxes\n","    ax[1].imshow(image.permute(1, 2, 0).cpu().numpy())\n","    for box, label in zip(gt_boxes, gt_labels):\n","        x_min, y_min, x_max, y_max = box\n","        rect = plt.Rectangle((x_min, y_min), x_max - x_min, y_max - y_min, fill=False, color='green', linewidth=2)\n","        ax[1].add_patch(rect)\n","        ax[1].text(x_min, y_min, f'GT: {label}', bbox={'facecolor': 'yellow', 'alpha': 0.5})\n","    ax[1].set_title('Ground Truth Bounding Boxes')\n","\n","    plt.show()\n","\n","# Initialize lists to store evaluation metrics\n","all_preds = []\n","all_gts = []\n","\n","# Evaluate the model and save predictions\n","results = []\n","\n","for images, targets in test_dataloader:\n","    images = list(image.to(device) for image in images)\n","    outputs = model(images)\n","\n","    for image, output, target in zip(images, outputs, targets):\n","        if isinstance(target, dict):\n","            gt_boxes = target['boxes']\n","            gt_labels = target['labels']\n","        else:\n","            raise TypeError(f\"Unexpected target type: {type(target)} with content: {target}\")\n","\n","        # Collect predictions and ground truths\n","        pred_boxes = output['boxes'].detach().cpu()\n","        pred_labels = output['labels'].detach().cpu()\n","\n","        all_preds.extend(pred_labels.numpy())\n","        all_gts.extend(gt_labels.numpy())\n","\n","        # Save predictions to results list\n","        result = {\n","            'image_id': target['image_id'],\n","            'predictions': [{'box': box.tolist(), 'label': label.item()} for box, label in zip(pred_boxes, pred_labels)],\n","            'ground_truths': [{'box': box.tolist(), 'label': label.item()} for box, label in zip(gt_boxes, gt_labels)]\n","        }\n","        results.append(result)\n","\n","        # Plot the image with predicted and ground truth bounding boxes\n","        plot_image_with_boxes(image.cpu(), pred_boxes, pred_labels, gt_boxes, gt_labels)\n","\n","# Print lengths of all_preds and all_gts for debugging\n","print(f\"Length of all_gts: {len(all_gts)}\")\n","print(f\"Length of all_preds: {len(all_preds)}\")\n","\n","# Save results to a JSON file\n","output_file = '/content/drive/MyDrive/RIPcurrent/predictions.json'\n","with open(output_file, 'w') as f:\n","    json.dump(results, f)\n","print(f'Saved predictions to {output_file}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":112},"id":"ShpfWHYnDnJO","executionInfo":{"status":"error","timestamp":1719914853309,"user_tz":-540,"elapsed":316,"user":{"displayName":"양유석","userId":"08992042625290856644"}},"outputId":"a07dcdec-05fc-4d41-ccdd-92c5883557d4"},"execution_count":null,"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"invalid decimal literal (<ipython-input-5-3faf10fe643c>, line 2)","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-3faf10fe643c>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    1231321h\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid decimal literal\n"]}]},{"cell_type":"code","source":["import os\n","import json\n","import torch\n","import torchvision\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","from torch.utils.data import Dataset, DataLoader\n","import torchvision.transforms as transforms\n","from torchvision.models.detection.faster_rcnn import FastRCNNPredictor, FasterRCNN_ResNet50_FPN_Weights\n","from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n","\n","# Load trained model checkpoint\n","def load_model(checkpoint_path, num_classes):\n","    weights = FasterRCNN_ResNet50_FPN_Weights.DEFAULT\n","    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=weights)\n","    in_features = model.roi_heads.box_predictor.cls_score.in_features\n","    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n","\n","    checkpoint = torch.load(checkpoint_path)\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","\n","    return model\n","\n","# Define the dataset class for evaluation\n","class RipCurrentTestDataset(Dataset):\n","    def __init__(self, image_dir, annotation_dir, transform=None, num_samples=20):\n","        self.image_dir = image_dir\n","        self.annotation_dir = annotation_dir\n","        self.transform = transform\n","\n","        self.image_files = [f for f in os.listdir(image_dir) if f.endswith('.jpg')]\n","        self.image_files = self.image_files[:num_samples]  # Limit to num_samples\n","\n","        self.annotations = self.load_annotations(self.image_files)\n","\n","    def load_annotations(self, image_files):\n","        annotations = []\n","        for img_name in image_files:\n","            annotation_path = os.path.join(self.annotation_dir, img_name.replace('.jpg', '.json'))\n","            with open(annotation_path) as f:\n","                annotations.append(json.load(f))\n","        return annotations\n","\n","    def __len__(self):\n","        return len(self.image_files)\n","\n","    def __getitem__(self, idx):\n","        img_name = self.image_files[idx]\n","        img_path = os.path.join(self.image_dir, img_name)\n","        image = Image.open(img_path).convert(\"RGB\")\n","\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        annotation = self.annotations[idx]\n","        boxes = annotation['annotations']['drawing']\n","        if annotation['annotations']['class'] == 0:\n","            # If class is 0, there are no bounding boxes\n","            boxes = torch.empty((0, 4), dtype=torch.float32)\n","            labels = torch.tensor([], dtype=torch.int64)\n","        else:\n","            # Convert points to [x_min, y_min, x_max, y_max] format\n","            valid_boxes = []\n","            for box in boxes:\n","                x_min = min(point[0] for point in box)\n","                y_min = min(point[1] for point in box)\n","                x_max = max(point[0] for point in box)\n","                y_max = max(point[1] for point in box)\n","                if x_max > x_min and y_max > y_min:\n","                    valid_boxes.append([x_min, y_min, x_max, y_max])\n","\n","            boxes = torch.tensor(valid_boxes, dtype=torch.float32)\n","            labels = torch.tensor([annotation['annotations']['class']] * len(boxes), dtype=torch.int64)\n","\n","        target = {}\n","        target['boxes'] = boxes\n","        target['labels'] = labels\n","        target['image_id'] = img_name\n","\n","        return image, target\n","\n","# Define collate_fn to handle batch of targets correctly\n","def collate_fn(batch):\n","    images, targets = zip(*batch)\n","    return list(images), list(targets)\n","\n","transform = transforms.Compose([\n","    transforms.ToTensor(),\n","])\n","\n","test_image_dir = '/content/drive/MyDrive/RIPcurrent/TS_1.image_1.Haeundae_2.PARA1'\n","test_annotation_dir = '/content/drive/MyDrive/RIPcurrent/TL_1.JSON_1.Haeundae_2.PARA1'\n","checkpoint_path = '/content/drive/MyDrive/RIPcurrent/model_checkpoints/RCNN_checkpoint_epoch0708PARA2GLORY_15.pth'\n","\n","test_dataset = RipCurrentTestDataset(image_dir=test_image_dir,\n","                                     annotation_dir=test_annotation_dir,\n","                                     transform=transform,\n","                                     num_samples=20)\n","\n","test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)\n","\n","device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","\n","model = load_model(checkpoint_path, num_classes=2)\n","model.to(device)\n","model.eval()\n","\n","# Function to plot the bounding boxes\n","def plot_image_with_boxes(image, pred_boxes, pred_labels, gt_boxes, gt_labels):\n","    fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n","\n","    # Plot predicted boxes\n","    ax[0].imshow(image.permute(1, 2, 0).cpu().numpy())\n","    for box, label in zip(pred_boxes, pred_labels):\n","        x_min, y_min, x_max, y_max = box\n","        rect = plt.Rectangle((x_min, y_min), x_max - x_min, y_max - y_min, fill=False, color='red', linewidth=2)\n","        ax[0].add_patch(rect)\n","        ax[0].text(x_min, y_min, f'Pred: {label}', bbox={'facecolor': 'yellow', 'alpha': 0.5})\n","    ax[0].set_title('Predicted Bounding Boxes')\n","\n","    # Plot ground truth boxes\n","    ax[1].imshow(image.permute(1, 2, 0).cpu().numpy())\n","    for box, label in zip(gt_boxes, gt_labels):\n","        x_min, y_min, x_max, y_max = box\n","        rect = plt.Rectangle((x_min, y_min), x_max - x_min, y_max - y_min, fill=False, color='green', linewidth=2)\n","        ax[1].add_patch(rect)\n","        ax[1].text(x_min, y_min, f'GT: {label}', bbox={'facecolor': 'yellow', 'alpha': 0.5})\n","    ax[1].set_title('Ground Truth Bounding Boxes')\n","\n","    plt.show()\n","\n","# Initialize lists to store evaluation metrics\n","all_preds = []\n","all_gts = []\n","\n","# Evaluate the model and save predictions\n","results = []\n","\n","for images, targets in test_dataloader:\n","    images = list(image.to(device) for image in images)\n","    outputs = model(images)\n","\n","    for image, output, target in zip(images, outputs, targets):\n","        if isinstance(target, dict):\n","            gt_boxes = target['boxes']\n","            gt_labels = target['labels']\n","        else:\n","            raise TypeError(f\"Unexpected target type: {type(target)} with content: {target}\")\n","\n","        # Collect predictions and ground truths\n","        pred_boxes = output['boxes'].detach().cpu()\n","        pred_labels = output['labels'].detach().cpu()\n","\n","        all_preds.extend(pred_labels.numpy().tolist())\n","        all_gts.extend(gt_labels.numpy().tolist())\n","\n","        # Save predictions to results list\n","        result = {\n","            'image_id': target['image_id'],\n","            'predictions': [{'box': box.tolist(), 'label': label.item()} for box, label in zip(pred_boxes, pred_labels)],\n","            'ground_truths': [{'box': box.tolist(), 'label': label.item()} for box, label in zip(gt_boxes, gt_labels)]\n","        }\n","        results.append(result)\n","\n","        # Plot the image with predicted and ground truth bounding boxes\n","        plot_image_with_boxes(image.cpu(), pred_boxes, pred_labels, gt_boxes, gt_labels)\n","\n","# Print lengths of all_preds and all_gts for debugging\n","print(f\"Length of all_gts: {len(all_gts)}\")\n","print(f\"Length of all_preds: {len(all_preds)}\")\n","\n","# Calculate evaluation metrics\n","precision = precision_score(all_gts, all_preds, average='weighted', zero_division=1)\n","recall = recall_score(all_gts, all_preds, average='weighted')\n","f1 = f1_score(all_gts, all_preds, average='weighted')\n","accuracy = accuracy_score(all_gts, all_preds)\n","\n","print(f'Precision: {precision:.4f}')\n","print(f'Recall: {recall:.4f}')\n","print(f'F1 Score: {f1:.4f}')\n","print(f'Accuracy: {accuracy:.4f}')\n","\n","# Save results to a JSON file\n","output_file = '/content/drive/MyDrive/RIPcurrent/predictions.json'\n","with open(output_file, 'w') as f:\n","    json.dump(results, f)\n","print(f'Saved predictions to {output_file}')\n"],"metadata":{"id":"Ed1CtSvYJJ8f","colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1-JK9Qt89Kvl5NZa-iybNvGSed0bYOAMn"},"executionInfo":{"status":"error","timestamp":1720347941067,"user_tz":-540,"elapsed":24312,"user":{"displayName":"양유석","userId":"08992042625290856644"}},"outputId":"e57056c2-1eae-41ec-fd95-d6b4b5e33928"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","source":["#0707 너무 많은 박스를 치는 것에 대한 해결방법 시험중\n","import os\n","import json\n","import torch\n","import torchvision\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","from torch.utils.data import Dataset, DataLoader\n","import torchvision.transforms as transforms\n","from torchvision.models.detection.faster_rcnn import FastRCNNPredictor, FasterRCNN_ResNet50_FPN_Weights\n","from torchvision.ops import nms\n","from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n","\n","# Load trained model checkpoint\n","def load_model(checkpoint_path, num_classes):\n","    weights = FasterRCNN_ResNet50_FPN_Weights.DEFAULT\n","    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=weights)\n","    in_features = model.roi_heads.box_predictor.cls_score.in_features\n","    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n","\n","    checkpoint = torch.load(checkpoint_path)\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","\n","    return model\n","\n","# Define the dataset class for evaluation\n","class RipCurrentTestDataset(Dataset):\n","    def __init__(self, image_dir, annotation_dir, transform=None, num_samples=20):\n","        self.image_dir = image_dir\n","        self.annotation_dir = annotation_dir\n","        self.transform = transform\n","\n","        self.image_files = [f for f in os.listdir(image_dir) if f.endswith('.jpg')]\n","        self.image_files = self.image_files[:num_samples]  # Limit to num_samples\n","\n","        self.annotations = self.load_annotations(self.image_files)\n","\n","    def load_annotations(self, image_files):\n","        annotations = []\n","        for img_name in image_files:\n","            annotation_path = os.path.join(self.annotation_dir, img_name.replace('.jpg', '.json'))\n","            with open(annotation_path) as f:\n","                annotations.append(json.load(f))\n","        return annotations\n","\n","    def __len__(self):\n","        return len(self.image_files)\n","\n","    def __getitem__(self, idx):\n","        img_name = self.image_files[idx]\n","        img_path = os.path.join(self.image_dir, img_name)\n","        image = Image.open(img_path).convert(\"RGB\")\n","\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        annotation = self.annotations[idx]\n","        boxes = annotation['annotations']['drawing']\n","        if annotation['annotations']['class'] == 0:\n","            # If class is 0, there are no bounding boxes\n","            boxes = torch.empty((0, 4), dtype=torch.float32)\n","            labels = torch.tensor([], dtype=torch.int64)\n","        else:\n","            # Convert points to [x_min, y_min, x_max, y_max] format\n","            valid_boxes = []\n","            for box in boxes:\n","                x_min = min(point[0] for point in box)\n","                y_min = min(point[1] for point in box)\n","                x_max = max(point[0] for point in box)\n","                y_max = max(point[1] for point in box)\n","                if x_max > x_min and y_max > y_min:\n","                    valid_boxes.append([x_min, y_min, x_max, y_max])\n","\n","            boxes = torch.tensor(valid_boxes, dtype=torch.float32)\n","            labels = torch.tensor([annotation['annotations']['class']] * len(boxes), dtype=torch.int64)\n","\n","        target = {}\n","        target['boxes'] = boxes\n","        target['labels'] = labels\n","        target['image_id'] = img_name\n","\n","        return image, target\n","\n","# Define collate_fn to handle batch of targets correctly\n","def collate_fn(batch):\n","    images, targets = zip(*batch)\n","    return list(images), list(targets)\n","\n","transform = transforms.Compose([\n","    transforms.ToTensor(),\n","])\n","\n","test_image_dir = '/content/drive/MyDrive/RIPcurrent/TS_1.image_1.Haeundae_2.PARA1'\n","test_annotation_dir = '/content/drive/MyDrive/RIPcurrent/TL_1.JSON_1.Haeundae_2.PARA1'\n","checkpoint_path = '/content/drive/MyDrive/RIPcurrent/model_checkpoints/RCNN_checkpoint_epoch0708PARA2GLORY_15.pth'\n","\n","test_dataset = RipCurrentTestDataset(image_dir=test_image_dir,\n","                                     annotation_dir=test_annotation_dir,\n","                                     transform=transform,\n","                                     num_samples=20)\n","\n","test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)\n","\n","device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","\n","model = load_model(checkpoint_path, num_classes=2)\n","model.to(device)\n","model.eval()\n","\n","# Function to plot the bounding boxes\n","def plot_image_with_boxes(image, pred_boxes, pred_labels, gt_boxes, gt_labels):\n","    fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n","\n","    # Plot predicted boxes\n","    ax[0].imshow(image.permute(1, 2, 0).cpu().numpy())\n","    for box, label in zip(pred_boxes, pred_labels):\n","        x_min, y_min, x_max, y_max = box\n","        rect = plt.Rectangle((x_min, y_min), x_max - x_min, y_max - y_min, fill=False, color='red', linewidth=2)\n","        ax[0].add_patch(rect)\n","        ax[0].text(x_min, y_min, f'Pred: {label}', bbox={'facecolor': 'yellow', 'alpha': 0.5})\n","    ax[0].set_title('Predicted Bounding Boxes')\n","\n","    # Plot ground truth boxes\n","    ax[1].imshow(image.permute(1, 2, 0).cpu().numpy())\n","    for box, label in zip(gt_boxes, gt_labels):\n","        x_min, y_min, x_max, y_max = box\n","        rect = plt.Rectangle((x_min, y_min), x_max - x_min, y_max - y_min, fill=False, color='green', linewidth=2)\n","        ax[1].add_patch(rect)\n","        ax[1].text(x_min, y_min, f'GT: {label}', bbox={'facecolor': 'yellow', 'alpha': 0.5})\n","    ax[1].set_title('Ground Truth Bounding Boxes')\n","\n","    plt.show()\n","\n","# Initialize lists to store evaluation metrics\n","all_preds = []\n","all_gts = []\n","\n","# Evaluate the model and save predictions\n","results = []\n","\n","# Set the confidence threshold and IoU threshold for NMS\n","conf_threshold = 0.5\n","iou_threshold = 0.5\n","\n","for images, targets in test_dataloader:\n","    images = list(image.to(device) for image in images)\n","    outputs = model(images)\n","\n","    for image, output, target in zip(images, outputs, targets):\n","        if isinstance(target, dict):\n","            gt_boxes = target['boxes']\n","            gt_labels = target['labels']\n","        else:\n","            raise TypeError(f\"Unexpected target type: {type(target)} with content: {target}\")\n","\n","        # Collect predictions and ground truths\n","        pred_boxes = output['boxes'].detach().cpu()\n","        pred_labels = output['labels'].detach().cpu()\n","        pred_scores = output['scores'].detach().cpu()\n","\n","        # Apply confidence threshold\n","        high_conf_indices = pred_scores > conf_threshold\n","        pred_boxes = pred_boxes[high_conf_indices]\n","        pred_labels = pred_labels[high_conf_indices]\n","        pred_scores = pred_scores[high_conf_indices]\n","\n","        # Apply NMS\n","        keep = nms(pred_boxes, pred_scores, iou_threshold)\n","        pred_boxes = pred_boxes[keep]\n","        pred_labels = pred_labels[keep]\n","\n","        all_preds.extend(pred_labels.numpy().tolist())\n","        all_gts.extend(gt_labels.numpy().tolist())\n","\n","        # Save predictions to results list\n","        result = {\n","            'image_id': target['image_id'],\n","            'predictions': [{'box': box.tolist(), 'label': label.item()} for box, label in zip(pred_boxes, pred_labels)],\n","            'ground_truths': [{'box': box.tolist(), 'label': label.item()} for box, label in zip(gt_boxes, gt_labels)]\n","        }\n","        results.append(result)\n","\n","        # Plot the image with predicted and ground truth bounding boxes\n","        plot_image_with_boxes(image.cpu(), pred_boxes, pred_labels, gt_boxes, gt_labels)\n","\n","# Print lengths of all_preds and all_gts for debugging\n","print(f\"Length of all_gts: {len(all_gts)}\")\n","print(f\"Length of all_preds: {len(all_preds)}\")\n","\n","# Calculate evaluation metrics\n","precision = precision_score(all_gts, all_preds, average='weighted', zero_division=1)\n","recall = recall_score(all_gts, all_preds, average='weighted')\n","f1 = f1_score(all_gts, all_preds, average='weighted')\n","accuracy = accuracy_score(all_gts, all_preds)\n","\n","print(f'Precision: {precision:.4f}')\n","print(f'Recall: {recall:.4f}')\n","print(f'F1 Score: {f1:.4f}')\n","print(f'Accuracy: {accuracy:.4f}')\n","\n","# Save results to a JSON file\n","output_file = '/content/drive/MyDrive/RIPcurrent/predictions.json'\n","with open(output_file, 'w') as f:\n","    json.dump(results, f)\n","print(f'Saved predictions to {output_file}')\n"],"metadata":{"id":"cnIcJNc8JJ3F","colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1d0W_9Rcu9TIGNzzNTYBg613KWVwOyYnT"},"executionInfo":{"status":"error","timestamp":1720349322917,"user_tz":-540,"elapsed":43982,"user":{"displayName":"양유석","userId":"08992042625290856644"}},"outputId":"3bb61b54-7de6-48c1-84a1-e20ffa6537ca"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","source":[],"metadata":{"id":"pbuQMfpYxNWj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import json\n","import torch\n","import torchvision\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","from torch.utils.data import Dataset, DataLoader\n","import torchvision.transforms as transforms\n","from torchvision.models.detection.faster_rcnn import FastRCNNPredictor, FasterRCNN_ResNet50_FPN_Weights\n","from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n","import cv2\n","import time\n","\n","# Load trained model checkpoint\n","def load_model(checkpoint_path, num_classes):\n","    weights = FasterRCNN_ResNet50_FPN_Weights.DEFAULT\n","    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=weights)\n","    in_features = model.roi_heads.box_predictor.cls_score.in_features\n","    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n","\n","    checkpoint = torch.load(checkpoint_path)\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","\n","    return model\n","\n","# Function to plot the bounding boxes\n","def plot_image_with_boxes(image, boxes, labels):\n","    plt.imshow(image.permute(1, 2, 0).cpu().numpy())\n","    ax = plt.gca()\n","    for box, label in zip(boxes, labels):\n","        x_min, y_min, x_max, y_max = box\n","        rect = plt.Rectangle((x_min, y_min), x_max - x_min, y_max - y_min, fill=False, color='red', linewidth=2)\n","        ax.add_patch(rect)\n","        ax.text(x_min, y_min, f'Class: {label}', bbox={'facecolor': 'yellow', 'alpha': 0.5})\n","    plt.show()\n","\n","# Initialize lists to store evaluation metrics\n","all_preds = []\n","all_gts = []\n","\n","# Set device\n","device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","\n","# Load model\n","checkpoint_path = '/content/drive/MyDrive/RIPcurrent/model_checkpoints/RCNN_checkpoint_epoch_9.pth'\n","model = load_model(checkpoint_path, num_classes=2)\n","model.to(device)\n","model.eval()\n","\n","# Set up video capture\n","cap = cv2.VideoCapture('rtmps://wsb.live.smilecdn.com/wsbrtsp15/stream15.stream')\n","\n","if not cap.isOpened():\n","    print(\"Can't open video.\")\n","    exit()\n","\n","# Set the interval to 1 second\n","interval = 1\n","\n","while True:\n","    start_time = time.time()\n","    ret, frame = cap.read()\n","    if not ret:\n","        break\n","\n","    # Convert frame to PIL image\n","    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","    pil_image = Image.fromarray(frame_rgb)\n","\n","    # Transform the image\n","    transform = transforms.Compose([\n","        transforms.ToTensor(),\n","    ])\n","    image = transform(pil_image).unsqueeze(0).to(device)\n","\n","    # Perform prediction\n","    outputs = model(image)\n","\n","    # Process outputs\n","    output = outputs[0]\n","    pred_boxes = output['boxes'].detach().cpu()\n","    pred_labels = output['labels'].detach().cpu()\n","\n","    # Plot the image with bounding boxes\n","    plot_image_with_boxes(image[0], pred_boxes, pred_labels)\n","\n","    # Save predictions to results list\n","    result = {\n","        'image_id': f'frame_{int(start_time)}',\n","        'predictions': [{'box': box.tolist(), 'label': label.item()} for box, label in zip(pred_boxes, pred_labels)]\n","    }\n","    print(result)\n","\n","    # Wait for the next interval\n","    while (time.time() - start_time) < interval:\n","        time.sleep(0.01)\n","\n","cap.release()\n","cv2.destroyAllWindows()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":112},"id":"tsS6Hu61JJn_","executionInfo":{"status":"error","timestamp":1719914872861,"user_tz":-540,"elapsed":288,"user":{"displayName":"양유석","userId":"08992042625290856644"}},"outputId":"bad941f1-cd48-475e-e029-5338a6906a7b"},"execution_count":null,"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"invalid syntax (<ipython-input-6-ae743a50e911>, line 1)","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-ae743a50e911>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    aimport os\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"ecVISy2SN69h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import json\n","import torch\n","import torchvision\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","from torch.utils.data import Dataset, DataLoader\n","import torchvision.transforms as transforms\n","from torchvision.models.detection.faster_rcnn import FastRCNNPredictor, FasterRCNN_ResNet50_FPN_Weights\n","from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n","import cv2\n","import time\n","from google.colab.patches import cv2_imshow\n","\n","# Load trained model checkpoint\n","def load_model(checkpoint_path, num_classes):\n","    weights = FasterRCNN_ResNet50_FPN_Weights.DEFAULT\n","    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=weights)\n","    in_features = model.roi_heads.box_predictor.cls_score.in_features\n","    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n","\n","    checkpoint = torch.load(checkpoint_path)\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","\n","    return model\n","\n","# Function to draw bounding boxes on the frame\n","def draw_boxes_on_frame(frame, boxes, labels):\n","    for box, label in zip(boxes, labels):\n","        x_min, y_min, x_max, y_max = map(int, box)\n","        cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), (0, 0, 255), 2)\n","        cv2.putText(frame, f'Class: {label}', (x_min, y_min-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 255), 2)\n","    return frame\n","\n","# Set device\n","device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","\n","# Load model\n","checkpoint_path = '/content/drive/MyDrive/RIPcurrent/model_checkpoints/RCNN_checkpoint_epoch_9.pth'\n","model = load_model(checkpoint_path, num_classes=2)\n","model.to(device)\n","model.eval()\n","\n","# Set up video capture\n","cap = cv2.VideoCapture('rtmps://wsb.live.smilecdn.com/wsbrtsp3/stream3.stream')\n","\n","if not cap.isOpened():\n","    print(\"Can't open video.\")\n","    exit()\n","\n","# Set the interval to 3 seconds\n","interval = 3\n","\n","while True:\n","    start_time = time.time()\n","    ret, frame = cap.read()\n","    if not ret:\n","        break\n","\n","    # Skip frames to maintain the interval\n","    while (time.time() - start_time) < interval:\n","        cap.grab()\n","\n","    # Convert frame to PIL image\n","    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","    pil_image = Image.fromarray(frame_rgb)\n","\n","    # Transform the image\n","    transform = transforms.Compose([\n","        transforms.ToTensor(),\n","    ])\n","    image = transform(pil_image).unsqueeze(0).to(device)\n","\n","    # Perform prediction\n","    outputs = model(image)\n","\n","    # Process outputs\n","    output = outputs[0]\n","    pred_boxes = output['boxes'].detach().cpu()\n","    pred_labels = output['labels'].detach().cpu()\n","\n","    # Draw bounding boxes on the frame\n","    frame_with_boxes = draw_boxes_on_frame(frame, pred_boxes, pred_labels)\n","\n","    # Display the frame with bounding boxes\n","    cv2_imshow(frame_with_boxes)\n","\n","    # Save predictions to results list\n","    result = {\n","        'image_id': f'frame_{int(start_time)}',\n","        'predictions': [{'box': box.tolist(), 'label': label.item()} for box, label in zip(pred_boxes, pred_labels)]\n","    }\n","    print(result)\n","\n","    # Wait for the next interval\n","    while (time.time() - start_time) < interval:\n","        time.sleep(0.01)\n","\n","cap.release()\n","cv2.destroyAllWindows()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":112},"id":"2b5KZruRN7WQ","executionInfo":{"status":"error","timestamp":1719914882754,"user_tz":-540,"elapsed":307,"user":{"displayName":"양유석","userId":"08992042625290856644"}},"outputId":"adf347fc-4877-4772-e5df-30d7191705c6"},"execution_count":null,"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"invalid syntax (<ipython-input-7-d31e15f93537>, line 1)","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-7-d31e15f93537>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    aimport os\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"224bfpaeexKa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import json\n","import torch\n","import torchvision\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","from torch.utils.data import Dataset, DataLoader\n","import torchvision.transforms as transforms\n","from torchvision.models.detection.faster_rcnn import FastRCNNPredictor, FasterRCNN_ResNet50_FPN_Weights\n","from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n","import cv2\n","import time\n","import numpy as np\n","\n","# Load trained model checkpoint\n","def load_model(checkpoint_path, num_classes):\n","    weights = FasterRCNN_ResNet50_FPN_Weights.DEFAULT\n","    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=weights)\n","    in_features = model.roi_heads.box_predictor.cls_score.in_features\n","    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n","\n","    checkpoint = torch.load(checkpoint_path)\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","\n","    return model\n","\n","# Function to draw bounding boxes on the frame\n","def draw_boxes_on_frame(frame, boxes, labels):\n","    for box, label in zip(boxes, labels):\n","        x_min, y_min, x_max, y_max = map(int, box)\n","        cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), (0, 0, 255), 2)\n","        cv2.putText(frame, f'Class: {label}', (x_min, y_min-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 255), 2)\n","    return frame\n","\n","# Function to plot the frame with bounding boxes using matplotlib\n","def plot_frame_with_boxes(frame, boxes, labels):\n","    frame_with_boxes = draw_boxes_on_frame(frame, boxes, labels)\n","    plt.imshow(cv2.cvtColor(frame_with_boxes, cv2.COLOR_BGR2RGB))\n","    plt.axis('on')\n","    plt.show()\n","\n","# Set device\n","device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","\n","# Load model\n","checkpoint_path = '/content/drive/MyDrive/RIPcurrent/model_checkpoints/RCNN_checkpoint_epoch0707PARAGLORY_9.pth'\n","model = load_model(checkpoint_path, num_classes=2)\n","model.to(device)\n","model.eval()\n","\n","# Set up video capture\n","cap = cv2.VideoCapture('rtmps://wsb.live.smilecdn.com/wsbrtsp3/stream3.stream')\n","\n","if not cap.isOpened():\n","    print(\"Can't open video.\")\n","    exit()\n","\n","# Set the interval to 3 seconds\n","interval = 1\n","\n","while True:\n","    start_time = time.time()\n","    ret, frame = cap.read()\n","    if not ret:\n","        break\n","\n","    # Skip frames to maintain the interval\n","    while (time.time() - start_time) < interval:\n","        cap.grab()\n","\n","    # Convert frame to PIL image\n","    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","    pil_image = Image.fromarray(frame_rgb)\n","\n","    # Transform the image\n","    transform = transforms.Compose([\n","        transforms.ToTensor(),\n","    ])\n","    image = transform(pil_image).unsqueeze(0).to(device)\n","\n","    # Perform prediction\n","    outputs = model(image)\n","\n","    # Process outputs\n","    output = outputs[0]\n","    pred_boxes = output['boxes'].detach().cpu()\n","    pred_labels = output['labels'].detach().cpu()\n","\n","    # Plot the frame with bounding boxes\n","    plot_frame_with_boxes(frame, pred_boxes, pred_labels)\n","\n","    # Save predictions to results list\n","    result = {\n","        'image_id': f'frame_{int(start_time)}',\n","        'predictions': [{'box': box.tolist(), 'label': label.item()} for box, label in zip(pred_boxes, pred_labels)]\n","    }\n","    print(result)\n","\n","    # Wait for the next interval\n","    while (time.time() - start_time) < interval:\n","        time.sleep(0.01)\n","\n","cap.release()\n","cv2.destroyAllWindows()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1ftJSky83wdp7UPGgeIQ5L_Fe85B9JX4F"},"id":"sqGZem3rex2-","executionInfo":{"status":"error","timestamp":1720216478163,"user_tz":-540,"elapsed":55682,"user":{"displayName":"양유석","userId":"08992042625290856644"}},"outputId":"40b770b6-003f-4d59-b722-ee601295d593"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","source":[],"metadata":{"id":"D6jBckAL4vUW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Pw0kdfAU4wX8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"7rbsUe_P4wUV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"LW0-V3Xh4wRe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install requests"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MLj1jHu54wOt","executionInfo":{"status":"ok","timestamp":1720417790417,"user_tz":-540,"elapsed":6841,"user":{"displayName":"양유석","userId":"08992042625290856644"}},"outputId":"a00ffaf6-9d7e-4557-d49b-c23862cca3aa"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.6.2)\n"]}]},{"cell_type":"code","source":["import requests\n","import json\n","from requests.exceptions import HTTPError\n","\n","url = \"http://localhost:8080/\"\n","data = {\n","    \"key1\": \"value1\",\n","    \"key2\": \"value2\"\n","}\n","headers = {\n","    \"Content-Type\": \"application/json\"\n","}\n","\n","try:\n","    response = requests.post(url, data=json.dumps(data), headers=headers)\n","    response.raise_for_status()  # HTTPError가 발생할 수 있는 곳\n","    print(\"POST 요청 성공\")\n","    print(\"응답 데이터:\", response.json())\n","except HTTPError as http_err:\n","    print(f\"HTTP 에러 발생: {http_err}\")\n","except Exception as err:\n","    print(f\"다른 에러 발생: {err}\")\n"],"metadata":{"id":"0BebnNW_4v1L"},"execution_count":null,"outputs":[]}]}